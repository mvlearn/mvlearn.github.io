{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 2-View Semi-Supervised Regression\n\nThis tutorial demonstrates co-training regression on a semi-supervised\nregression task. The data only has targets for 20% of its samples, and\nalthough it does not have multiple views, co-training regression can still be\nbeneficial. In order to get this benefit, the CTRegressor object is\ninitialized with 2 different types of KNeighborsRegressors (in this case, the\npower parameter for the Minkowski metric is different in each view). Then, the\nsingle view of data (X) is passed in twice as if it shows two different views.\nThe MSE of the predictions on test data from the resulting CTRegressor is\ncompared to the MSE from using each of the individual KNeighborsRegressor\nobjects after fitting on the labeled samples of the training data. The MSE\nshows that the CTRegressor does better than using either KNeighborsRegressor\nalone in this semi-supervised case.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# License: MIT\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom mvlearn.semi_supervised import CTRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating 3D Mexican Hat Data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "N_samples = 3750\nN_test = 1250\nlabeled_portion = .2\n\nseed = 42\nnp.random.seed(seed)\n\nX = np.random.uniform(-4*np.pi, 4*np.pi, size=(N_samples, 2))\ny = ((np.sin(np.linalg.norm(X, axis=1)))/np.linalg.norm(X, axis=1)).squeeze()\nX_test = np.random.uniform(-4*np.pi, 4*np.pi, size=(N_test, 2))\ny_test = ((np.sin(np.linalg.norm(X_test, axis=1))) /\n          np.linalg.norm(X_test, axis=1)).squeeze()\n\ny_train = y.copy()\nnp.random.seed(1)\n\n# Randomly selecting the index which are to be made nan\nselector = np.random.uniform(size=(N_samples,))\nselector[selector > labeled_portion] = np.nan\ny_train[np.isnan(selector)] = np.nan\nlab_samples = ~np.isnan(y_train)\n\nnot_null = [i for i in range(len(y_train)) if not np.isnan(y_train[i])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization of Data\n\nHere, we plot the labeled samples that we have.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\nax = plt.axes(projection=Axes3D.name)\n\nz_points = y[lab_samples]\nx_points = X[lab_samples, 0]\ny_points = X[lab_samples, 1]\nax.scatter3D(x_points, y_points, z_points)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Co-Training on 2 views vs Single view training\n\nHere, we are using the KNeighborsRegressor as the estimators for regression.\nWe are using the default value for all the parameters except the p value in\norder to make the estimators independent.\nThe same p values are used for training the corresponding single view model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Single view semi-supervised learning\nknn1 = KNeighborsRegressor(p=2)\nknn2 = KNeighborsRegressor(p=5)\n\n# Train single view on only the examples with labels\nknn1.fit(X[not_null], y[not_null])\npred1 = knn1.predict(X_test)\nerror1 = mean_squared_error(y_test, pred1)\n\nknn2.fit(X[not_null], y[not_null])\npred2 = knn2.predict(X_test)\nerror2 = mean_squared_error(y_test, pred2)\n\nprint(\"MSE of single view with knn1 {}\\n\".format(error1))\nprint(\"MSE of single view with knn2 {}\\n\".format(error2))\n\n# Multi-view co-training semi-supervised learning\nestimator1 = KNeighborsRegressor(p=2)\nestimator2 = KNeighborsRegressor(p=5)\nknn = CTRegressor(estimator1, estimator2, random_state=26)\n\n# Train a CTClassifier on all the labeled and unlabeled training data\nknn.fit([X, X], y_train)\npred_multi_view = knn.predict([X_test, X_test])\nerror_multi_view = mean_squared_error(y_test, pred_multi_view)\n\nprint(f\"MSE of cotraining semi supervised regression {error_multi_view}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}