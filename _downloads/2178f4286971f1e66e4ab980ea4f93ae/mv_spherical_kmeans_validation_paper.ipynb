{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Multiview Spherical KMeans on the Newsgroup dataset\n\nHere we will validate the implementation of multi-view spherical kmeans by\nreplicating the right side of figure 3 from the Multi-View Clustering paper\nby Bickel and Scheffer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom scipy import sparse\nfrom mvlearn.cluster import MultiviewSphericalKMeans\nfrom joblib import Parallel, delayed\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter('ignore')  # Ignore warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recreating the artificial dataset from the paper\n\nThe experiment in the paper used the 20 Newsgroup dataset, which consists of\naround 18000 newsgroups posts on 20 topics. This dataset can be obtained from\nscikit-learn. To create the artificial dataset used in the experiment, 10 of\nthe 20 classes from the 20 newsgroups dataset were selected and grouped into\n2 groups of 5 classes, and then encoded as tfidf vectors. These now\nrepresented the 5 multi-view classes, each with 2 views (one from each\ngroup). 200 examples were randomly sampled from each of the 20 newsgroups,\nproducing 1000 concatenated examples uniformly distributed over the 5\nclasses.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "NUM_SAMPLES = 200\n\n# Load in the vectorized news group data from scikit-learn package\nnews = fetch_20newsgroups(subset='all')\nall_data = np.array(news.data)\nall_targets = np.array(news.target)\nclass_names = news.target_names\n\n\n# A function to get the 20 newsgroup data\ndef get_data():\n    # Set class pairings as described in the multiview clustering paper\n    view1_classes = ['comp.graphics', 'rec.motorcycles',\n                     'sci.space', 'rec.sport.hockey',\n                     'comp.sys.ibm.pc.hardware']\n    view2_classes = ['rec.autos', 'sci.med', 'misc.forsale',\n                     'soc.religion.christian', 'comp.os.ms-windows.misc']\n\n    # Create lists to hold data and labels for each of the 5 classes across\n    # 2 different views\n    labels = [num for num in range(len(view1_classes))\n              for _ in range(NUM_SAMPLES)]\n    labels = np.array(labels)\n    view1_data = list()\n    view2_data = list()\n\n    # Randomly sample 200 items from each of the selected classes in view1\n    for ind in range(len(view1_classes)):\n        class_num = class_names.index(view1_classes[ind])\n        class_data = all_data[(all_targets == class_num)]\n        indices = np.random.choice(class_data.shape[0], NUM_SAMPLES)\n        view1_data.append(class_data[indices])\n    view1_data = np.concatenate(view1_data)\n\n    # Randomly sample 200 items from each of the selected classes in view2\n    for ind in range(len(view2_classes)):\n        class_num = class_names.index(view2_classes[ind])\n        class_data = all_data[(all_targets == class_num)]\n        indices = np.random.choice(class_data.shape[0], NUM_SAMPLES)\n        view2_data.append(class_data[indices])\n    view2_data = np.concatenate(view2_data)\n\n    # Vectorize the data\n    vectorizer = TfidfVectorizer()\n    view1_data = vectorizer.fit_transform(view1_data)\n    view2_data = vectorizer.fit_transform(view2_data)\n\n    # Shuffle and normalize vectors\n    shuffled_inds = np.random.permutation(NUM_SAMPLES * len(view1_classes))\n    view1_data = sparse.vstack(view1_data)\n    view2_data = sparse.vstack(view2_data)\n    view1_data = np.array(view1_data[shuffled_inds].todense())\n    view2_data = np.array(view2_data[shuffled_inds].todense())\n    magnitudes1 = np.linalg.norm(view1_data, axis=1)\n    magnitudes2 = np.linalg.norm(view2_data, axis=1)\n    magnitudes1[magnitudes1 == 0] = 1\n    magnitudes2[magnitudes2 == 0] = 1\n    magnitudes1 = magnitudes1.reshape((-1, 1))\n    magnitudes2 = magnitudes2.reshape((-1, 1))\n    view1_data /= magnitudes1\n    view2_data /= magnitudes2\n    labels = labels[shuffled_inds]\n\n    return view1_data, view2_data, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function to compute cluster entropy\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compute_entropy(partitions, labels, k, num_classes):\n    total_entropy = 0\n    num_examples = partitions.shape[0]\n    for part in range(k):\n        labs = labels[partitions == part]\n        part_size = labs.shape[0]\n        part_entropy = 0\n        for cl in range(num_classes):\n            prop = np.sum(labs == cl) * 1.0 / part_size\n            ent = 0\n            if(prop != 0):\n                ent = - prop * np.log2(prop)\n            part_entropy += ent\n        part_entropy = part_entropy * part_size / num_examples\n        total_entropy += part_entropy\n    return total_entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Functions to Initialize Centroids and Run Experiment\n\nThe randSpherical function initializes the initial cluster centroids by\ntaking a uniform random sampling of points on the surface of a unit\nhypersphere. The getEntropies function runs Multi-View Spherical Kmeans\nClustering on the data with n_clusters from 1 to 10 once each. This function\nessentially runs one trial of the experiment.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def randSpherical(n_clusters, n_feat1, n_feat2):\n    c_centers1 = np.random.normal(0, 1, (n_clusters, n_feat1))\n    c_centers1 /= np.linalg.norm(c_centers1, axis=1).reshape((-1, 1))\n    c_centers2 = np.random.normal(0, 1, (n_clusters, n_feat2))\n    c_centers2 /= np.linalg.norm(c_centers2, axis=1).reshape((-1, 1))\n    return [c_centers1, c_centers2]\n\n\ndef getEntropies():\n    v1_data, v2_data, labels = get_data()\n\n    entropies = list()\n    for num in range(1, 11):\n        centers = randSpherical(num, v1_data.shape[1], v2_data.shape[1])\n        kmeans = MultiviewSphericalKMeans(n_clusters=num, init=centers,\n                                          n_init=1)\n        pred = kmeans.fit_predict([v1_data, v2_data])\n        ent = compute_entropy(pred, labels, num, 5)\n        entropies.append(ent)\n    print('done')\n    return entropies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running multiple trials of the experiment\n\nIt was difficult to exactly reproduce the results from the Multi-View\nClustering Paper because the experimentors randomly sampled a subset of the\n20 newsgroup dataset samples to create the artificial dataset, and this\nrandom subset was not reported. Therefore, in an attempt to at least\nreplicate the overall shape of the distribution of cluster entropy over the\nnumber of clusters, we resample the dataset and recreate the artificial\ndataset each trial. Therefore, each trial consists of resampling and\nrecreating the artificial dataset, and then running Multi-view Spherical\nKMeans clustering on that dataset for n_clusters 1 to 10 once each. We\nperformed 5 such trials and the results of this are shown below.\nYou can use 80 trials for better results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Do spherical kmeans and get entropy values for each k for multiple trials\nn_jobs = 1\nn_trials = 5\nmult_entropies1 = Parallel(n_jobs=n_jobs)(\n    delayed(getEntropies)() for i in range(n_trials))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment Results\n\nWe see the results of this experiment below. Here, we have more or less\nreproduced the shape of the distribution as seen in figure 3 from the Multi-\nview Clustering Paper.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mult_entropies1 = np.array(mult_entropies1)\nave_m_entropies = np.mean(mult_entropies1, axis=0)\nstd_m_entropies = np.std(mult_entropies1, axis=0)\nx_values = list(range(1, 11))\nplt.errorbar(x_values, ave_m_entropies, std_m_entropies, capsize=5,\n             color='#F46C12')\nplt.xlabel('k')\nplt.ylabel('Entropy')\nplt.legend(['2 Views'])\nplt.rc('axes', labelsize=12)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}