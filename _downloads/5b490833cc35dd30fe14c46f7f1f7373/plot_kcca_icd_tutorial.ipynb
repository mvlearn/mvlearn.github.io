{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Incomplete Cholesky Decomposition (ICD) for KCCA\n\nKernel matrices grow exponentially with the size of the data. There are\nimmense storage and run-time constraints that arise when working with large\ndatasets. The Incomplete Cholesky Decomposition (ICD) looks for a low rank\napproximation of the Cholesky decomposition of the kernel matrix. This\nreduces storage requirements from O(n^2) to O(nm), where n is the number of\nsubjects (rows) and m is the rank of the kernel matrix. This also reduces the\nrun-time from O(n^3) to O(nm^2).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\nimport matplotlib.cbook\nimport warnings\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom mvlearn.plotting.plot import crossviews_plot\nfrom mvlearn.embed.kcca import KCCA\nimport numpy as np\nimport sys\nsys.path.append(\"../../..\")\n\nwarnings.filterwarnings(\"ignore\", category=matplotlib.cbook.mplDeprecation)\n\n\ndef make_data(kernel, N):\n    # Define two latent variables (number of samples x 1)\n    latvar1 = np.random.randn(N,)\n    latvar2 = np.random.randn(N,)\n\n    # Define independent components for each dataset\n    # (number of observations x dataset dimensions)\n    indep1 = np.random.randn(N, 4)\n    indep2 = np.random.randn(N, 5)\n\n    if kernel == \"linear\":\n        x = 0.25*indep1 + 0.75 * \\\n            np.vstack((latvar1, latvar2, latvar1, latvar2)).T\n        y = 0.25*indep2 + 0.75 * \\\n            np.vstack((latvar1, latvar2, latvar1, latvar2, latvar1)).T\n\n        return [x, y]\n\n    elif kernel == \"poly\":\n        x = 0.25*indep1 + 0.75 * \\\n            np.vstack((latvar1**2, latvar2**2, latvar1**2, latvar2**2)).T\n        y = 0.25*indep2 + 0.75 * \\\n            np.vstack((latvar1, latvar2, latvar1, latvar2, latvar1)).T\n\n        return [x, y]\n\n    elif kernel == \"gaussian\":\n        t = np.random.uniform(-np.pi, np.pi, N)\n        e1 = np.random.normal(0, 0.05, (N, 2))\n        e2 = np.random.normal(0, 0.05, (N, 2))\n\n        x = np.zeros((N, 2))\n        x[:, 0] = t\n        x[:, 1] = np.sin(3*t)\n        x += e1\n\n        y = np.zeros((N, 2))\n        y[:, 0] = np.exp(t/4)*np.cos(2*t)\n        y[:, 1] = np.exp(t/4)*np.sin(2*t)\n        y += e2\n\n        return [x, y]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full Decomposition vs ICD on Sample Data\n\nICD is run on two views of data that each have two dimensions that are\nsinuisoidally related. The data has 100 samples and thus the fully decomposed\nkernel matrix would have dimensions (100, 100). Instead we implement ICD with\na kernel matrix of rank 50 (mrank = 50).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\nXsg = make_data('gaussian', 100)\n\n\ncrossviews_plot(Xsg, ax_ticks=False, ax_labels=True, equal_axes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Full Decomposition\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kcca_g = KCCA(ktype=\"gaussian\", n_components=2, reg=0.01)\nkcca_g.fit(Xsg)\ngausskcca = kcca_g.transform(Xsg)\n\n\ncrossviews_plot(gausskcca, ax_ticks=False, ax_labels=True, equal_axes=True)\n\n\n(gr1, _) = stats.pearsonr(gausskcca[0][:, 0], gausskcca[1][:, 0])\n(gr2, _) = stats.pearsonr(gausskcca[0][:, 1], gausskcca[1][:, 1])\n\nprint(\"Below are the canonical correlation of the two components:\")\nprint(gr1, gr2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ICD Decomposition\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kcca_g_icd = KCCA(ktype=\"gaussian\", sigma=1.0, n_components=2,\n                  reg=0.01, decomp='icd', mrank=50)\nicd_g = kcca_g_icd.fit_transform(Xsg)\n\n\ncrossviews_plot(icd_g, ax_ticks=False, ax_labels=True, equal_axes=True)\n\n\n(icdr1, _) = stats.pearsonr(icd_g[0][:, 0], icd_g[1][:, 0])\n(icdr2, _) = stats.pearsonr(icd_g[0][:, 1], icd_g[1][:, 1])\n\nprint(\"Below are the canonical correlation of the two components:\")\nprint(icdr1, icdr2)\n\n# The canonical correlations of full vs ICD (mrank=50) are very similar!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ICD Kernel Rank vs. Canonical Correlation\n\nWe can observe the relationship between the ICD kernel rank and canonical\ncorrelation of the first canonical component.\n\nWe observe that around rank=10-15 we achieve the same canonical correlation\nas the fully decomposed kernel matrix (rank=100).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "can_corrs = []\nrank = [1, 2, 3, 4, 5, 10, 15, 20, 25, 30, 35, 40,\n        45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n\nfor i in rank:\n    kcca_g_icd = KCCA(ktype=\"gaussian\", sigma=1.0,\n                      n_components=2, reg=0.01, decomp='icd', mrank=i)\n    icd_g = kcca_g_icd.fit_transform(Xsg)\n    (icdr1, _) = stats.pearsonr(icd_g[0][:, 0], icd_g[1][:, 0])\n    can_corrs.append(icdr1)\n\n\nplt.plot(rank, can_corrs)\nplt.xlabel('Rank')\nplt.ylabel('Canonical Correlation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ICD Kernel Rank vs Run-Time\n\nWe can observe the relationship between the ICD kernel rank and run-time to\nfit and transform the two views. We average the run-time of each rank over 5\ntrials.\n\nFrom the rank vs canonical correlation analysis in the previous section, we\ndiscovered that a rank of 10-15 will preserve the canonical correlation\n(accuracy). We can see that at a rank of 10-15, we can get an order of\nmagnitude decrease in run-time compared to a rank of 100\n(full decomposition).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "run_time = []\n\nfor i in rank:\n    run_time_sample = []\n    for a in range(5):\n        kcca_g_icd = KCCA(ktype=\"gaussian\", sigma=1.0,\n                          n_components=2, reg=0.01, decomp='icd', mrank=i)\n        start = time.time()\n        icd_g = kcca_g_icd.fit_transform(Xsg)\n        run_time_sample.append(time.time()-start)\n    run_time.append(sum(run_time_sample) / len(run_time_sample))\n\n\nplt.plot(rank, run_time)\nplt.xlabel('Rank')\nplt.ylabel('Run-Time')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}