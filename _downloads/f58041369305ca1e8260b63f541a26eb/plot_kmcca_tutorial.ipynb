{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Kernel MCCA (KMCCA) Tutorial\n\nKMCCA is a variant of Canonical Correlation Analysis that can use a\nnonlinear kernel to uncover nonlinear correlations between views of data\nand thereby transform data into a lower dimensional space which captures\nthe correlated information between views.\n\nThis tutorial runs KMCCA on two views of data. The kernel implementations,\nparameter 'ktype', are linear, polynomial and gaussian. Polynomial kernel has\ntwo parameters: 'constant', 'degree'. Gaussian kernel has one parameter:\n'sigma'.\n\nUseful information, like canonical correlations between transformed data and\nstatistical tests for significance of these correlations can be computed using\nthe get_stats() function of the KMCCA object.\n\nWhen initializing KMCCA, you can also set the following parameters:\nthe number of canonical components 'n_components', the regularization\nparameter 'reg', the decomposition type 'decomposition', and the decomposition\nmethod 'method'. There are two decomposition types: 'full' and 'icd'. In some\ncases, ICD will run faster than the full decomposition at the cost of\nperformance. The only method as of now is 'kettenring-like'.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Theodore Lee, Ronan Perry\n# License: MIT\n\nimport numpy as np\nfrom mvlearn.embed import KMCCA\nfrom mvlearn.model_selection import train_test_split\nfrom mvlearn.plotting import crossviews_plot\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Function creates Xs, a list of two views of data with a linear relationship,\n# polynomial relationship (2nd degree) and a gaussian (sinusoidal)\n# relationship.\n\n\ndef make_data(kernel, N):\n    # Define two latent variables (number of samples x 1)\n    latvar1 = np.random.randn(N,)\n    latvar2 = np.random.randn(N,)\n\n    # Define independent components for each dataset\n    # (number of observations x dataset dimensions)\n    indep1 = np.random.randn(N, 4)\n    indep2 = np.random.randn(N, 5)\n\n    if kernel == \"linear\":\n        x = 0.25 * indep1 + 0.75 * \\\n            np.vstack((latvar1, latvar2, latvar1, latvar2)).T\n        y = 0.25 * indep2 + 0.75 * \\\n            np.vstack((latvar1, latvar2, latvar1, latvar2, latvar1)).T\n        return [x, y]\n\n    elif kernel == \"poly\":\n        x = 0.25 * indep1 + 0.75 * \\\n            np.vstack((latvar1**2, latvar2**2, latvar1**2, latvar2**2)).T\n        y = 0.25 * indep2 + 0.75 * \\\n            np.vstack((latvar1, latvar2, latvar1, latvar2, latvar1)).T\n        return [x, y]\n\n    elif kernel == \"gaussian\":\n        t = np.random.uniform(-np.pi, np.pi, N)\n        e1 = np.random.normal(0, 0.05, (N, 2))\n        e2 = np.random.normal(0, 0.05, (N, 2))\n\n        x = np.zeros((N, 2))\n        x[:, 0] = t\n        x[:, 1] = np.sin(3*t)\n        x += e1\n\n        y = np.zeros((N, 2))\n        y[:, 0] = np.exp(t/4)*np.cos(2*t)\n        y[:, 1] = np.exp(t/4)*np.sin(2*t)\n        y += e2\n\n        return [x, y]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Kernel\n\nHere we show how KMCCA with a linear kernel can uncover the highly correlated\nlatent distribution of the 2 views which are related with a linear\nrelationship, and then transform the data into that latent space.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\nXs = make_data('linear', 250)\nXs_train, Xs_test = train_test_split(Xs, test_size=0.3, random_state=42)\n\nkmcca = KMCCA(n_components=4, regs=0.01)\nscores = kmcca.fit_transform(Xs_test)\n\ncrossviews_plot(Xs, ax_ticks=False, ax_labels=True, equal_axes=True,\n                title='Simulated data crossplot: linear setting')\n\ncrossviews_plot(scores, ax_ticks=False, ax_labels=True, equal_axes=True,\n                title='Scores crossplot: linear KMCCA')\n\n# Now, we assess the canonical correlations achieved on the testing data\n\nprint(f'Test data canonical correlations: {kmcca.canon_corrs(scores)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Polynomial Kernel\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Here we show how KMCCA with a polynomial kernel can uncover the highly\n# correlated latent distribution of the 2 views which are related with a\n# polynomial relationship, and then transform the data into that latent space.\n\n\nXs = make_data(\"poly\", 250)\nXs_train, Xs_test = train_test_split(Xs, test_size=0.3, random_state=42)\n\nkmcca = KMCCA(\n    kernel=\"poly\", kernel_params={'degree': 2.0, 'coef0': 0.1},\n    n_components=4, regs=0.01)\nscores = kmcca.fit(Xs_train).transform(Xs_test)\n\ncrossviews_plot(Xs, ax_ticks=False, ax_labels=True, equal_axes=True,\n                title='Simulated data crossplot: polynomial setting')\n\ncrossviews_plot(scores, ax_ticks=False, ax_labels=True, equal_axes=True,\n                title='Scores crossplot: polynomial KMCCA')\n\n# Now, we assess the canonical correlations achieved on the testing data\n\nprint(f'Test data canonical correlations: {kmcca.canon_corrs(scores)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gaussian Kernel\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Here we show how KMCCA with a gaussian kernel can uncover the highly\n# correlated latent distribution of the 2 views which are related with a\n# sinusoidal relationship, and then transform the data into that latent space.\n\n\nXs = make_data(\"gaussian\", 250)\nXs_train, Xs_test = train_test_split(Xs, test_size=0.3, random_state=42)\n\nkmcca = KMCCA(\n    kernel=\"rbf\", kernel_params={'gamma': 1}, n_components=2, regs=0.01)\nscores = kmcca.fit(Xs_train).transform(Xs_test)\n\ncrossviews_plot(Xs, ax_ticks=False, ax_labels=True, equal_axes=True,\n                title='Simulated data crossplot: Gaussian setting')\n\ncrossviews_plot(scores, ax_ticks=False, ax_labels=True, equal_axes=True,\n                title='Scores crossplot: Gaussian KMCCA')\n\n# Now, we assess the canonical correlations achieved on the testing data\n\nprint(f'Test data canonical correlations: {kmcca.canon_corrs(scores)}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}