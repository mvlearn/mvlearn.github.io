{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Deep CCA (DCCA) Tutorial\n\nDeep CCA is an extension of Canonical Correlation Analysis which uses 2\nDeep Neural Networks to transform each view into a lower dimensional space\nwhich is highly correlated with the other. This can be used to uncover\nlatent nonlinear relations between views and is often used for feature\nlearning.\n\nThis tutorial uses a synthetic multiview dataset which contains\nlatent information shared between the views, and DCCA is used to uncover\nthis information in a low dimensional embedding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# License: MIT\n\nimport numpy as np\nfrom mvlearn.embed import DCCA\nfrom mvlearn.datasets import GaussianMixture\nfrom mvlearn.plotting import crossviews_plot\nfrom mvlearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Polynomial-Transformed Latent Correlation\n\nLatent variables are sampled from two multivariate Gaussians with equal\nprior probability. Then a polynomial transformation is applied and noise is\nadded independently to both the transformed and untransformed latents.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_samples = 2000\nmeans = [[0, 1], [0, -1]]\ncovariances = [np.eye(2), np.eye(2)]\ngm = GaussianMixture(n_samples, means, covariances, random_state=42,\n                     shuffle=True, shuffle_random_state=42)\nlatent, y = gm.get_Xy(latents=True)\n\n# Plot latent data against itself to reveal the underlying distribtution.\ncrossviews_plot([latent, latent], labels=y,\n                title='Latent Variable', equal_axes=True)\n\n\n# Split data into train and test sets\nXs, y = gm.sample_views(transform='poly', n_noise=2).get_Xy()\nXs_train, Xs_test, y_train, y_test = train_test_split(Xs, y, test_size=0.3,\n                                                      random_state=42)\n\n\n# Plot the testing data after polynomial transformation\ncrossviews_plot(Xs_test, labels=y_test,\n                title='Testing Data View 1 vs. View 2 '\n                      '(Polynomial Transform + noise)',\n                equal_axes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit DCCA Model to Uncover Latent Distribution\n\nThe output dimensionality is still 4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define parameters and layers for deep model\nfeatures1 = Xs_train[0].shape[1]  # Feature sizes\nfeatures2 = Xs_train[1].shape[1]\nlayers1 = [256, 256, 4]  # nodes in each hidden layer and the output size\nlayers2 = [256, 256, 4]\n\ndcca = DCCA(input_size1=features1, input_size2=features2, n_components=4,\n            layer_sizes1=layers1, layer_sizes2=layers2, epoch_num=500)\ndcca.fit(Xs_train)\nXs_transformed = dcca.transform(Xs_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the Transformed Data\n\nWe can see that it has uncovered the latent correlation between views.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "crossviews_plot(Xs_transformed, labels=y_test,\n                title='Transformed Testing Data View 1 vs. View 2 '\n                      '(Polynomial Transform + noise)',\n                equal_axes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sinusoidal-Transformed Latent Correlation\n\nFollowing the same procedure as above, latent variables are sampled from two\nmultivariate Gaussians with equal prior probability. This time, a sinusoidal\ntransformation is applied and noise is added independently to both the\ntransformed and untransformed latents.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_samples = 2000\nmeans = [[0, 1], [0, -1]]\ncovariances = [np.eye(2), np.eye(2)]\ngm = GaussianMixture(n_samples, means, covariances, random_state=42,\n                     shuffle=True, shuffle_random_state=42)\n\n# Split data into train and test segments\nXs, y = gm.sample_views(transform='sin', n_noise=2).get_Xy()\nXs_train, Xs_test, y_train, y_test = train_test_split(Xs, y, test_size=0.3,\n                                                      random_state=42)\n\n# Plot the testing data against itself after polynomial transformation\ncrossviews_plot(Xs_test, labels=y_test,\n                title='Testing Data View 1 vs. View 2 '\n                      '(Polynomial Transform + noise)',\n                equal_axes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit DCCA Model to Uncover Latent Distribution\n\nThe output dimensionality is still 4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define parameters and layers for deep model\nfeatures1 = Xs_train[0].shape[1]  # Feature sizes\nfeatures2 = Xs_train[1].shape[1]\nlayers1 = [256, 256, 4]  # nodes in each hidden layer and the output size\nlayers2 = [256, 256, 4]\n\ndcca = DCCA(input_size1=features1, input_size2=features2, n_components=4,\n            layer_sizes1=layers1, layer_sizes2=layers2, epoch_num=500)\ndcca.fit(Xs_train)\nXs_transformed = dcca.transform(Xs_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the Transformed Data\n\nWe can see that it has uncovered the latent correlation between views.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "crossviews_plot(Xs_transformed, labels=y_test,\n                title='Transformed Testing Data View 1 vs. View 2 '\n                      '(Sinusoidal Transform + noise)',\n                equal_axes=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}