{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Comparing CCA Variants\n\nThis tutorial shows a comparison of Canonical Correlation Analysis (CCA),\nKernel CCA (KCCA) with two different types of kernel, and Deep CCA (DCCA).\nCCA is equivalent to KCCA with a linear kernel. Each learns kernels suitable\nfor different situations. The point of this tutorial is to illustrate, in toy\nexamples, the rough intuition as to when such methods work well and find\nhighly correlated projections.\n\nThe simulated latent data has two signal dimensions draw from independent\nGaussians. Two views of data were derived from this.\n\n- View 1: The latent data.\n- View 2: A transformation of the latent data.\n\nTo each view, two additional independent Gaussian noise dimensions were added.\n\nEach 2x2 grid of subplots in the figure corresponds to a transformation and\neither the raw data or a CCA variant. The x-axes are the data from view 1\nand the y-axes are the data from view 2. Plotted are the correlations between\nthe signal dimensions of the raw views and the top two components of each\nview after a CCA variant transformation. Linearly correlated plots on the\ndiagonals of the 2x2 grids indicate that the CCA method was able to\nsuccessfully learn the underlying functional relationship between the two\nviews.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Ronan Perry\n# License: MIT\n\nfrom mvlearn.embed import CCA, KMCCA, DCCA\nfrom mvlearn.datasets import make_gaussian_mixture\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# GMM settings\nn_samples = 200\ncenters = [[0, 0], [0, 0]]\ncovariances = 4*np.array([np.eye(2), np.eye(2)])\ntransforms = ['linear', 'poly', 'sin']\n\nXs_train_sets = []\nXs_test_sets = []\nfor transform in transforms:\n    Xs_train, _ = make_gaussian_mixture(\n        n_samples, centers, covariances, transform=transform, noise=0.25,\n        noise_dims=2, random_state=41)\n    Xs_test, _, latents = make_gaussian_mixture(\n        n_samples, centers, covariances, transform=transform, noise=0.25,\n        noise_dims=2, random_state=42, return_latents=True)\n\n    Xs_train_sets.append(Xs_train)\n    Xs_test_sets.append(Xs_test)\n\n\n# Plotting parameters\nlabels = latents[:, 0]\ncmap = matplotlib.colors.ListedColormap(\n    sns.diverging_palette(240, 10, n=len(labels), center='light').as_hex())\ncmap = 'coolwarm'\n\nmethod_labels = \\\n    ['Raw Views', 'CCA', 'Polynomial KCCA', 'Gaussian KCCA', 'DCCA']\ntransform_labels = \\\n    ['Linear Transform', 'Polynomial Transform', 'Sinusoidal Transform']\n\ninput_size1 = Xs_train_sets[0][0].shape[1]\ninput_size2 = Xs_train_sets[0][1].shape[1]\noutdim_size = min(Xs_train_sets[0][0].shape[1], 2)\nlayer_sizes1 = [256, 256, outdim_size]\nlayer_sizes2 = [256, 256, outdim_size]\nmethods = [\n    CCA(regs=0.1, n_components=2),\n    KMCCA(kernel='poly', regs=0.1, kernel_params={'degree': 2, 'coef0': 0.1},\n          n_components=2),\n    KMCCA(kernel='rbf', regs=0.1, kernel_params={'gamma': 1/4},\n          n_components=2),\n    DCCA(input_size1, input_size2, outdim_size, layer_sizes1, layer_sizes2,\n         epoch_num=400)\n]\n\nfig, axes = plt.subplots(3 * 2, 5 * 2, figsize=(22, 12))\nsns.set_context('notebook')\n\nfor r, transform in enumerate(transforms):\n    axs = axes[2 * r:2 * r + 2, :2]\n    for i, ax in enumerate(axs.flatten()):\n        dim2 = int(i / 2)\n        dim1 = i % 2\n        ax.scatter(\n            Xs_test_sets[r][0][:, dim1],\n            Xs_test_sets[r][1][:, dim2],\n            cmap=cmap,\n            c=labels,\n        )\n        ax.set_xticks([], [])\n        ax.set_yticks([], [])\n        if dim1 == 0:\n            ax.set_ylabel(f\"View 2 Dim {dim2+1}\", fontsize=14)\n        if dim1 == 0 and dim2 == 0:\n            ax.text(-0.4, -0.1, transform_labels[r], transform=ax.transAxes,\n                    fontsize=22, rotation=90, verticalalignment='center')\n        if dim2 == 1 and r == len(transforms)-1:\n            ax.set_xlabel(f\"View 1 Dim {dim1+1}\", fontsize=14)\n        if i == 0 and r == 0:\n            ax.set_title(method_labels[r],\n                         {'position': (1.11, 1), 'fontsize': 22})\n\n    for c, method in enumerate(methods):\n        axs = axes[2*r: 2*r+2, 2*c+2:2*c+4]\n        Xs = method.fit(Xs_train_sets[r]).transform(Xs_test_sets[r])\n        for i, ax in enumerate(axs.flatten()):\n            dim2 = int(i / 2)\n            dim1 = i % 2\n            ax.scatter(\n                Xs[0][:, dim1],\n                Xs[1][:, dim2],\n                cmap=cmap,\n                c=labels,\n            )\n            if dim2 == 1 and r == len(transforms)-1:\n                ax.set_xlabel(f\"View 1 Dim {dim1+1}\", fontsize=16)\n            if i == 0 and r == 0:\n                ax.set_title(method_labels[c + 1], {'position': (1.11, 1),\n                             'fontsize': 22})\n            ax.axis(\"equal\")\n            ax.set_xticks([], [])\n            ax.set_yticks([], [])\n\nplt.tight_layout()\nplt.subplots_adjust(wspace=0.15, hspace=0.15)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}