{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Kernel CCA (KCCA) Tutorial\n\nKCCA is a variant of Canonical Correlation Analysis that can use a\nnonlinear kernel to uncover nonlinear correlations between views of data\nand thereby transform data into a lower dimensional space which captures\nthe correlated information between views.\n\nThis tutorial runs KCCA on two views of data. The kernel implementations,\nparameter 'ktype', are linear, polynomial and gaussian. Polynomial kernel has\ntwo parameters: 'constant', 'degree'. Gaussian kernel has one parameter:\n'sigma'.\n\nUseful information, like canonical correlations between transformed data and\nstatistical tests for significance of these correlations can be computed using\nthe get_stats() function of the KCCA object.\n\nWhen initializing KCCA, you can also set the following parameters:\nthe number of canonical components 'n_components', the regularization\nparameter 'reg', the decomposition type 'decomposition', and the decomposition\nmethod 'method'. There are two decomposition types: 'full' and 'icd'. In some\ncases, ICD will run faster than the full decomposition at the cost of\nperformance. The only method as of now is 'kettenring-like'.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom mvlearn.embed import KCCA\nfrom mvlearn.model_selection import train_test_split\nfrom mvlearn.plotting import crossviews_plot\n\n\n# Function creates Xs, a list of two views of data with a linear relationship,\n# polynomial relationship (2nd degree) and a gaussian (sinusoidal)\n# relationship.\n\n\ndef make_data(kernel, N):\n    # Define two latent variables (number of samples x 1)\n    latvar1 = np.random.randn(N,)\n    latvar2 = np.random.randn(N,)\n\n    # Define independent components for each dataset\n    # (number of observations x dataset dimensions)\n    indep1 = np.random.randn(N, 4)\n    indep2 = np.random.randn(N, 5)\n\n    if kernel == \"linear\":\n        x = 0.25 * indep1 + 0.75 * \\\n            np.vstack((latvar1, latvar2, latvar1, latvar2)).T\n        y = 0.25 * indep2 + 0.75 * \\\n            np.vstack((latvar1, latvar2, latvar1, latvar2, latvar1)).T\n        return [x, y]\n\n    elif kernel == \"poly\":\n        x = 0.25 * indep1 + 0.75 * \\\n            np.vstack((latvar1**2, latvar2**2, latvar1**2, latvar2**2)).T\n        y = 0.25 * indep2 + 0.75 * \\\n            np.vstack((latvar1, latvar2, latvar1, latvar2, latvar1)).T\n        return [x, y]\n\n    elif kernel == \"gaussian\":\n        t = np.random.uniform(-np.pi, np.pi, N)\n        e1 = np.random.normal(0, 0.05, (N, 2))\n        e2 = np.random.normal(0, 0.05, (N, 2))\n\n        x = np.zeros((N, 2))\n        x[:, 0] = t\n        x[:, 1] = np.sin(3*t)\n        x += e1\n\n        y = np.zeros((N, 2))\n        y[:, 0] = np.exp(t/4)*np.cos(2*t)\n        y[:, 1] = np.exp(t/4)*np.sin(2*t)\n        y += e2\n\n        return [x, y]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear kernel implementation\n\nHere we show how KCCA with a linear kernel can uncover the highly correlated\nlatent distribution of the 2 views which are related with a linear\nrelationship, and then transform the data into that latent space. We use an\n80-20, train-test data split to develop the embedding.\n\nAlso, we use statistical tests (Wilk's Lambda) to check the significance of\nthe canonical correlations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\nXs = make_data('linear', 250)\nXs_train, Xs_test = train_test_split(Xs, test_size=0.3, random_state=42)\n\nkcca_l = KCCA(n_components=4, reg=0.01)\nkcca_l.fit(Xs_train)\nlinearkcca = kcca_l.transform(Xs_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Original Data Plotted\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "crossviews_plot(Xs, ax_ticks=False, ax_labels=True, equal_axes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformed Test Data Plotted\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "crossviews_plot(linearkcca, ax_ticks=False, ax_labels=True, equal_axes=True)\n\n# Now, we assess the canonical correlations achieved on the testing data, and\n# their significance using the p-values from a Wilk's Lambda test\n\n\ncca_stats = kcca_l.get_stats()\nprint(cca_stats['r'])\nprint(cca_stats['pF'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Polynomial kernel implementation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Here we show how KCCA with a polynomial kernel can uncover the highly\n# correlated latent distribution of the 2 views which are related with a\n# polynomial relationship, and then transform the data into that latent space.\n\n\nXsp = make_data(\"poly\", 250)\nXsp_train, Xsp_test = train_test_split(Xsp, test_size=0.3, random_state=42)\n\nkcca_p = KCCA(ktype=\"poly\", degree=2.0, n_components=4, reg=0.001)\nkcca_p.fit(Xsp_train)\npolykcca = kcca_p.transform(Xsp_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Original Data Plotted\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "crossviews_plot(Xsp, ax_ticks=False, ax_labels=True, equal_axes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformed Test Data Plotted\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "crossviews_plot(polykcca, ax_ticks=False, ax_labels=True, equal_axes=True)\n\n# Now, we assess the canonical correlations achieved on the testing data by\n# printing the canonical correlations for each component\n\n\nkcca_stats = kcca_p.get_stats()\nprint(kcca_stats['r'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gaussian Kernel Implementation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Here we show how KCCA with a gaussian kernel can uncover the highly\n# correlated latent distribution of the 2 views which are related with a\n# sinusoidal relationship, and then transform the data into that latent space.\n\n\nXsg = make_data(\"gaussian\", 250)\nXsg_train, Xsg_test = train_test_split(Xsg, test_size=0.3, random_state=42)\n\nkcca_g = KCCA(ktype=\"gaussian\", sigma=1.0, n_components=4, reg=0.01)\nkcca_g.fit(Xsg_train)\ngausskcca = kcca_g.transform(Xsg_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Original Data Plotted\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "crossviews_plot(Xsg, ax_ticks=False, ax_labels=True, equal_axes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformed Test Data Plotted\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "crossviews_plot(gausskcca, ax_ticks=False, ax_labels=True, equal_axes=True)\n\n# Now, we assess the canonical correlations achieved on the testing data by\n# printing the canonical correlations for each component\n\n\nkcca_stats = kcca_g.get_stats()\nprint(kcca_stats['r'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}