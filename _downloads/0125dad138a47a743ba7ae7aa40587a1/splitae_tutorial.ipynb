{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# SplitAE embeddings on multiview MNIST data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport PIL\n\n# tsnecuda is a bit harder to install, if you want to use MulticoreTSNE instead\n# (sklearn is too slow)\n# then uncomment the below MulticoreTSNE line, comment out the tsnecuda line,\n# and replace\n# all TSNE() lines with TSNE(n_jobs=12), where 12 is replaced with the number\n# of cores on your machine\n\n# from MulticoreTSNE import MulticoreTSNE as TSNE\nfrom tsnecuda import TSNE\nfrom mvlearn.embed import SplitAE\n\nplt.style.use(\"default\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup the multiview MNIST data\n\nLet's make a simple two view dataset based on MNIST as described in\nhttp://proceedings.mlr.press/v37/wangb15.pdf .\n\nThe \"underlying data\" of our views is a digit from 0-9 -- e.g. \"2\" or \"7\" or\n\"9\".\n\nThe first view of this underlying data is a random MNIST image with the\ncorrect digit, rotated randomly +- 45 degrees.\n\nThe second view of this underlying data is another random MNIST image (not\nrotated) with the correct digit, but with the addition of uniform noise from\n[0,1]\n\nAn example point of our data is:\n\n- view1: an MNIST image with the label \"9\"\n- view2: a different MNIST image with the label \"9\" with noise added.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class NoisyMnist(Dataset):\n\n    MNIST_MEAN, MNIST_STD = (0.1307, 0.3081)\n\n    def __init__(self, train=True):\n        super().__init__()\n        self.mnistDataset = datasets.MNIST(\n            \"./mnist\", train=train, download=True)\n\n    def __len__(self):\n        return len(self.mnistDataset)\n\n    def __getitem__(self, idx):\n        def randomIndex(): return np.random.randint(len(self.mnistDataset))\n        image1, label1 = self.mnistDataset[idx]\n        image2, label2 = self.mnistDataset[randomIndex()]\n        while not label1 == label2:\n            image2, label2 = self.mnistDataset[randomIndex()]\n\n        image1 = torchvision.transforms.RandomRotation(\n            (-45, 45), resample=PIL.Image.BICUBIC)(image1)\n        image1 = np.array(image1) / 255\n        image2 = np.array(image2) / 255\n\n        # add noise to the view2 image\n        image2 = np.clip(image2 +\n                         np.random.uniform(0, 1, size=image2.shape), 0, 1)\n\n        # standardize both images\n        image1 = (image1 - self.MNIST_MEAN) / self.MNIST_STD\n        image2 = (image2 - (self.MNIST_MEAN+0.447)) / self.MNIST_STD\n\n        image1 = torch.FloatTensor(image1).unsqueeze(0)  # image1 is view1\n        image2 = torch.FloatTensor(image2).unsqueeze(0)  # image2 is view2\n\n        return (image1, image2, label1)\n\n# Let's look at this datset we made. The first row is view1 and the second row\n# is the corresponding view2.\n\n\ndataset = NoisyMnist()\nprint(\"Dataset length is\", len(dataset))\ndataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=8)\nview1, view2, labels = next(iter(dataloader))\n\nview1Row = torch.cat([*view1.squeeze()], dim=1)\nview2Row = torch.cat([*view2.squeeze()], dim=1)\n# make between 0 and 1 again:\nview1Row = (view1Row - torch.min(view1Row)) / \\\n    (torch.max(view1Row) - torch.min(view1Row))\nview2Row = (view2Row - torch.min(view2Row)) / \\\n    (torch.max(view2Row) - torch.min(view2Row))\nplt.imshow(torch.cat([view1Row, view2Row], dim=0))\n\n# Sklearn API doesn't use Dataloaders\n# so let's get our dataset into a different format. Each view will be an array\n# of the shape (nSamples, nFeatures). We will do the same for the test dataset.\n\n\n# since batch_size=len(dataset), we get the full dataset with one\n# next(iter(dataset)) call\ndataloader = DataLoader(dataset, batch_size=len(\n    dataset), shuffle=True, num_workers=8)\nview1, view2, labels = next(iter(dataloader))\nview1 = view1.view(view1.shape[0], -1)\nview2 = view2.view(view2.shape[0], -1)\n\ntestDataset = NoisyMnist(train=False)\nprint(\"Test dataset length is\", len(testDataset))\ntestDataloader = DataLoader(\n    testDataset, batch_size=10000, shuffle=True, num_workers=8)\ntestView1, testView2, testLabels = next(iter(testDataloader))\ntestView1 = testView1.view(testView1.shape[0], -1)\ntestView2 = testView2.view(testView2.shape[0], -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run SplitAE\n\nSplitAE does two things. It creates a shared embedding for view1 and view2.\nAnd it allows predicting view2 from view1. The autoencoder network takes in\nview1 as input, squeezes it into a low-dimensional representation, and then\nfrom that low-dimensional representation (the embedding), it tries to\nrecreate view1 and predict view2. Let's see that:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "splitae = SplitAE(hidden_size=1024, num_hidden_layers=2, embed_size=10,\n                  training_epochs=10, batch_size=128, learning_rate=0.001,\n                  print_info=False, print_graph=True)\nsplitae.fit([view1, view2], validation_Xs=[testView1, testView2])\n# if the named parameter validationXs is passed with held-out data, then .fit\n# will print validation error as well.\n\n# We can see from the graph that test error did not diverge from train error,\n# which means we're not overfitting, which is good! Let's see the actual view1\n# recreation and the view2 prediction on test data:\n\n\nMNIST_MEAN, MNIST_STD = (0.1307, 0.3081)\ntestEmbed, testView1Reconstruction, testView2Prediction = \\\n    splitae.transform([testView1, testView2])\n\nnumImages = 8\nrandIndices = np.random.choice(\n    range(len(testDataset)), numImages, replace=False)\n\n\ndef plotRow(title, view):\n    samples = view[randIndices].reshape(-1, 28, 28)\n    row = np.concatenate([*samples], axis=1)\n    row = np.clip(row * MNIST_STD + MNIST_MEAN, 0, 1)  # denormalize\n    plt.imshow(row)\n    plt.title(title)\n    plt.show()\n\n\nplotRow(\"view 1\", testView1)\nplotRow(\"reconstructed view 1\", testView1Reconstruction)\nplotRow(\"predicted view 2\", testView2Prediction)\n\n# Notice the view 2 predictions. Had our view2 images been randomly rotated,\n# the predictions would have a hazy circle, since the best guess would be the\n# mean of all the rotated digits. Since we don't rotate our view2 images, we\n# instead get something that's only a bit hazy around the edges -- corresonding\n# to the mean of all the non-rotated digits.\n\n# Next let's visualize our 20d test embeddings with T-SNE and see if they\n# represent our original underlying representation -- the digits from 0-9 -- of\n# which we made two views of. In the perfect scenario, each of the 10,000\n# vectors of our test embedding would be one of ten vectors, representing the\n# digits from 0-9. (Our network wouldn't do this, as it tries to reconstruct\n# each unique view1 image exactly). In lieu of this we can hope for embedding\n# vectors corresponding to the same digits to be closer together.\n\n\ntsne = TSNE()\ntsneEmbeddings = tsne.fit_transform(testEmbed)\n\n\ndef plot2DEmbeddings(embeddings, labels):\n    pointColors = []\n    origColors = [\n        [55, 55, 55], [255, 34, 34], [38, 255, 38],\n        [10, 10, 255], [255, 12, 255], [250, 200, 160],\n        [120, 210, 180], [150, 180, 205], [210, 160, 210],\n        [190, 190, 110]\n        ]\n    origColors = (np.array(origColors)) / 255\n    for l in labels.cpu().numpy():\n        pointColors.append(tuple(origColors[l].tolist()))\n\n    fig, ax = plt.subplots()\n    for i, label in enumerate(np.unique(labels)):\n        idxs = np.where(testLabels == label)\n        ax.scatter(embeddings[idxs][:, 0], embeddings[idxs]\n                   [:, 1], c=[origColors[i]], label=i, s=5)\n\n    legend = plt.legend(loc=\"lower left\")\n    for handle in legend.legendHandles:\n        handle.set_sizes([30.0])\n    plt.show()\n\n\nplot2DEmbeddings(tsneEmbeddings, testLabels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check reconstruction with tSNE\nLets check the variability of multiple TSNE runs:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n    tsneEmbeddings = tsne.fit_transform(testEmbed)\n    plot2DEmbeddings(tsneEmbeddings, testLabels)\n\n# Now let's check the variability of both training the model plus TSNE-ing the\n# test embeddings.\n\n\nfor i in range(3):\n\n    splitae = SplitAE(hidden_size=1024, num_hidden_layers=2, embed_size=10,\n                      training_epochs=12, batch_size=128, learning_rate=0.001,\n                      print_info=False, print_graph=True)\n    splitae.fit([view1, view2])\n\n    testEmbed, testView1Reconstruction, testView2Reconstruction = \\\n        splitae.transform([testView1, testView2])\n\n    tsneEmbeddings = tsne.fit_transform(testEmbed)\n    plot2DEmbeddings(tsneEmbeddings, testLabels)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}