{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# CCA Tutorial\n\nThis tutorial demonstrates the use of CCA for 2 views and multiview CCA (MCCA)\nfor more than 2 views. As is demonstrated, they allow for both the addition of\nregularization as well as the use of a kernel to compute the distance\n(Gram) matrices (KMCCA).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Iain Carmichael, Ronan Perry\n# License: MIT\n\nfrom mvlearn.datasets import sample_joint_factor_model\nfrom mvlearn.embed import CCA, MCCA, KMCCA\nfrom mvlearn.plotting import crossviews_plot\n\n\nn_views = 3\nn_samples = 1000\nn_features = [10, 20, 30]\njoint_rank = 3\n\n# sample 3 views of data from a joint factor model\n# m, noise_std control the difficulty of the problem\nXs, U_true, Ws_true = sample_joint_factor_model(\n    n_views=n_views, n_samples=n_samples, n_features=n_features,\n    joint_rank=joint_rank, m=5, noise_std=1, random_state=23,\n    return_decomp=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CCA\n\nCCA, equivalent to 2 view MCCA, learns transformations of the views,\nprojecting a linear combination of the features to a component such that the\nsum of correlations between the ith components of each view is maximized. We\nsee the top three components of the first two views plotted against each\nother, pairwise. The strong linear shape on the diagonals shows that the\nfound components correlate well.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# the default is no regularization meaning this is SUMCORR-AVGVAR MCCA\ncca = CCA(n_components=joint_rank)\n\n# the fit-transform method outputs the scores for each view\ncca_scores = cca.fit_transform(Xs[:2])\ncrossviews_plot(cca_scores,\n                title='CCA scores (first two views fitted)',\n                equal_axes=True,\n                scatter_kwargs={'alpha': 0.4, 's': 2.0})\n\n# In the 2 view setting, a variety of interpretable statistics can be\n# calculated. We assess the canonical correlations achieved and\n# their significance using the p-values from a Wilk's Lambda test\n\nstats = cca.stats(cca_scores)\nprint(f'Canonical Correlations: {stats[\"r\"]}')\nprint(f'Wilk\\'s Lambda Test pvalues: {stats[\"pF\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regularized CCA\n\nWe can add regularization with the `regs` argument to handle\nhigh-dimensional data. This data is simple, and so it makes little\ndifference. Here, we use MCCA for all 3 views.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# regularization value of .5 for each view\nmcca = MCCA(n_components=joint_rank, regs=0.5)\n\n# the fit-transform method outputs the scores for each view\nmcca_scores = mcca.fit_transform(Xs)\ncrossviews_plot(mcca_scores[[0, 1]],\n                title='MCCA scores with regularization (first 2 views shown)',\n                equal_axes=True,\n                scatter_kwargs={'alpha': 0.4, 's': 2.0})\n\nprint('Canonical Correlations:')\nprint(mcca.canon_corrs(mcca_scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Informative MCCA: PCA then MCCA\n\nWe can also handle high-dimensional data with i-MCCA. We first compute a\nlow rank PCA for each view, then run MCCA on the reduced data. With rank 2\nPCA, only the first two CCA components are informative, as we can see in the\nplot.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# i-MCCA where we first extract the first 2 PCs from each data view\nmcca = MCCA(n_components=joint_rank, signal_ranks=[2, 2, 2])\n\nmcca_scores = mcca.fit_transform(Xs)\ncrossviews_plot(mcca_scores[[0, 1]],\n                title='PCA-MCCA scores: rank 2 PCA (first 2 views shown)',\n                equal_axes=True,\n                scatter_kwargs={'alpha': 0.4, 's': 2.0})\n\nprint('Canonical Correlations:')\nprint(mcca.canon_corrs(mcca_scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kernel MCCA\n\nWe can compute kernel MCCA with the KMCCA() object. With the linear kernel,\nthis behaves just like MCCA.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# fit kernel MCCA with a linear kernel\nkmcca = KMCCA(n_components=joint_rank, kernel='linear')\n\nkmcca_scores = kmcca.fit_transform(Xs)\ncrossviews_plot(kmcca_scores[[0, 1]],\n                title='KMCCA scores: linear kernel (first 2 views shown)',\n                equal_axes=True,\n                scatter_kwargs={'alpha': 0.4, 's': 2.0})\n\nprint('Canonical Correlations:')\nprint(mcca.canon_corrs(mcca_scores))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}