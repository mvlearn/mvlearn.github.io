{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Multiview CCA Tutorial\n\nThis tutorial demonstrates the use of multiview CCA, which allows for greater\nthan two views. As is demonstrated, it allows for both the addition of\nregularization as well as the use of a kernel to compute the distance\n(Gram) matrices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Iain Carmichael, Ronan Perry\n# License: MIT\n\nfrom mvlearn.datasets import sample_joint_factor_model\nfrom mvlearn.embed import MCCA, KMCCA\nfrom mvlearn.plotting import crossviews_plot\n\n\nn_views = 3\nn_samples = 1000\nn_features = [10, 20, 30]\njoint_rank = 3\n\n# sample data from a joint factor model\n# m, noise_std control the difficulty of the problem\nXs, U_true, Ws_true = sample_joint_factor_model(\n    n_views=n_views, n_samples=n_samples, n_features=n_features,\n    joint_rank=joint_rank, m=5, noise_std=1, random_state=23,\n    return_decomp=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multiview CCA (MCCA)\n\nMCCA learns transformations of the views, projecting a linear combination\nof the features to a component such that the sum of correlations between\nthe ith components of each view is maximized. We see the top three\ncomponents of the first two views plotted against each other, pairwise.\nThe strong linear shape on the diagonals shows that the found components\ncorrelate well.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# the default is no regularization meaning this is SUMCORR-AVGVAR MCCA\nmcca = MCCA(n_components=joint_rank)\n\n# the fit-transform method outputs the scores for each view\nmcca_scores = mcca.fit_transform(Xs)\ncrossviews_plot(mcca_scores[[0, 1]],\n                title='MCCA embedding dimensions of the first two views',\n                equal_axes=True,\n                scatter_kwargs={'alpha': 0.4, 's': 2.0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regularized MCCA\n\nWe can add regularization with the `regs` argument to handle\nhigh-dimensional data. This data is simple, and so it makes little\ndifference.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# regularization value of .5 for each view\nmcca = MCCA(n_components=joint_rank, regs=0.5)\n\n# the fit-transform method outputs the scores for each view\nmcca_scores = mcca.fit_transform(Xs)\ncrossviews_plot(mcca_scores[[0, 1]],\n                title='MCCA embedding with regularization',\n                equal_axes=True,\n                scatter_kwargs={'alpha': 0.4, 's': 2.0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Informative MCCA: PCA then MCCA\n\nWe can also handle high-dimensional data with i-MCCA. We first compute a\nlow rank PCA for each view, then run MCCA on the reduced data. With rank 2\nPCA, only the first two CCA components are informative, as we can see in the\nplot.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# i-MCCA where we first extract the first 2 PCs from each data view\nmcca = MCCA(n_components=joint_rank, signal_ranks=[2, 2, 2])\n\nmcca_scores = mcca.fit_transform(Xs)\ncrossviews_plot(mcca_scores[[0, 1]],\n                title='PCA-MCCA embeddings: rank 2 PCA',\n                equal_axes=True,\n                scatter_kwargs={'alpha': 0.4, 's': 2.0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kernel MCCA\n\nWe can compute kernel MCCA with the KMCCA() object. With the linear kernel,\nthis behaves just like MCCA.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# fit kernel MCCA with a linear kernel\nkmcca = KMCCA(n_components=joint_rank, kernel='linear')\n\nkmcca_scores = kmcca.fit_transform(Xs)\ncrossviews_plot(kmcca_scores[[0, 1]],\n                title='KMCCA embeddings',\n                equal_axes=True,\n                scatter_kwargs={'alpha': 0.4, 's': 2.0})"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}