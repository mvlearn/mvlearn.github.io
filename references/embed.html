

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Embedding &mdash; mvlearn alpha documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-rendered-html.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Decomposition" href="decomposition.html" />
    <link rel="prev" title="Reference" href="index.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/mvlearn-logo-transparent-white.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.4.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Using mvlearn</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../install.html">Install</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../install.html#installing-the-released-version-with-pip">Installing the released version with pip</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../install.html#including-optional-dependencies-for-full-functionality">Including optional dependencies for full functionality</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#installing-the-released-version-with-conda-forge">Installing the released version with conda-forge</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#python-package-dependencies">Python package dependencies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#hardware-requirements">Hardware requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#os-requirements">OS Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#testing">Testing</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../auto_examples/index.html">Examples Gallery</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#examples-on-cluster">Examples on cluster</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/cluster/plot_mv_coregularized_spectral_tutorial.html">Multiview Coregularized Spectral Clustering Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/cluster/plot_mv_spherical_kmeans_tutorial.html">Multiview Spherical KMeans Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/cluster/plot_mv_kmeans_tutorial.html">Multiview KMeans Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/cluster/plot_mv_kmeans_validation_simulated.html">Multiview vs. Singleview KMeans</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/cluster/plot_mv_spectral_tutorial.html">Multiview Spectral Clustering Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/cluster/plot_mv_spectral_validation_simulated.html">Multiview vs. Singleview Spectral Clustering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/cluster/plot_mv_vs_singleview_spectral.html">Multiview vs. Singleview Spectral Clustering of UCI Multiview Digits</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/cluster/plot_mv_spectral_validation_complex.html">Conditional Independence of Views on Multiview Spectral Clustering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/cluster/plot_mv_kmeans_validation_complex.html">Conditional Independence of Views on Multiview KMeans Clustering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#examples-on-compose">Examples on compose</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/compose/plot_multiview_construction.html">Constructing multiple views to classify singleview data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/compose/plot_pipeline_sklearn_integration.html">Integrating mvlearn with scikit-learn</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#examples-on-datasets">Examples on datasets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/datasets/plot_load_ucimultifeature.html">Loading and Viewing the UCI Multiple Features Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/datasets/plot_gaussianmixtures.html">Generating Multiview Data from Gaussian Mixtures</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#examples-on-decomposition">Examples on decomposition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/decomposition/plot_group_ica_tutorial.html">ICA: a tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/decomposition/plot_mv_ica_tutorial.html">Multiview Independent Component Analysis (ICA) Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/decomposition/plot_ajive_tutorial.html">Angle-based Joint and Individual Variation Explained (AJIVE)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#examples-on-embed">Examples on embed</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/embed/plot_gcca_tutorial.html">Generalized Canonical Correlation Analysis (GCCA) Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/embed/plot_mcca_tutorial.html">CCA Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/embed/plot_dcca_tutorial.html">Deep CCA (DCCA) Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/embed/plot_omnibus_embedding.html">Omnbius Graph Embedding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/embed/plot_kmcca_pgso_tutorial.html">Partial Gram-Schmidt Orthogonalization (PGSO) for KMCCA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/embed/plot_mvmds_tutorial.html">Multidimensional Scaling (MVMDS) Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/embed/plot_cca_comparison.html">Comparing CCA Variants</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/embed/plot_kmcca_tutorial.html">Kernel MCCA (KMCCA) Tutorial</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#examples-on-plotting">Examples on plotting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plotting/plot_quick_visualize_tutorial.html">Quickly Visualizing Multiview Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plotting/plot_crossviews_plot.html">Plotting Multiview Data with a Cross-view Plot</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#examples-on-semi-supervised">Examples on semi_supervised</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/semi_supervised/plot_cotraining_regression.html">2-View Semi-Supervised Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/semi_supervised/plot_cotraining_classification.html">2-View Semi-Supervised Classification</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Reference</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Embedding</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#canonical-correlation-analysis-cca">Canonical Correlation Analysis (CCA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multiview-canonical-correlation-analysis-mcca">Multiview Canonical Correlation Analysis (MCCA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#kernel-mcca">Kernel MCCA</a></li>
<li class="toctree-l3"><a class="reference internal" href="#generalized-canonical-correlation-analysis-gcca">Generalized Canonical Correlation Analysis (GCCA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deep-canonical-correlation-analysis-dcca">Deep Canonical Correlation Analysis (DCCA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#omnibus-embedding">Omnibus Embedding</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multiview-multidimensional-scaling">Multiview Multidimensional Scaling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#split-autoencoder">Split Autoencoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dcca-utilities">DCCA Utilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dimension-selection">Dimension Selection</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="decomposition.html">Decomposition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="decomposition.html#multiview-ica">Multiview ICA</a></li>
<li class="toctree-l3"><a class="reference internal" href="decomposition.html#group-ica">Group ICA</a></li>
<li class="toctree-l3"><a class="reference internal" href="decomposition.html#group-pca">Group PCA</a></li>
<li class="toctree-l3"><a class="reference internal" href="decomposition.html#angle-based-joint-and-individual-variation-explained-ajive">Angle-Based Joint and Individual Variation Explained (AJIVE)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cluster.html">Clustering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cluster.html#multiview-spectral-clustering">Multiview Spectral Clustering</a></li>
<li class="toctree-l3"><a class="reference internal" href="cluster.html#co-regularized-multiview-spectral-clustering">Co-Regularized Multiview Spectral Clustering</a></li>
<li class="toctree-l3"><a class="reference internal" href="cluster.html#multiview-k-means">Multiview K Means</a></li>
<li class="toctree-l3"><a class="reference internal" href="cluster.html#multiview-spherical-k-means">Multiview Spherical K Means</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="semi_supervised.html">Semi-Supervised</a><ul>
<li class="toctree-l3"><a class="reference internal" href="semi_supervised.html#cotraining-classifier">Cotraining Classifier</a></li>
<li class="toctree-l3"><a class="reference internal" href="semi_supervised.html#cotraining-regressor">Cotraining Regressor</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_selection.html">Model Selection</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_selection.html#cross-validation">Cross Validation</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_selection.html#train-test-split">Train-Test Split</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="compose.html">Compose</a><ul>
<li class="toctree-l3"><a class="reference internal" href="compose.html#averagemerger">AverageMerger</a></li>
<li class="toctree-l3"><a class="reference internal" href="compose.html#concatmerger">ConcatMerger</a></li>
<li class="toctree-l3"><a class="reference internal" href="compose.html#randomgaussianprojection">RandomGaussianProjection</a></li>
<li class="toctree-l3"><a class="reference internal" href="compose.html#randomsubspacemethod">RandomSubspaceMethod</a></li>
<li class="toctree-l3"><a class="reference internal" href="compose.html#simplesplitter">SimpleSplitter</a></li>
<li class="toctree-l3"><a class="reference internal" href="compose.html#viewclassifier">ViewClassifier</a></li>
<li class="toctree-l3"><a class="reference internal" href="compose.html#viewtransformer">ViewTransformer</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="datasets.html">Multiview Datasets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="datasets.html#uci-multiple-feature-dataset-located-here">UCI multiple feature dataset (located here)</a></li>
<li class="toctree-l3"><a class="reference internal" href="datasets.html#data-simulator">Data Simulator</a></li>
<li class="toctree-l3"><a class="reference internal" href="datasets.html#factor-model">Factor Model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="plotting.html">Plotting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="plotting.html#quick-visualize">Quick Visualize</a></li>
<li class="toctree-l3"><a class="reference internal" href="plotting.html#crossviews-plot">Crossviews Plot</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="utils.html">Utility Functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="utils.html#io">IO</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Developer Information</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">Contributing to mvlearn</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#submitting-a-bug-report-or-a-feature-request">Submitting a bug report or a feature request</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#how-to-make-a-good-bug-report">How to make a good bug report</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#contributing-code">Contributing Code</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#pull-request-checklist">Pull Request Checklist</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#guidelines">Guidelines</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#coding-guidelines">Coding Guidelines</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#docstring-guidelines">Docstring Guidelines</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#api-of-mvlearn-objects">API of mvlearn Objects</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#estimators">Estimators</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#additional-functionality">Additional Functionality</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#version-0-4-1">Version 0.4.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#version-0-4-0">Version 0.4.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../changelog.html#id2">mvlearn.compose</a></li>
<li class="toctree-l3"><a class="reference internal" href="../changelog.html#id10">mvlearn.construct</a></li>
<li class="toctree-l3"><a class="reference internal" href="../changelog.html#id12">mvlearn.decomposition</a></li>
<li class="toctree-l3"><a class="reference internal" href="../changelog.html#id14">mvlearn.embed</a></li>
<li class="toctree-l3"><a class="reference internal" href="../changelog.html#id18">mvlearn.model_selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../changelog.html#id21">mvlearn.utils</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#version-0-3-0">Version 0.3.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#patch-0-2-1">Patch 0.2.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#version-0-2-0">Version 0.2.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#version-0-1-0">Version 0.1.0</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>
<p class="caption"><span class="caption-text">Useful Links</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/mvlearn/mvlearn">mvlearn &#64; GitHub</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pypi.org/project/mvlearn/">mvlearn &#64; PyPI</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/mvlearn/mvlearn/issues">Issue Tracker</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">mvlearn</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">Reference</a> &raquo;</li>
        
      <li>Embedding</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mvlearn/mvlearn/blob/master/docs/references/embed.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="embedding">
<h1>Embedding<a class="headerlink" href="#embedding" title="Permalink to this headline">¶</a></h1>
<div class="section" id="canonical-correlation-analysis-cca">
<h2>Canonical Correlation Analysis (CCA)<a class="headerlink" href="#canonical-correlation-analysis-cca" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="mvlearn.embed.CCA">
<em class="property">class </em><code class="descclassname">mvlearn.embed.</code><code class="descname">CCA</code><span class="sig-paren">(</span><em>n_components=1</em>, <em>regs=None</em>, <em>signal_ranks=None</em>, <em>center=True</em>, <em>i_mcca_method='auto'</em>, <em>multiview_output=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/cca.html#CCA"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.CCA" title="Permalink to this definition">¶</a></dt>
<dd><p>Canonical Correlation Analysis (CCA)</p>
<p>CCA inherits from MultiCCA (MCCA) but is restricted to 2 views which
allows for certain statistics to be computed about the results.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_components</strong> (<em>int | 'min' | 'max' | None</em><em> (</em><em>default 1</em><em>)</em>) -- Number of final components to compute. If <cite>int</cite>, will compute that
many. If None, will compute as many as possible. 'min' and 'max' will
respectively use the minimum/maximum number of features among views.</li>
<li><strong>regs</strong> (<em>float | 'lw' | 'oas' | None</em><em>, or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)"><em>list</em></a><em>, </em><em>optional</em><em> (</em><em>default None</em><em>)</em>) -- <p>MCCA regularization for each data view, which can be important
for high dimensional data. A list will specify for each view
separately. If float, must be between 0 and 1 (inclusive).</p>
<ul>
<li>0 or None : corresponds to SUMCORR-AVGVAR MCCA.</li>
<li>1 : partial least squares SVD (generalizes to more than 2 views)</li>
<li>'lw' : Default <code class="docutils literal notranslate"><span class="pre">sklearn.covariance.ledoit_wolf</span></code> regularization</li>
<li>'oas' : Default <code class="docutils literal notranslate"><span class="pre">sklearn.covariance.oas</span></code> regularization</li>
</ul>
</li>
<li><strong>signal_ranks</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.9)"><em>None</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)"><em>list</em></a><em>, </em><em>optional</em><em> (</em><em>default None</em><em>)</em>) -- The initial signal rank to compute. If None, will compute the full SVD.
A list will specify for each view separately.</li>
<li><strong>center</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)"><em>bool</em></a><em>, or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)"><em>list</em></a><em> (</em><em>default True</em><em>)</em>) -- Whether or not to initially mean center the data. A list will specify
for each view separately.</li>
<li><strong>i_mcca_method</strong> (<em>'auto' | 'svd' | 'gevp'</em><em> (</em><em>default 'auto'</em><em>)</em>) -- Whether or not to use the SVD based method (only works with no
regularization) or the gevp based method for informative MCCA.</li>
<li><strong>multiview_output</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)"><em>bool</em></a><em>, </em><em>optional</em><em> (</em><em>default True</em><em>)</em>) -- If True, the <code class="docutils literal notranslate"><span class="pre">.transform</span></code> method returns one dataset per view.
Otherwise, it returns one dataset, of shape (n_samples, n_components)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="mvlearn.embed.CCA.means_">
<code class="descname">means_</code><a class="headerlink" href="#mvlearn.embed.CCA.means_" title="Permalink to this definition">¶</a></dt>
<dd><p>The means of each view, each of shape (n_features,)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">list of numpy.ndarray</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.CCA.loadings_">
<code class="descname">loadings_</code><a class="headerlink" href="#mvlearn.embed.CCA.loadings_" title="Permalink to this definition">¶</a></dt>
<dd><p>The loadings for each view used to project new data,
each of shape (n_features_b, n_components).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">list of numpy.ndarray</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.CCA.common_score_norms_">
<code class="descname">common_score_norms_</code><a class="headerlink" href="#mvlearn.embed.CCA.common_score_norms_" title="Permalink to this definition">¶</a></dt>
<dd><p>Column norms of the sum of the fitted view scores.
Used for projecting new data</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape (n_components,)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.CCA.evals_">
<code class="descname">evals_</code><a class="headerlink" href="#mvlearn.embed.CCA.evals_" title="Permalink to this definition">¶</a></dt>
<dd><p>The generalized eigenvalue problem eigenvalues.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape (n_components,)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.CCA.n_views_">
<code class="descname">n_views_</code><a class="headerlink" href="#mvlearn.embed.CCA.n_views_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of views</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.CCA.n_features_">
<code class="descname">n_features_</code><a class="headerlink" href="#mvlearn.embed.CCA.n_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of features in each fitted view</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)">list</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.CCA.n_components_">
<code class="descname">n_components_</code><a class="headerlink" href="#mvlearn.embed.CCA.n_components_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of components in each transformed view</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="#mvlearn.embed.MCCA" title="mvlearn.embed.MCCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">MCCA</span></code></a>, <a class="reference internal" href="#mvlearn.embed.KMCCA" title="mvlearn.embed.KMCCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">KMCCA</span></code></a></p>
</div>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="cca" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Kettenring, J. R., &quot;Canonical Analysis of Several Sets of
Variables.&quot; Biometrika, 58:433-451, (1971)</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>Tenenhaus, A., et al. &quot;Regularized generalized canonical
correlation analysis.&quot; Psychometrika, 76:257–284, 2011</td></tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mvlearn.embed</span> <span class="kn">import</span> <span class="n">CCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X1</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span><span class="mf">5.</span><span class="p">,</span><span class="mf">4.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X2</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.2</span><span class="p">,</span> <span class="mf">5.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">11.9</span><span class="p">,</span> <span class="mf">12.3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cca</span> <span class="o">=</span> <span class="n">CCA</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cca</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">])</span>
<span class="go">CCA()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Xs_scores</span> <span class="o">=</span> <span class="n">cca</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">])</span>
</pre></div>
</div>
<dl class="method">
<dt id="mvlearn.embed.CCA.stats">
<code class="descname">stats</code><span class="sig-paren">(</span><em>scores</em>, <em>stat=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/cca.html#CCA.stats"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.CCA.stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute relevant statistics from the fitted CCA.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>scores</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>2</em><em>, </em><em>n_samples</em><em>, </em><em>n_components</em><em>)</em>) -- The CCA scores.</li>
<li><strong>stat</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)"><em>str</em></a><em>, </em><em>optional</em><em> (</em><em>default None</em><em>)</em>) -- <p>The statistic to return. If None, returns a dictionary of all
statistics. Otherwise, specifies one of the following statistics</p>
<ul>
<li><dl class="first docutils">
<dt>'r' <span class="classifier-delimiter">:</span> <span class="classifier">numpy.ndarray of shape (n_components,)</span></dt>
<dd>Canonical correlations of each component.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>'Wilks' <span class="classifier-delimiter">:</span> <span class="classifier">numpy.ndarray of shape (n_components,)</span></dt>
<dd>Wilks' Lambda likelihood ratio statistic.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>'df1' <span class="classifier-delimiter">:</span> <span class="classifier">numpy.ndarray of shape (n_components,)</span></dt>
<dd>Degrees of freedom for the chi-squared statistic, and
the numerator degrees of freedom for the F statistic.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>'df2' <span class="classifier-delimiter">:</span> <span class="classifier">numpy.ndarray of shape (n_components,)</span></dt>
<dd>Denominator degrees of freedom for the F statistic.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>'F' <span class="classifier-delimiter">:</span> <span class="classifier">numpy.ndarray of shape (n_components,)</span></dt>
<dd>Rao's approximate F statistic for H_0(k).</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>'pF' <span class="classifier-delimiter">:</span> <span class="classifier">numpy.ndarray of shape (n_components,)</span></dt>
<dd>Right-tail pvalue for stats['F'].</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>'chisq' <span class="classifier-delimiter">:</span> <span class="classifier">numpy.ndarray of shape (n_components,)</span></dt>
<dd>Bartlett's approximate chi-squared statistic for H_0(k)
with Lawley's modification.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>'pChisq' <span class="classifier-delimiter">:</span> <span class="classifier">numpy.ndarray of shape (n_components,)</span></dt>
<dd>Right-tail pvalue for stats['chisq'].</dd>
</dl>
</li>
</ul>
</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>stats</strong> -- Dict containing the statistics with keys specified above or
one of the statistics if specified by the <cite>stat</cite> parameter.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.9)">dict</a> or <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.CCA.canon_corrs">
<code class="descname">canon_corrs</code><span class="sig-paren">(</span><em>scores</em><span class="sig-paren">)</span><a class="headerlink" href="#mvlearn.embed.CCA.canon_corrs" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the canonical correlations between scores from all views.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>scores</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_views</em><em>, </em><em>n_samples</em><em>, </em><em>n_components</em><em>)</em>) -- The CCA scores.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>r</strong> -- The canonical correlations between each component. If more than
two views, returns the correlation matrices.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape (n_components,)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.CCA.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>Xs</em>, <em>y=None</em><span class="sig-paren">)</span><a class="headerlink" href="#mvlearn.embed.CCA.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Learns decompositions of the views.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> (<em>list of array-likes</em><em> or </em><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a>) -- <ul>
<li>Xs length: n_views</li>
<li>Xs[i] shape: (n_samples, n_features_i)</li>
</ul>
<p>The data to fit to.</p>
</li>
<li><strong>y</strong> (<a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.9)"><em>None</em></a>) -- Ignored variable.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>self</strong> -- Returns the instance itself.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.9)">object</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.CCA.fit_transform">
<code class="descname">fit_transform</code><span class="sig-paren">(</span><em>Xs</em>, <em>y=None</em><span class="sig-paren">)</span><a class="headerlink" href="#mvlearn.embed.CCA.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit CCA to the data and transforms the data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> (<em>list of array-likes</em><em> or </em><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a>) -- <ul>
<li>Xs length: n_views</li>
<li>Xs[i] shape: (n_samples, n_features_i)</li>
</ul>
<p>The views to fit and transform</p>
</li>
<li><strong>y</strong> (<a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.9)"><em>None</em></a>) -- Ignored variable.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>Xs_scores</strong> -- If <cite>multiview_output</cite>, returns the normed sum of transformed views</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape (n_views, n_samples, n_components)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.CCA.inverse_transform">
<code class="descname">inverse_transform</code><span class="sig-paren">(</span><em>scores</em><span class="sig-paren">)</span><a class="headerlink" href="#mvlearn.embed.CCA.inverse_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transforms scores back to the original space.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>scores</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_views</em><em>, </em><em>n_samples</em><em>, </em><em>n_components</em><em>)</em>) -- The CCA scores.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>Xs_hat</strong> --<ul class="simple">
<li>Xs_hat length: n_views</li>
<li>Xs_hat[i] shape: (n_samples, n_features_i)</li>
</ul>
<p>The reconstructed views</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">list of array-likes</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.CCA.inverse_transform_view">
<code class="descname">inverse_transform_view</code><span class="sig-paren">(</span><em>scores</em>, <em>view</em><span class="sig-paren">)</span><a class="headerlink" href="#mvlearn.embed.CCA.inverse_transform_view" title="Permalink to this definition">¶</a></dt>
<dd><p>Transforms scores back to the original space.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>scores</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_components</em><em>)</em>) -- The scores</li>
<li><strong>view</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) -- The numeric index of the single view X with respect to the fitted
views.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>X_hat</strong> -- The reconstructed view</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape (n_samples, n_features)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.CCA.score">
<code class="descname">score</code><span class="sig-paren">(</span><em>Xs</em>, <em>y=None</em><span class="sig-paren">)</span><a class="headerlink" href="#mvlearn.embed.CCA.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the sums of squared reconstruction errors for all views.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[||X_{hat} - X||_2^2\]</div>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> (<em>list of array-likes</em><em> or </em><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a>) -- <ul>
<li>Xs length: n_views</li>
<li>Xs[i] shape: (n_samples, n_features_i)</li>
</ul>
<p>The views to reconstruct and score</p>
</li>
<li><strong>y</strong> (<a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.9)"><em>None</em></a>) -- Ignored variable.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>sse</strong> -- Sums of squared reconstruction errors. If
<code class="docutils literal notranslate"><span class="pre">self.multiview_output</span></code> is True, then the mean score is
returned.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape (n_views,)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.CCA.score_view">
<code class="descname">score_view</code><span class="sig-paren">(</span><em>X</em>, <em>view</em><span class="sig-paren">)</span><a class="headerlink" href="#mvlearn.embed.CCA.score_view" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the sum of squared reconstruction error for a view.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[||X_{hat} - X||_2^2\]</div>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) -- The view to reconstruct and score</li>
<li><strong>view</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) -- The numeric index of the single view Xs with respect to the fitted
views.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>sse</strong> -- Sum of squared reconstruction errors</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.CCA.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>Xs</em><span class="sig-paren">)</span><a class="headerlink" href="#mvlearn.embed.CCA.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform the views, projecting them using fitted loadings.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>Xs</strong> (<em>list of array-likes</em><em> or </em><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a>) -- <ul class="simple">
<li>Xs length: n_views</li>
<li>Xs[i] shape: (n_samples, n_features_i)</li>
</ul>
<p>The views to transform</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>Xs_scores</strong> -- If <cite>multiview_output</cite>, returns the normed sum of transformed views</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape (n_views, n_samples, n_components)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.CCA.transform_view">
<code class="descname">transform_view</code><span class="sig-paren">(</span><em>X</em>, <em>view</em><span class="sig-paren">)</span><a class="headerlink" href="#mvlearn.embed.CCA.transform_view" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform a view, projecting it using fitted loadings.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) -- The view to transform</li>
<li><strong>view</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) -- The numeric index of the single view X with respect to the fitted
views.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>X_scores</strong> -- Transformed view</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape (n_samples, n_components)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="multiview-canonical-correlation-analysis-mcca">
<h2>Multiview Canonical Correlation Analysis (MCCA)<a class="headerlink" href="#multiview-canonical-correlation-analysis-mcca" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="mvlearn.embed.MCCA">
<em class="property">class </em><code class="descclassname">mvlearn.embed.</code><code class="descname">MCCA</code><span class="sig-paren">(</span><em>n_components=1</em>, <em>regs=None</em>, <em>signal_ranks=None</em>, <em>center=True</em>, <em>i_mcca_method='auto'</em>, <em>multiview_output=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/mcca.html#MCCA"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.MCCA" title="Permalink to this definition">¶</a></dt>
<dd><p>Multiview canonical correlation analysis for any number of views. Includes
options for regularized MCCA and informative MCCA (where a low rank PCA is
first computed).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_components</strong> (<em>int | 'min' | 'max' | None</em><em> (</em><em>default 1</em><em>)</em>) -- Number of final components to compute. If <cite>int</cite>, will compute that
many. If None, will compute as many as possible. 'min' and 'max' will
respectively use the minimum/maximum number of features among views.</li>
<li><strong>regs</strong> (<em>float | 'lw' | 'oas' | None</em><em>, or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)"><em>list</em></a><em>, </em><em>optional</em><em> (</em><em>default None</em><em>)</em>) -- <p>MCCA regularization for each data view, which can be important
for high dimensional data. A list will specify for each view
separately. If float, must be between 0 and 1 (inclusive).</p>
<ul>
<li>0 or None : corresponds to SUMCORR-AVGVAR MCCA.</li>
<li>1 : partial least squares SVD (generalizes to more than 2 views)</li>
<li>'lw' : Default <code class="docutils literal notranslate"><span class="pre">sklearn.covariance.ledoit_wolf</span></code> regularization</li>
<li>'oas' : Default <code class="docutils literal notranslate"><span class="pre">sklearn.covariance.oas</span></code> regularization</li>
</ul>
</li>
<li><strong>signal_ranks</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.9)"><em>None</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)"><em>list</em></a><em>, </em><em>optional</em><em> (</em><em>default None</em><em>)</em>) -- The initial signal rank to compute. If None, will compute the full SVD.
A list will specify for each view separately.</li>
<li><strong>center</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)"><em>bool</em></a><em>, or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)"><em>list</em></a><em> (</em><em>default True</em><em>)</em>) -- Whether or not to initially mean center the data. A list will specify
for each view separately.</li>
<li><strong>i_mcca_method</strong> (<em>'auto' | 'svd' | 'gevp'</em><em> (</em><em>default 'auto'</em><em>)</em>) -- Whether or not to use the SVD based method (only works with no
regularization) or the gevp based method for informative MCCA.</li>
<li><strong>multiview_output</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)"><em>bool</em></a><em>, </em><em>optional</em><em> (</em><em>default True</em><em>)</em>) -- If True, the <code class="docutils literal notranslate"><span class="pre">.transform</span></code> method returns one dataset per view.
Otherwise, it returns one dataset, of shape (n_samples, n_components)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="mvlearn.embed.MCCA.means_">
<code class="descname">means_</code><a class="headerlink" href="#mvlearn.embed.MCCA.means_" title="Permalink to this definition">¶</a></dt>
<dd><p>The means of each view, each of shape (n_features,)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">list of numpy.ndarray</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.MCCA.loadings_">
<code class="descname">loadings_</code><a class="headerlink" href="#mvlearn.embed.MCCA.loadings_" title="Permalink to this definition">¶</a></dt>
<dd><p>The loadings for each view used to project new data,
each of shape (n_features_b, n_components).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">list of numpy.ndarray</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.MCCA.common_score_norms_">
<code class="descname">common_score_norms_</code><a class="headerlink" href="#mvlearn.embed.MCCA.common_score_norms_" title="Permalink to this definition">¶</a></dt>
<dd><p>Column norms of the sum of the fitted view scores.
Used for projecting new data</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape (n_components,)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.MCCA.evals_">
<code class="descname">evals_</code><a class="headerlink" href="#mvlearn.embed.MCCA.evals_" title="Permalink to this definition">¶</a></dt>
<dd><p>The generalized eigenvalue problem eigenvalues.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape (n_components,)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.MCCA.n_views_">
<code class="descname">n_views_</code><a class="headerlink" href="#mvlearn.embed.MCCA.n_views_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of views</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.MCCA.n_features_">
<code class="descname">n_features_</code><a class="headerlink" href="#mvlearn.embed.MCCA.n_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of features in each fitted view</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)">list</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.MCCA.n_components_">
<code class="descname">n_components_</code><a class="headerlink" href="#mvlearn.embed.MCCA.n_components_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of components in each transformed view</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="#mvlearn.embed.KMCCA" title="mvlearn.embed.KMCCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">KMCCA</span></code></a></p>
</div>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="mcca" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td>Kettenring, J. R., &quot;Canonical Analysis of Several Sets of
Variables.&quot; Biometrika, 58:433-451, (1971)</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td>Tenenhaus, A., et al. &quot;Regularized generalized canonical
correlation analysis.&quot; Psychometrika, 76:257–284, 2011</td></tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mvlearn.embed</span> <span class="kn">import</span> <span class="n">MCCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X1</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X2</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.2</span><span class="p">,</span> <span class="mf">5.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">11.9</span><span class="p">,</span> <span class="mf">12.3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X3</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,],</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mcca</span> <span class="o">=</span> <span class="n">MCCA</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mcca</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">X3</span><span class="p">])</span>
<span class="go">MCCA()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Xs_scores</span> <span class="o">=</span> <span class="n">mcca</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">X3</span><span class="p">])</span>
</pre></div>
</div>
<dl class="method">
<dt id="mvlearn.embed.MCCA.inverse_transform">
<code class="descname">inverse_transform</code><span class="sig-paren">(</span><em>scores</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/mcca.html#MCCA.inverse_transform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.MCCA.inverse_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transforms scores back to the original space.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>scores</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_views</em><em>, </em><em>n_samples</em><em>, </em><em>n_components</em><em>)</em>) -- The CCA scores.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>Xs_hat</strong> --<ul class="simple">
<li>Xs_hat length: n_views</li>
<li>Xs_hat[i] shape: (n_samples, n_features_i)</li>
</ul>
<p>The reconstructed views</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">list of array-likes</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.MCCA.inverse_transform_view">
<code class="descname">inverse_transform_view</code><span class="sig-paren">(</span><em>scores</em>, <em>view</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/mcca.html#MCCA.inverse_transform_view"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.MCCA.inverse_transform_view" title="Permalink to this definition">¶</a></dt>
<dd><p>Transforms scores back to the original space.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>scores</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_components</em><em>)</em>) -- The scores</li>
<li><strong>view</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) -- The numeric index of the single view X with respect to the fitted
views.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>X_hat</strong> -- The reconstructed view</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape (n_samples, n_features)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.MCCA.score">
<code class="descname">score</code><span class="sig-paren">(</span><em>Xs</em>, <em>y=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/mcca.html#MCCA.score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.MCCA.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the sums of squared reconstruction errors for all views.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[||X_{hat} - X||_2^2\]</div>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> (<em>list of array-likes</em><em> or </em><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a>) -- <ul>
<li>Xs length: n_views</li>
<li>Xs[i] shape: (n_samples, n_features_i)</li>
</ul>
<p>The views to reconstruct and score</p>
</li>
<li><strong>y</strong> (<a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.9)"><em>None</em></a>) -- Ignored variable.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>sse</strong> -- Sums of squared reconstruction errors. If
<code class="docutils literal notranslate"><span class="pre">self.multiview_output</span></code> is True, then the mean score is
returned.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape (n_views,)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.MCCA.score_view">
<code class="descname">score_view</code><span class="sig-paren">(</span><em>X</em>, <em>view</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/mcca.html#MCCA.score_view"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.MCCA.score_view" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the sum of squared reconstruction error for a view.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[||X_{hat} - X||_2^2\]</div>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) -- The view to reconstruct and score</li>
<li><strong>view</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) -- The numeric index of the single view Xs with respect to the fitted
views.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>sse</strong> -- Sum of squared reconstruction errors</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.MCCA.canon_corrs">
<code class="descname">canon_corrs</code><span class="sig-paren">(</span><em>scores</em><span class="sig-paren">)</span><a class="headerlink" href="#mvlearn.embed.MCCA.canon_corrs" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the canonical correlations between scores from all views.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>scores</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_views</em><em>, </em><em>n_samples</em><em>, </em><em>n_components</em><em>)</em>) -- The CCA scores.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>r</strong> -- The canonical correlations between each component. If more than
two views, returns the correlation matrices.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape (n_components,)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.MCCA.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>Xs</em>, <em>y=None</em><span class="sig-paren">)</span><a class="headerlink" href="#mvlearn.embed.MCCA.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Learns decompositions of the views.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> (<em>list of array-likes</em><em> or </em><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a>) -- <ul>
<li>Xs length: n_views</li>
<li>Xs[i] shape: (n_samples, n_features_i)</li>
</ul>
<p>The data to fit to.</p>
</li>
<li><strong>y</strong> (<a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.9)"><em>None</em></a>) -- Ignored variable.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>self</strong> -- Returns the instance itself.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.9)">object</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.MCCA.fit_transform">
<code class="descname">fit_transform</code><span class="sig-paren">(</span><em>Xs</em>, <em>y=None</em><span class="sig-paren">)</span><a class="headerlink" href="#mvlearn.embed.MCCA.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit CCA to the data and transforms the data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> (<em>list of array-likes</em><em> or </em><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a>) -- <ul>
<li>Xs length: n_views</li>
<li>Xs[i] shape: (n_samples, n_features_i)</li>
</ul>
<p>The views to fit and transform</p>
</li>
<li><strong>y</strong> (<a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.9)"><em>None</em></a>) -- Ignored variable.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>Xs_scores</strong> -- If <cite>multiview_output</cite>, returns the normed sum of transformed views</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape (n_views, n_samples, n_components)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.MCCA.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>Xs</em><span class="sig-paren">)</span><a class="headerlink" href="#mvlearn.embed.MCCA.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform the views, projecting them using fitted loadings.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>Xs</strong> (<em>list of array-likes</em><em> or </em><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a>) -- <ul class="simple">
<li>Xs length: n_views</li>
<li>Xs[i] shape: (n_samples, n_features_i)</li>
</ul>
<p>The views to transform</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>Xs_scores</strong> -- If <cite>multiview_output</cite>, returns the normed sum of transformed views</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape (n_views, n_samples, n_components)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.MCCA.transform_view">
<code class="descname">transform_view</code><span class="sig-paren">(</span><em>X</em>, <em>view</em><span class="sig-paren">)</span><a class="headerlink" href="#mvlearn.embed.MCCA.transform_view" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform a view, projecting it using fitted loadings.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) -- The view to transform</li>
<li><strong>view</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) -- The numeric index of the single view X with respect to the fitted
views.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>X_scores</strong> -- Transformed view</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape (n_samples, n_components)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="kernel-mcca">
<h2>Kernel MCCA<a class="headerlink" href="#kernel-mcca" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="mvlearn.embed.KMCCA">
<em class="property">class </em><code class="descclassname">mvlearn.embed.</code><code class="descname">KMCCA</code><span class="sig-paren">(</span><em>n_components=1</em>, <em>kernel='linear'</em>, <em>kernel_params={}</em>, <em>regs=None</em>, <em>signal_ranks=None</em>, <em>sval_thresh=0.001</em>, <em>diag_mode='A'</em>, <em>center=True</em>, <em>filter_params=False</em>, <em>n_jobs=None</em>, <em>multiview_output=True</em>, <em>pgso=False</em>, <em>tol=0.1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/kmcca.html#KMCCA"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.KMCCA" title="Permalink to this definition">¶</a></dt>
<dd><p>Kernel multi-view canonical correlation analysis.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_components</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.9)"><em>None</em></a><em> (</em><em>default 1</em><em>)</em>) -- Number of components to compute. If None, will use the number of
features.</li>
<li><strong>kernel</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)"><em>str</em></a><em>, </em><em>callable</em><em>, or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)"><em>list</em></a><em> (</em><em>default 'linear'</em><em>)</em>) -- The kernel function to use. This is the metric argument to
<code class="docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise.pairwise_kernels</span></code>. A list will
specify for each view separately.</li>
<li><strong>kernel_params</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.9)"><em>dict</em></a><em>, or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)"><em>list</em></a><em> (</em><em>default {}</em><em>)</em>) -- Key word arguments to <code class="docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise.pairwise_kernels</span></code>.
A list will specify for each view separately.</li>
<li><strong>regs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.9)"><em>None</em></a><em>, or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)"><em>list</em></a><em>, </em><em>optional</em><em> (</em><em>default None</em><em>)</em>) -- None equates to 0. Floats are nonnegative. The value is used to
regularize singular values in each view based on <cite>diag_mode</cite>
A list will specify the method for each view separately.</li>
<li><strong>signal_ranks</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.9)"><em>None</em></a><em>, or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)"><em>list</em></a><em>, </em><em>optional</em><em> (</em><em>default None</em><em>)</em>) -- Largest SVD rank to compute for each view. If None, the full rank
decomposition will be used. A list will specify for each view
separately.</li>
<li><strong>sval_thresh</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a><em>, or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)"><em>list</em></a><em> (</em><em>default 1e-3</em><em>)</em>) -- For each view we throw out singular values of (1/n)K, the gram matrix
scaled by n_samples, below this threshold. A non-zero value deals with
singular gram matrices.</li>
<li><strong>diag_mode</strong> (<em>'A' | 'B' | 'C'</em><em> (</em><em>default 'A'</em><em>)</em>) -- <p>Method of regularizing singular values <cite>s</cite> with regularization
parameter <cite>r</cite></p>
<ul>
<li>'A' : <span class="math notranslate nohighlight">\((1 - r) * K^2 + r * K\)</span> <a class="footnote-reference" href="#kmcca" id="id3">[5]</a></li>
<li>'B' : <span class="math notranslate nohighlight">\((1-r) (K + n/2 \kappa * I)^2\)</span> where
<span class="math notranslate nohighlight">\(\kappa = r / (1 - r)\)</span> <a class="footnote-reference" href="#id7" id="id4">[6]</a></li>
<li>'C' : <span class="math notranslate nohighlight">\((1 - r) K^2 + r * I_n\)</span> <a class="footnote-reference" href="#id8" id="id5">[7]</a></li>
</ul>
</li>
<li><strong>center</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)"><em>bool</em></a><em>, or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)"><em>list</em></a><em> (</em><em>default True</em><em>)</em>) -- Whether or not to initially mean center the data. A list will
specify for each view separately.</li>
<li><strong>filter_params</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)"><em>bool</em></a><em> (</em><em>default False</em><em>)</em>) -- See <code class="docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise.pairwise_kernels</span></code> documentation.</li>
<li><strong>n_jobs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.9)"><em>None</em></a><em>, </em><em>optional</em><em> (</em><em>default None</em><em>)</em>) -- Number of jobs to run in parallel when computing kernel matrices.
See <code class="docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise.pairwise_kernels</span></code> documentation.</li>
<li><strong>multiview_output</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)"><em>bool</em></a><em>, </em><em>optional</em><em> (</em><em>default True</em><em>)</em>) -- If True, the <code class="docutils literal notranslate"><span class="pre">.transform</span></code> method returns one dataset per view.
Otherwise, it returns one dataset, of shape (n_samples, n_components)</li>
<li><strong>pgso</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)"><em>bool</em></a><em>, </em><em>optional</em><em> (</em><em>default False</em><em>)</em>) -- If True, computes a partial Gram-Schmidt orthogonalization
approximation of the kernel matrices to the given tolerance.</li>
<li><strong>tol</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a><em> (</em><em>default 0.1</em><em>)</em>) -- The minimum matrix trace difference between a kernel matrix and its
computed pgso approximation, relative to the kernel trace.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="mvlearn.embed.KMCCA.kernel_col_means_">
<code class="descname">kernel_col_means_</code><a class="headerlink" href="#mvlearn.embed.KMCCA.kernel_col_means_" title="Permalink to this definition">¶</a></dt>
<dd><p>The column means of each gram matrix</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">list of numpy.ndarray, shape (n_samples,)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.KMCCA.kernel_mat_means_">
<code class="descname">kernel_mat_means_</code><a class="headerlink" href="#mvlearn.embed.KMCCA.kernel_mat_means_" title="Permalink to this definition">¶</a></dt>
<dd><p>The total means of each gram matrix</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)">list</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.KMCCA.dual_vars_">
<code class="descname">dual_vars_</code><a class="headerlink" href="#mvlearn.embed.KMCCA.dual_vars_" title="Permalink to this definition">¶</a></dt>
<dd><p>The loadings for the gram matrix of each view</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape (n_views, n_samples, n_components)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.KMCCA.common_score_norms_">
<code class="descname">common_score_norms_</code><a class="headerlink" href="#mvlearn.embed.KMCCA.common_score_norms_" title="Permalink to this definition">¶</a></dt>
<dd><p>Column norms of the sum of the view scores.
Useful for projecting new data</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape (n_components,)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.KMCCA.evals_">
<code class="descname">evals_</code><a class="headerlink" href="#mvlearn.embed.KMCCA.evals_" title="Permalink to this definition">¶</a></dt>
<dd><p>The generalized eigenvalue problem eigenvalues.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape (n_components,)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.KMCCA.Xs_">
<code class="descname">Xs_</code><a class="headerlink" href="#mvlearn.embed.KMCCA.Xs_" title="Permalink to this definition">¶</a></dt>
<dd><ul class="simple">
<li>Xs[i] shape (n_samples, n_features_i)</li>
</ul>
<p>The original data matrices for use in kernel matrix computation
during calls to <code class="docutils literal notranslate"><span class="pre">.transform</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">list of numpy.ndarray, length (n_views,)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.KMCCA.n_views_">
<code class="descname">n_views_</code><a class="headerlink" href="#mvlearn.embed.KMCCA.n_views_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of views</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.KMCCA.n_features_">
<code class="descname">n_features_</code><a class="headerlink" href="#mvlearn.embed.KMCCA.n_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of features in each fitted view</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)">list</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.KMCCA.n_components_">
<code class="descname">n_components_</code><a class="headerlink" href="#mvlearn.embed.KMCCA.n_components_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of components in each transformed view</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.KMCCA.pgso_Ls_">
<code class="descname">pgso_Ls_</code><a class="headerlink" href="#mvlearn.embed.KMCCA.pgso_Ls_" title="Permalink to this definition">¶</a></dt>
<dd><ul class="simple">
<li>pgso_Ls_[i] shape (n_samples, rank_i)</li>
</ul>
<p>The Gram-Schmidt approximations of the kernel matrices</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">list of numpy.ndarray, length (n_views,)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.KMCCA.pgso_norms_">
<code class="descname">pgso_norms_</code><a class="headerlink" href="#mvlearn.embed.KMCCA.pgso_norms_" title="Permalink to this definition">¶</a></dt>
<dd><ul class="simple">
<li>pgso_norms_[i] shape (rank_i,)</li>
</ul>
<p>The maximum norms found during the Gram-Schmidt procedure, descending</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">list of numpy.ndarray, length (n_views,)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.KMCCA.pgso_idxs_">
<code class="descname">pgso_idxs_</code><a class="headerlink" href="#mvlearn.embed.KMCCA.pgso_idxs_" title="Permalink to this definition">¶</a></dt>
<dd><ul class="simple">
<li>pgso_idxs_[i] shape (rank_i,)</li>
</ul>
<p>The sample indices of the maximum norms</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">list of numpy.ndarray, length (n_views,)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.KMCCA.pgso_Xs_">
<code class="descname">pgso_Xs_</code><a class="headerlink" href="#mvlearn.embed.KMCCA.pgso_Xs_" title="Permalink to this definition">¶</a></dt>
<dd><ul class="simple">
<li>pgso_Xs_[i] shape (rank_i, n_features)</li>
</ul>
<p>The samples with indices saved in <a href="#id21"><span class="problematic" id="id22">pgso_idxs_</span></a>, sorted by <a href="#id23"><span class="problematic" id="id24">pgso_norms_</span></a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">list of numpy.ndarray, length (n_views,)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.KMCCA.pgso_ranks_">
<code class="descname">pgso_ranks_</code><a class="headerlink" href="#mvlearn.embed.KMCCA.pgso_ranks_" title="Permalink to this definition">¶</a></dt>
<dd><p>The ranks of the partial Gram-Schmidt results for each view.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)">list</a>, length (n_views,)</td>
</tr>
</tbody>
</table>
</dd></dl>

<p class="rubric">Notes</p>
<p>Traditional CCA aims to find useful projections of features in each view
of data, computing a weighted sum, but may not extract useful descriptors
of the data because of its linearity. KMCCA offers an alternative solution
by first projecting the data onto a higher dimensional feature space.</p>
<div class="math notranslate nohighlight">
\[\phi: \mathbf{x} = (x_1,...,x_m) \mapsto
\phi(\mathbf{x}) = (z_1,...,z_N),
(m &lt;&lt; N)\]</div>
<p>before performing CCA in the new feature space.</p>
<p>Kernels are effectively distance functions that compute inner products in
the higher dimensional feature space, a method known as the kernel trick.
A kernel function K, such that for all <span class="math notranslate nohighlight">\(\mathbf{x},
\mathbf{z} \in X\)</span></p>
<div class="math notranslate nohighlight">
\[K(\mathbf{x}, \mathbf{z}) = \langle\phi(\mathbf{x})
\cdot \phi(\mathbf{z})\rangle.\]</div>
<p>The kernel matrix <span class="math notranslate nohighlight">\(K_i\)</span> has entries computed from the kernel
function. Using the kernel trick, loadings of the kernel matrix
(<a href="#id25"><span class="problematic" id="id26">dual_vars_</span></a>) are solved for rather than of the features from <span class="math notranslate nohighlight">\(\phi\)</span>.</p>
<p>Kernel matrices grow exponentially with the size of data. They not only
have to store <span class="math notranslate nohighlight">\(n^2\)</span> elements, but also face the complexity of matrix
eigenvalue problems. In a Cholesky decomposition a positive definite
matrix K is decomposed to a lower triangular matrix <span class="math notranslate nohighlight">\(L\)</span> :
<span class="math notranslate nohighlight">\(K = LL'\)</span>.</p>
<p>The dual partial Gram-Schmidt orthogonalization (PSGO) is equivalent to the
Incomplete Cholesky Decomposition (ICD) which looks for a low rank
approximation of <span class="math notranslate nohighlight">\(L\)</span>, reducing the cost of operations of the matrix
such that <span class="math notranslate nohighlight">\(\frac{1}{\sum_i K_{ii}} tr(K - LL^T) \leq tol\)</span>.</p>
<p>A PSGO tolerance yielding rank <span class="math notranslate nohighlight">\(m\)</span> leads to storage requirements of
<span class="math notranslate nohighlight">\(O(mn)\)</span> instead of <span class="math notranslate nohighlight">\(O(n^2)\)</span> and becomes <span class="math notranslate nohighlight">\(O(nm^2)\)</span> instead
of <span class="math notranslate nohighlight">\(O(n^3)\)</span> <a class="footnote-reference" href="#id8" id="id6">[7]</a>.</p>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="#mvlearn.embed.MCCA" title="mvlearn.embed.MCCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">MCCA</span></code></a></p>
</div>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="kmcca" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id3">[5]</a></td><td>Hardoon D., et al. &quot;Canonical Correlation Analysis: An
Overview with Application to Learning Methods&quot;, Neural
Computation, Volume 16 (12), pp 2639-2664, 2004.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id7" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id4">[6]</a></td><td>Bach, F. and Jordan, M. &quot;Kernel Independent Component
Analysis.&quot; Journal of Machine Learning Research, 3:1-48, 2002.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id8" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[7]</td><td><em>(<a class="fn-backref" href="#id5">1</a>, <a class="fn-backref" href="#id6">2</a>)</em> Kuss, M. and Graepel, T.. &quot;The Geometry of Kernel Canonical
Correlation Analysis.&quot; MPI Technical Report, 108. (2003).</td></tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mvlearn.embed</span> <span class="kn">import</span> <span class="n">KMCCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X1</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X2</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.2</span><span class="p">,</span> <span class="mf">5.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">11.9</span><span class="p">,</span> <span class="mf">12.3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X3</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,],</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kmcca</span> <span class="o">=</span> <span class="n">KMCCA</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kmcca</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">X3</span><span class="p">])</span>
<span class="go">KMCCA()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Xs_scores</span> <span class="o">=</span> <span class="n">kmcca</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">X3</span><span class="p">])</span>
</pre></div>
</div>
<dl class="method">
<dt id="mvlearn.embed.KMCCA.transform_view">
<code class="descname">transform_view</code><span class="sig-paren">(</span><em>X</em>, <em>view</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/kmcca.html#KMCCA.transform_view"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.KMCCA.transform_view" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform a view, projecting it using fitted loadings.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) -- The view to transform</li>
<li><strong>view</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) -- The numeric index of the single view X with respect to the fitted
views.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>X_scores</strong> -- Transformed view</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape (n_samples, n_components)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.KMCCA.canon_corrs">
<code class="descname">canon_corrs</code><span class="sig-paren">(</span><em>scores</em><span class="sig-paren">)</span><a class="headerlink" href="#mvlearn.embed.KMCCA.canon_corrs" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the canonical correlations between scores from all views.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>scores</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_views</em><em>, </em><em>n_samples</em><em>, </em><em>n_components</em><em>)</em>) -- The CCA scores.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>r</strong> -- The canonical correlations between each component. If more than
two views, returns the correlation matrices.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape (n_components,)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.KMCCA.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>Xs</em>, <em>y=None</em><span class="sig-paren">)</span><a class="headerlink" href="#mvlearn.embed.KMCCA.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Learns decompositions of the views.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> (<em>list of array-likes</em><em> or </em><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a>) -- <ul>
<li>Xs length: n_views</li>
<li>Xs[i] shape: (n_samples, n_features_i)</li>
</ul>
<p>The data to fit to.</p>
</li>
<li><strong>y</strong> (<a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.9)"><em>None</em></a>) -- Ignored variable.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>self</strong> -- Returns the instance itself.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.9)">object</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.KMCCA.fit_transform">
<code class="descname">fit_transform</code><span class="sig-paren">(</span><em>Xs</em>, <em>y=None</em><span class="sig-paren">)</span><a class="headerlink" href="#mvlearn.embed.KMCCA.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit CCA to the data and transforms the data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> (<em>list of array-likes</em><em> or </em><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a>) -- <ul>
<li>Xs length: n_views</li>
<li>Xs[i] shape: (n_samples, n_features_i)</li>
</ul>
<p>The views to fit and transform</p>
</li>
<li><strong>y</strong> (<a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.9)"><em>None</em></a>) -- Ignored variable.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>Xs_scores</strong> -- If <cite>multiview_output</cite>, returns the normed sum of transformed views</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape (n_views, n_samples, n_components)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.KMCCA.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>Xs</em><span class="sig-paren">)</span><a class="headerlink" href="#mvlearn.embed.KMCCA.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform the views, projecting them using fitted loadings.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>Xs</strong> (<em>list of array-likes</em><em> or </em><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a>) -- <ul class="simple">
<li>Xs length: n_views</li>
<li>Xs[i] shape: (n_samples, n_features_i)</li>
</ul>
<p>The views to transform</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>Xs_scores</strong> -- If <cite>multiview_output</cite>, returns the normed sum of transformed views</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape (n_views, n_samples, n_components)</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="generalized-canonical-correlation-analysis-gcca">
<h2>Generalized Canonical Correlation Analysis (GCCA)<a class="headerlink" href="#generalized-canonical-correlation-analysis-gcca" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="mvlearn.embed.GCCA">
<em class="property">class </em><code class="descclassname">mvlearn.embed.</code><code class="descname">GCCA</code><span class="sig-paren">(</span><em>n_components=None</em>, <em>fraction_var=None</em>, <em>sv_tolerance=None</em>, <em>n_elbows=2</em>, <em>tall=False</em>, <em>max_rank=False</em>, <em>n_jobs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/gcca.html#GCCA"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.GCCA" title="Permalink to this definition">¶</a></dt>
<dd><p>An implementation of Generalized Canonical Correlation Analysis <a class="footnote-reference" href="#gcca" id="id9">[8]</a>
suitable for cases where the number of features exceeds the number of
samples by first applying single view dimensionality reduction. Computes
individual projections into a common subspace such that the correlations
between pairwise projections are minimized (ie. maximize pairwise
correlation). An important note: this is applicable to any number of
views, not just two.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_components</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em> (</em><em>positive</em><em>)</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) -- If <code class="docutils literal notranslate"><span class="pre">self.sv_tolerance=None</span></code>, selects the number of SVD
components to keep for each view. If none, another selection
method is used.</li>
<li><strong>fraction_var</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a><em>, </em><em>default=None</em>) -- If <code class="docutils literal notranslate"><span class="pre">self.sv_tolerance=None</span></code>, and <code class="docutils literal notranslate"><span class="pre">self.n_components=None</span></code>,
selects the number of SVD components to keep for each view by
capturing enough of the variance. If none, another selection
method is used.</li>
<li><strong>sv_tolerance</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a><em>, </em><em>optional</em><em>, </em><em>default=None</em>) -- Selects the number of SVD components to keep for each view by
thresholding singular values. If none, another selection
method is used.</li>
<li><strong>n_elbows</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em>, </em><em>optional</em><em>, </em><em>default: 2</em>) -- If <code class="docutils literal notranslate"><span class="pre">self.fraction_var=None</span></code>, <code class="docutils literal notranslate"><span class="pre">self.sv_tolerance=None</span></code>, and
<code class="docutils literal notranslate"><span class="pre">self.n_components=None</span></code>, then compute the optimal embedding
dimension using <code class="xref py py-func docutils literal notranslate"><span class="pre">utils.select_dimension()</span></code>.
Otherwise, ignored.</li>
<li><strong>tall</strong> (<em>boolean</em><em>, </em><em>default=False</em>) -- Set to true if n_samples &gt; n_features, speeds up SVD</li>
<li><strong>max_rank</strong> (<em>boolean</em><em>, </em><em>default=False</em>) -- If true, sets the rank of the common latent space as the maximum rank
of the individual spaces. If false, uses the minimum individual rank.</li>
<li><strong>n_jobs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em> (</em><em>positive</em><em>)</em><em>, </em><em>default=None</em>) -- The number of jobs to run in parallel when computing the SVDs for each
view in <cite>fit</cite> and <cite>partial_fit</cite>. <cite>None</cite> means 1 job, <cite>-1</cite> means using
all processors.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="mvlearn.embed.GCCA.projection_mats_">
<code class="descname">projection_mats_</code><a class="headerlink" href="#mvlearn.embed.GCCA.projection_mats_" title="Permalink to this definition">¶</a></dt>
<dd><p>A projection matrix for each view, from the given space to the
latent space</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">list of arrays</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.GCCA.ranks_">
<code class="descname">ranks_</code><a class="headerlink" href="#mvlearn.embed.GCCA.ranks_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of left singular vectors kept for each view during the first
SVD</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">list of ints</td>
</tr>
</tbody>
</table>
</dd></dl>

<p class="rubric">Notes</p>
<p>Consider two views <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span>. Canonical Correlation
Analysis seeks to find vectors <span class="math notranslate nohighlight">\(a_1\)</span> and <span class="math notranslate nohighlight">\(a_2\)</span> to maximize
the correlation <span class="math notranslate nohighlight">\(X_1 a_1\)</span> and <span class="math notranslate nohighlight">\(X_2 a_2\)</span>, expanded below.</p>
<div class="math notranslate nohighlight">
\[\left(\frac{a_1^TC_{12}a_2}
    {\sqrt{a_1^TC_{11}a_1a_2^TC_{22}a_2}}
    \right)\]</div>
<p>where <span class="math notranslate nohighlight">\(C_{11}\)</span>, <span class="math notranslate nohighlight">\(C_{22}\)</span>, and <span class="math notranslate nohighlight">\(C_{12}\)</span> are respectively
the view 1, view 2, and between view covariance matrix estimates. GCCA
maximizes the sum of these correlations across all pairwise views and
computes a set of linearly independent components. This specific algorithm
first applies principal component analysis (PCA) independently to each view
and then aligns the most informative projections to find correlated and
informative subspaces. Parameters that control the embedding dimension
apply to the PCA step. The dimension of each aligned subspace is the
maximum or minimum of the individual dimensions, per the <cite>max_ranks</cite>
parameter. Using the maximum will capture the most information from all
views but also noise from some views. Using the minimum will better remove
noise dimensions but at the cost of information from some views.</p>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="gcca" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id9">[8]</a></td><td>B. Afshin-Pour, G.A. Hossein-Zadeh, S.C. Strother, H.
Soltanian-Zadeh. &quot;Enhancing reproducibility of fMRI statistical
maps using generalized canonical correlation analysis in NPAIRS
framework.&quot; Neuroimage, volume 60, pp. 1970-1981, 2012</td></tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mvlearn.datasets</span> <span class="kn">import</span> <span class="n">load_UCImultifeature</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mvlearn.embed</span> <span class="kn">import</span> <span class="n">GCCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Load full dataset, labels not needed</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Xs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">load_UCImultifeature</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gcca</span> <span class="o">=</span> <span class="n">GCCA</span><span class="p">(</span><span class="n">fraction_var</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Transform the first 5 views</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Xs_latents</span> <span class="o">=</span> <span class="n">gcca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Xs</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">([</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">X</span> <span class="ow">in</span> <span class="n">Xs_latents</span><span class="p">])</span>
<span class="go">[9, 9, 9, 9, 9]</span>
</pre></div>
</div>
<dl class="method">
<dt id="mvlearn.embed.GCCA.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>Xs</em>, <em>y=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/gcca.html#GCCA.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.GCCA.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates a projection from each view to a latent space such that
the sum of pairwise latent space correlations is maximized. Each view
'X' is normalized and the left singular vectors of 'X^T X' are
calculated using SVD. The number of singular vectors kept is determined
by either the percent variance explained, a given rank threshold, or a
given number of components. The singular vectors kept are concatenated
and SVD of that is taken and used to calculated projections for each
view.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> (<em>list of array-likes</em><em> or </em><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a>) -- <ul>
<li>Xs length: n_views</li>
<li>Xs[i] shape: (n_samples, n_features_i)</li>
</ul>
<p>The data to fit to. Each view will receive its own embedding.</p>
</li>
<li><strong>y</strong> (<em>ignored</em>) -- Included for API compliance.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>self</strong></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">returns an instance of self.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.GCCA.partial_fit">
<code class="descname">partial_fit</code><span class="sig-paren">(</span><em>Xs</em>, <em>reset=False</em>, <em>multiview_step=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/gcca.html#GCCA.partial_fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.GCCA.partial_fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs like <cite>fit</cite>, but will not overwrite previously fitted single
views and instead uses them as well as the new data. Useful if the data
needs to be processed in batches.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> (<em>list of array-likes</em><em> or </em><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a>) -- <ul>
<li>Xs length: n_views</li>
<li>Xs[i] shape: (n_samples, n_features_i)</li>
</ul>
<p>The data to fit to. Each view will receive its own embedding.</p>
</li>
<li><strong>reset</strong> (<em>boolean</em><em> (</em><em>default = False</em><em>)</em>) -- If True, overwrites all prior computations.</li>
<li><strong>multiview_step</strong> (<em>boolean</em><em>, </em><em>(</em><em>default = True</em><em>)</em>) -- If True, performs the joint SVD step on the results from individual
views. Must be set to True in the final call.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>self</strong></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">returns an instance of self.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.GCCA.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>Xs</em>, <em>view_idx=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/gcca.html#GCCA.transform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.GCCA.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Embeds data matrix(s) using the fitted projection matrices. May be
used for out-of-sample embeddings.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> (<em>list of array-likes</em><em> or </em><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a>) -- <ul>
<li>Xs length: n_views</li>
<li>Xs[i] shape: (n_samples, n_features_i)</li>
</ul>
<p>A list of data matrices from each view to transform based on the
prior fit function. If view_idx is defined, then Xs is a 2D data
matrix corresponding to a single view.</p>
</li>
<li><strong>view_idx</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em>, </em><em>default=None</em>) -- For transformation of a single view. If not None, then Xs is 2D
and views_idx specifies the index of the view from which Xs comes
from.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>Xs_transformed</strong> -- Same shape as Xs</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">list of array-likes or array-like</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.GCCA.fit_transform">
<code class="descname">fit_transform</code><span class="sig-paren">(</span><em>Xs</em>, <em>y=None</em><span class="sig-paren">)</span><a class="headerlink" href="#mvlearn.embed.GCCA.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit an embedder to the data and transform the data</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> (<em>list of array-likes</em><em> or </em><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a>) -- <ul>
<li>Xs length: n_views</li>
<li>Xs[i] shape: (n_samples, n_features_i)</li>
</ul>
</li>
<li><strong>y</strong> (<em>array</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>optional</em>) -- Targets to be used if fitting the algorithm is supervised.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><p><strong>X_transformed</strong> --</p>
<ul class="simple">
<li>X_transformed length: n_views</li>
<li>X_transformed[i] shape: (n_samples, n_components_i)</li>
</ul>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">list of numpy.ndarray</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="deep-canonical-correlation-analysis-dcca">
<h2>Deep Canonical Correlation Analysis (DCCA)<a class="headerlink" href="#deep-canonical-correlation-analysis-dcca" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="mvlearn.embed.DCCA">
<em class="property">class </em><code class="descclassname">mvlearn.embed.</code><code class="descname">DCCA</code><span class="sig-paren">(</span><em>input_size1=None</em>, <em>input_size2=None</em>, <em>n_components=2</em>, <em>layer_sizes1=None</em>, <em>layer_sizes2=None</em>, <em>use_all_singular_values=False</em>, <em>device=device(type='cpu')</em>, <em>epoch_num=200</em>, <em>batch_size=800</em>, <em>learning_rate=0.001</em>, <em>reg_par=1e-05</em>, <em>tolerance=0.001</em>, <em>print_train_log_info=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/dcca.html#DCCA"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.DCCA" title="Permalink to this definition">¶</a></dt>
<dd><p>An implementation of Deep Canonical Correlation Analysis <a class="footnote-reference" href="#dcca" id="id10">[9]</a> with
PyTorch. It computes projections into a common subspace in order to
maximize the correlation between pairwise projections into the subspace
from two views of data. To obtain these projections, two fully connected
deep networks are trained to initially transform the two views of data.
Then, the transformed data is projected using linear CCA. This can be
thought of as training a kernel for each view that initially acts on the
data before projection. The networks are trained to maximize the ability
of the linear CCA to maximize the correlation between the final
dimensions.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size1</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em> (</em><em>positive</em><em>)</em>) -- The dimensionality of the input vectors in view 1.</li>
<li><strong>input_size2</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em> (</em><em>positive</em><em>)</em>) -- The dimensionality of the input vectors in view 2.</li>
<li><strong>n_components</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em> (</em><em>positive</em><em>)</em><em>, </em><em>default=2</em>) -- The output dimensionality of the correlated projections. The deep
network wil transform the data to this size. Must satisfy:
<code class="docutils literal notranslate"><span class="pre">n_components</span></code> &lt;= max(layer_sizes1[-1], layer_sizes2[-1]).</li>
<li><strong>layer_sizes1</strong> (<em>list of ints</em><em>, </em><em>default=None</em>) -- The sizes of the layers of the deep network applied to view 1 before
CCA. For example, if the input dimensionality is 256, and there is one
hidden layer with 1024 units and the output dimensionality is 100
before applying CCA, layer_sizes1=[1024, 100]. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, set to
[1000, <code class="docutils literal notranslate"><span class="pre">self.n_components_</span></code>].</li>
<li><strong>layer_sizes2</strong> (<em>list of ints</em><em>, </em><em>default=None</em>) -- The sizes of the layers of the deep network applied to view 2 before
CCA. Does not need to have the same hidden layer architecture as
layer_sizes1, but the final dimensionality must be the same. If
<code class="docutils literal notranslate"><span class="pre">None</span></code>, set to [1000, <code class="docutils literal notranslate"><span class="pre">self.n_components_</span></code>].</li>
<li><strong>use_all_singular_values</strong> (<em>boolean</em><em> (</em><em>default=False</em><em>)</em>) -- Whether or not to use all the singular values in the CCA computation
to calculate the loss. If False, only the top <code class="docutils literal notranslate"><span class="pre">n_components</span></code>
singular values are used.</li>
<li><strong>device</strong> (<em>string</em><em>, </em><em>default='cpu'</em>) -- The torch device for processing. Can be used with a GPU if available.</li>
<li><strong>epoch_num</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em> (</em><em>positive</em><em>)</em><em>, </em><em>default=200</em>) -- The max number of epochs to train the deep networks.</li>
<li><strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em> (</em><em>positive</em><em>)</em><em>, </em><em>default=800</em>) -- Batch size for training the deep networks.</li>
<li><strong>learning_rate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a><em> (</em><em>positive</em><em>)</em><em>, </em><em>default=1e-3</em>) -- Learning rate for training the deep networks.</li>
<li><strong>reg_par</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a><em> (</em><em>positive</em><em>)</em><em>, </em><em>default=1e-5</em>) -- Weight decay parameter used in the RMSprop optimizer.</li>
<li><strong>tolerance</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a><em>, </em><em>(</em><em>positive</em><em>)</em><em>, </em><em>default=1e-2</em>) -- Threshold difference between successive iteration losses to define
convergence and stop training.</li>
<li><strong>print_train_log_info</strong> (<em>boolean</em><em>, </em><em>default=False</em>) -- If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the training loss at each epoch will be printed to the
console when DCCA.fit() is called.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="mvlearn.embed.DCCA.input_size1_">
<code class="descname">input_size1_</code><a class="headerlink" href="#mvlearn.embed.DCCA.input_size1_" title="Permalink to this definition">¶</a></dt>
<dd><p>The dimensionality of the input vectors in view 1.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a> (positive)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.DCCA.input_size2_">
<code class="descname">input_size2_</code><a class="headerlink" href="#mvlearn.embed.DCCA.input_size2_" title="Permalink to this definition">¶</a></dt>
<dd><p>The dimensionality of the input vectors in view 2.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a> (positive)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.DCCA.n_components_">
<code class="descname">n_components_</code><a class="headerlink" href="#mvlearn.embed.DCCA.n_components_" title="Permalink to this definition">¶</a></dt>
<dd><p>The output dimensionality of the correlated projections. The deep
network wil transform the data to this size. If not specified, will
be set to 2.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a> (positive)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.DCCA.layer_sizes1_">
<code class="descname">layer_sizes1_</code><a class="headerlink" href="#mvlearn.embed.DCCA.layer_sizes1_" title="Permalink to this definition">¶</a></dt>
<dd><p>The sizes of the layers of the deep network applied to view 1 before
CCA. For example, if the input dimensionality is 256, and there is one
hidden layer with 1024 units and the output dimensionality is 100
before applying CCA, layer_sizes1=[1024, 100].</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">list of ints</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.DCCA.layer_sizes2_">
<code class="descname">layer_sizes2_</code><a class="headerlink" href="#mvlearn.embed.DCCA.layer_sizes2_" title="Permalink to this definition">¶</a></dt>
<dd><p>The sizes of the layers of the deep network applied to view 2 before
CCA. Does not need to have the same hidden layer architecture as
layer_sizes1, but the final dimensionality must be the same.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">list of ints</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.DCCA.device_">
<code class="descname">device_</code><a class="headerlink" href="#mvlearn.embed.DCCA.device_" title="Permalink to this definition">¶</a></dt>
<dd><p>The torch device for processing.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">string</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.DCCA.batch_size_">
<code class="descname">batch_size_</code><a class="headerlink" href="#mvlearn.embed.DCCA.batch_size_" title="Permalink to this definition">¶</a></dt>
<dd><p>Batch size for training the deep networks.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a> (positive)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.DCCA.learning_rate_">
<code class="descname">learning_rate_</code><a class="headerlink" href="#mvlearn.embed.DCCA.learning_rate_" title="Permalink to this definition">¶</a></dt>
<dd><p>Learning rate for training the deep networks.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a> (positive)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.DCCA.reg_par_">
<code class="descname">reg_par_</code><a class="headerlink" href="#mvlearn.embed.DCCA.reg_par_" title="Permalink to this definition">¶</a></dt>
<dd><p>Weight decay parameter used in the RMSprop optimizer.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a> (positive)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.DCCA.deep_model_">
<code class="descname">deep_model_</code><a class="headerlink" href="#mvlearn.embed.DCCA.deep_model_" title="Permalink to this definition">¶</a></dt>
<dd><p>2 view Deep CCA object used to transform 2 views of data together.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">DeepPairedNetworks</span></code> object</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.DCCA.linear_cca_">
<code class="descname">linear_cca_</code><a class="headerlink" href="#mvlearn.embed.DCCA.linear_cca_" title="Permalink to this definition">¶</a></dt>
<dd><p>Linear CCA object used to project final transformations from output
of <code class="docutils literal notranslate"><span class="pre">deep_model</span></code> to the <code class="docutils literal notranslate"><span class="pre">n_components</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">linear_cca</span></code> object</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.DCCA.model_">
<code class="descname">model_</code><a class="headerlink" href="#mvlearn.embed.DCCA.model_" title="Permalink to this definition">¶</a></dt>
<dd><p>Wrapper around <code class="docutils literal notranslate"><span class="pre">deep_model</span></code> to allow parallelisation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">torch.nn.DataParallel object</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.DCCA.loss_">
<code class="descname">loss_</code><a class="headerlink" href="#mvlearn.embed.DCCA.loss_" title="Permalink to this definition">¶</a></dt>
<dd><p>Loss function for <code class="docutils literal notranslate"><span class="pre">deep_model</span></code>. Defined as the negative correlation
between outputs of transformed views.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">cca_loss</span></code> object</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.DCCA.optimizer_">
<code class="descname">optimizer_</code><a class="headerlink" href="#mvlearn.embed.DCCA.optimizer_" title="Permalink to this definition">¶</a></dt>
<dd><p>Optimizer used to train the networks.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">torch.optim.RMSprop object</td>
</tr>
</tbody>
</table>
</dd></dl>

<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Warns:</th><td class="field-body"><ul class="first last simple">
<li><strong>In order to run DCCA, pytorch and other certain optional dependencies must</strong></li>
<li><strong>be installed. See the installation page for details.</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>Deep Canonical Correlation Analysis is a method of finding highly
correlated subspaces for 2 views of data using nonlinear transformations
learned by deep networks. It can be thought of as using deep networks
to learn the best potentially nonlinear kernels for a variant of kernel
CCA.</p>
<p>The networks used for each view in DCCA consist of fully connected linear
layers with a sigmoid activation function.</p>
<p>The problem DCCA problem is formulated from <a class="footnote-reference" href="#dcca" id="id11">[9]</a>. Consider two
views <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span>. DCCA seeks to find the parameters for
each view, <span class="math notranslate nohighlight">\(\Theta_1\)</span> and <span class="math notranslate nohighlight">\(\Theta_2\)</span>, such that they maximize</p>
<div class="math notranslate nohighlight">
\[\text{corr}\left(f_1\left(X_1;\Theta_1\right),
f_2\left(X_2;\Theta_2\right)\right)\]</div>
<p>These parameters are estimated in the deep network by following gradient
descent on the input data. Taking <span class="math notranslate nohighlight">\(H_1, H_2 \in R^{o \times m}\)</span> to
be the outputs of the deep network in each column for the input data of
size <span class="math notranslate nohighlight">\(m\)</span>. Take the centered matrix <span class="math notranslate nohighlight">\(\bar{H}_1 =
H_1-\frac{1}{m}H_1{1}\)</span>, and <span class="math notranslate nohighlight">\(\bar{H}_2 = H_2-\frac{1}{m}H_2{1}\)</span>.
Then, define</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\hat{\Sigma}_{12} &amp;= \frac{1}{m-1}\bar{H}_1\bar{H}_2^T \\
\hat{\Sigma}_{11} &amp;= \frac{1}{m-1}\bar{H}_1\bar{H}_1^T + r_1I \\
\hat{\Sigma}_{22} &amp;= \frac{1}{m-1}\bar{H}_2\bar{H}_2^T + r_2I
\end{align*}\end{split}\]</div>
<p>Where <span class="math notranslate nohighlight">\(r_1\)</span> and <span class="math notranslate nohighlight">\(r_2\)</span> are regularization constants <span class="math notranslate nohighlight">\(&gt;0\)</span>
so the matrices are guaranteed to be positive definite.</p>
<p>The correlation objective function is the sum of the top <span class="math notranslate nohighlight">\(k\)</span>
singular values of the matrix <span class="math notranslate nohighlight">\(T\)</span>, where</p>
<div class="math notranslate nohighlight">
\[T = \hat{\Sigma}_{11}^{-1/2}\hat{\Sigma}_{12}\hat{\Sigma}_{22}^{-1/2}\]</div>
<p>Which is the matrix norm of T. Thus, the loss is</p>
<div class="math notranslate nohighlight">
\[L(X_1, X2) = -\text{corr}\left(H_1, H_2\right) =
-\text{tr}(T^TT)^{1/2}.\]</div>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mvlearn.embed</span> <span class="kn">import</span> <span class="n">DCCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Exponential data as example of finding good correlation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">view1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">75</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">view2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">view1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">view1_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">75</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">view2_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">view1_test</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_size1</span><span class="p">,</span> <span class="n">input_size2</span> <span class="o">=</span> <span class="mi">75</span><span class="p">,</span> <span class="mi">75</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer_sizes1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer_sizes2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dcca</span> <span class="o">=</span> <span class="n">DCCA</span><span class="p">(</span><span class="n">input_size1</span><span class="p">,</span> <span class="n">input_size2</span><span class="p">,</span> <span class="n">n_components</span><span class="p">,</span> <span class="n">layer_sizes1</span><span class="p">,</span>
<span class="gp">... </span>            <span class="n">layer_sizes2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dcca</span> <span class="o">=</span> <span class="n">dcca</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">view1</span><span class="p">,</span> <span class="n">view2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">dcca</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="n">view1_test</span><span class="p">,</span> <span class="n">view2_test</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(200, 2)</span>
</pre></div>
</div>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="dcca" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[9]</td><td><em>(<a class="fn-backref" href="#id10">1</a>, <a class="fn-backref" href="#id11">2</a>, <a class="fn-backref" href="#id16">3</a>, <a class="fn-backref" href="#id17">4</a>)</em> Andrew, G., et al., &quot;Deep canonical correlation analysis.&quot; In
Proceedings of the 30th International Conference on International
Conferenceon Machine Learning, volume 28, pages 1247–1255.
JMLR.org, 2013.</td></tr>
</tbody>
</table>
<dl class="method">
<dt id="mvlearn.embed.DCCA.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>Xs</em>, <em>y=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/dcca.html#DCCA.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.DCCA.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits the deep networks for each view such that the output of the
linear CCA has maximum correlation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> (<em>list of array-likes</em><em> or </em><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a>) -- <ul>
<li>Xs length: n_views</li>
<li>Xs[i] shape: (n_samples, n_features_i)</li>
</ul>
<p>The data to fit to. Each view will receive its own embedding.</p>
</li>
<li><strong>y</strong> (<em>ignored</em>) -- Included for API compliance.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>self</strong></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">returns an instance of self.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.DCCA.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>Xs</em>, <em>return_loss=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/dcca.html#DCCA.transform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.DCCA.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Embeds data matrix(s) using the trained deep networks and fitted CCA
projection matrices. May be used for out-of-sample embeddings.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>Xs</strong> (<em>list of array-likes</em><em> or </em><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a>) -- <ul class="simple">
<li>Xs length: n_views</li>
<li>Xs[i] shape: (n_samples, n_features_i)</li>
</ul>
<p>A list of data matrices from each view to transform based on the
prior fit function. If view_idx defined, then Xs is a 2D data
matrix corresponding to a single view.</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><ul class="simple">
<li><strong>Xs_transformed</strong> (<em>list of array-likes or array-like</em>) -- Transformed samples. Same structure as Xs, but potentially
different n_features_i.</li>
<li><strong>loss</strong> (<em>float</em>) -- Average loss over data, defined as negative correlation of
transformed views. Only returned if <code class="docutils literal notranslate"><span class="pre">return_loss=True</span></code>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.DCCA.fit_transform">
<code class="descname">fit_transform</code><span class="sig-paren">(</span><em>Xs</em>, <em>y=None</em><span class="sig-paren">)</span><a class="headerlink" href="#mvlearn.embed.DCCA.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit an embedder to the data and transform the data</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> (<em>list of array-likes</em><em> or </em><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a>) -- <ul>
<li>Xs length: n_views</li>
<li>Xs[i] shape: (n_samples, n_features_i)</li>
</ul>
</li>
<li><strong>y</strong> (<em>array</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>optional</em>) -- Targets to be used if fitting the algorithm is supervised.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><p><strong>X_transformed</strong> --</p>
<ul class="simple">
<li>X_transformed length: n_views</li>
<li>X_transformed[i] shape: (n_samples, n_components_i)</li>
</ul>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">list of numpy.ndarray</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="omnibus-embedding">
<h2>Omnibus Embedding<a class="headerlink" href="#omnibus-embedding" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="mvlearn.embed.Omnibus">
<em class="property">class </em><code class="descclassname">mvlearn.embed.</code><code class="descname">Omnibus</code><span class="sig-paren">(</span><em>n_components=2</em>, <em>distance_metric='euclidean'</em>, <em>normalize='l1'</em>, <em>algorithm='randomized'</em>, <em>n_iter=5</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/omnibus.html#Omnibus"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.Omnibus" title="Permalink to this definition">¶</a></dt>
<dd><p>Omnibus computes the pairwise distances for each view. Each
of these matrices is a n x n dissimilarity matrix where n is the number
of rows in each view. Omnibus embedding <a class="footnote-reference" href="#omni" id="id12">[10]</a> is then performed
over the dissimilarity matrices and the computed embeddings are returned.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_components</strong> (<em>strictly positive int</em><em> (</em><em>default = 2</em><em>)</em>) -- Desired dimensionality of output embeddings. See graspy docs for
additional details.</li>
<li><strong>distance_metric</strong> (<em>string</em><em> (</em><em>default = 'euclidean'</em><em>)</em>) -- Distance metric used to compute pairwise distances. Metrics must
be found in sklearn.neighbors.DistanceMetric.</li>
<li><strong>normalize</strong> (<em>string</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.9)"><em>None</em></a><em> (</em><em>default = 'l1'</em><em>)</em>) -- Normalize function to use on views before computing
pairwise distances. Must be 'l2', 'l1', 'max'
or None. If None, the distance matrices will not be normalized.</li>
<li><strong>algorithm</strong> (<em>string</em><em> (</em><em>default = 'randomized'</em><em>)</em>) -- SVD solver to use. Must be 'full', 'randomized', or 'truncated'.
See graspy docs for details.</li>
<li><strong>n_iter</strong> (<em>positive int</em><em> (</em><em>default = 5</em><em>)</em>) -- Number of iterations for randomized SVD solver. See graspy docs for
details.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="mvlearn.embed.Omnibus.embeddings_">
<code class="descname">embeddings_</code><a class="headerlink" href="#mvlearn.embed.Omnibus.embeddings_" title="Permalink to this definition">¶</a></dt>
<dd><p>List of Omnibus embeddings. One embedding matrix is provided
per view. If fit() has not been called, <a href="#id27"><span class="problematic" id="id28">embeddings_</span></a> is set to
None.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">list of arrays (default = None)</td>
</tr>
</tbody>
</table>
</dd></dl>

<p class="rubric">Notes</p>
<p>From an implementation perspective, omnibus embedding is performed
using the GrasPy package's implementation graspy.embed.OmnibusEmbed
for dissimilarity matrices.</p>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="omni" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id12">[10]</a></td><td><a class="reference external" href="https://graspy.neurodata.io/tutorials/embedding/omnibus">https://graspy.neurodata.io/tutorials/embedding/omnibus</a></td></tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mvlearn.embed</span> <span class="kn">import</span> <span class="n">omnibus</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Create 2 random data views with feature sizes 50 and 100</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">view1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">view2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedder</span> <span class="o">=</span> <span class="n">omnibus</span><span class="o">.</span><span class="n">Omnibus</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([</span><span class="n">view1</span><span class="p">,</span> <span class="n">view2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">view1_hat</span><span class="p">,</span> <span class="n">view2_hat</span> <span class="o">=</span> <span class="n">embeddings</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">view1_hat</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">view2_hat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(1000, 3) (1000, 3)</span>
</pre></div>
</div>
<dl class="method">
<dt id="mvlearn.embed.Omnibus.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>Xs</em>, <em>y=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/omnibus.html#Omnibus.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.Omnibus.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with Xs and apply the embedding on Xs.
The embeddings are saved as a class attribute.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>Xs</strong> (<em>list of array-likes</em><em> or </em><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a>) -- <ul>
<li>Xs length: n_views</li>
<li>Xs[i] shape: (n_samples, n_features_i)</li>
</ul>
<p>The data to embed based on the prior fit function. Each
X in Xs will receive its own embedding.</p>
</li>
<li><strong>y</strong> (<em>ignored</em>) -- Included for API compliance.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.Omnibus.fit_transform">
<code class="descname">fit_transform</code><span class="sig-paren">(</span><em>Xs</em>, <em>y=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/omnibus.html#Omnibus.fit_transform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.Omnibus.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with Xs and apply the embedding on Xs using
the fit() function. The resulting embeddings are returned.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>Xs</strong> (<em>list of array-likes</em><em> or </em><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a>) -- <ul class="simple">
<li>Xs length: n_views</li>
<li>Xs[i] shape: (n_samples, n_features_i)</li>
</ul>
<p>The data to embed based on the prior fit function. Each
X in Xs will receive its own embedding.</p>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>y <span class="classifier-delimiter">:</span> <span class="classifier">ignored</span></dt>
<dd>Included for API compliance.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><strong>embeddings</strong> -- list of (n_samples, n_components) matrices for each X in Xs.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">list of arrays</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="multiview-multidimensional-scaling">
<h2>Multiview Multidimensional Scaling<a class="headerlink" href="#multiview-multidimensional-scaling" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="mvlearn.embed.MVMDS">
<em class="property">class </em><code class="descclassname">mvlearn.embed.</code><code class="descname">MVMDS</code><span class="sig-paren">(</span><em>n_components=2</em>, <em>num_iter=15</em>, <em>dissimilarity='euclidean'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/mvmds.html#MVMDS"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.MVMDS" title="Permalink to this definition">¶</a></dt>
<dd><p>An implementation of Classical Multiview Multidimensional Scaling for
jointly reducing the dimensions of multiple views of data <a class="footnote-reference" href="#mvmds" id="id13">[11]</a>.
A Euclidean distance matrix is created for each view, double centered,
and the k largest common eigenvectors between the matrices are found
based on the stepwise estimation of common principal components. Using
these common principal components, the views are jointly reduced and
a single view of k-dimensions is returned.</p>
<p>MVMDS is often a better alternative to PCA for multi-view data.
See the <code class="docutils literal notranslate"><span class="pre">tutorials</span></code> in the documentation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_components</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em> (</em><em>positive</em><em>)</em><em>, </em><em>default=2</em>) -- Represents the number of components that the user would like to
be returned from the algorithm. This value must be greater than
0 and less than the number of samples within each view.</li>
<li><strong>num_iter</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em> (</em><em>positive</em><em>)</em><em>, </em><em>default=15</em>) -- Number of iterations stepwise estimation goes through.</li>
<li><strong>dissimilarity</strong> (<em>{'euclidean'</em><em>, </em><em>'precomputed'}</em><em>, </em><em>default='euclidean'</em>) -- <p>Dissimilarity measure to use:</p>
<p>'euclidean':
Pairwise Euclidean distances between points in the dataset.</p>
<p>'precomputed':
Xs is treated as pre-computed dissimilarity matrices.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="mvlearn.embed.MVMDS.components_">
<code class="descname">components_</code><a class="headerlink" href="#mvlearn.embed.MVMDS.components_" title="Permalink to this definition">¶</a></dt>
<dd><p>Joint transformed MVMDS components of the input views.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape(n_samples, n_components)</td>
</tr>
</tbody>
</table>
</dd></dl>

<p class="rubric">Notes</p>
<p>Classical Multiview Multidimensional Scaling can be broken down into two
steps. The first step involves calculating the Euclidean Distance matrices,
<span class="math notranslate nohighlight">\(Z_i\)</span>, for each of the <span class="math notranslate nohighlight">\(k\)</span> views and double-centering
these matrices through the following calculations:</p>
<div class="math notranslate nohighlight">
\[\Sigma_{i}=-\frac{1}{2}J_iZ_iJ_i\]</div>
<div class="math notranslate nohighlight">
\[\text{where }J_i=I_i-{\frac {1}{n}}\mathbb{1}\mathbb{1}^T\]</div>
<p>The second step involves finding the common principal components of the
<span class="math notranslate nohighlight">\(\Sigma\)</span> matrices. These can be thought of as multiview
generalizations of the principal components found in principal component
analysis (PCA) given several covariance matrices. The central hypothesis of
the common principal component model states that given k normal populations
(views), their <span class="math notranslate nohighlight">\(p\)</span> x <span class="math notranslate nohighlight">\(p\)</span> covariance matrices
<span class="math notranslate nohighlight">\(\Sigma_{i}\)</span>, for <span class="math notranslate nohighlight">\(i = 1,2,...,k\)</span> are simultaneously
diagonalizable as:</p>
<div class="math notranslate nohighlight">
\[\Sigma_{i} = QD_i^2Q^T\]</div>
<p>where <span class="math notranslate nohighlight">\(Q\)</span> is the common <span class="math notranslate nohighlight">\(p\)</span> x <span class="math notranslate nohighlight">\(p\)</span> orthogonal matrix and
<span class="math notranslate nohighlight">\(D_i^2\)</span> are positive <span class="math notranslate nohighlight">\(p\)</span> x <span class="math notranslate nohighlight">\(p\)</span> diagonal matrices. The
<span class="math notranslate nohighlight">\(Q\)</span> matrix contains all the common principal components. The common
principal component, <span class="math notranslate nohighlight">\(q_j\)</span>, is found by solving the minimization
problem:</p>
<div class="math notranslate nohighlight">
\[\text{Minimize} \sum_{i=1}^{k}n_ilog(q_j^TS_iq_j)\]</div>
<div class="math notranslate nohighlight">
\[\text{Subject to } q_j^Tq_j = 1\]</div>
<p>where <span class="math notranslate nohighlight">\(n_i\)</span> represent the degrees of freedom and <span class="math notranslate nohighlight">\(S_i\)</span>
represent sample covariance matrices.</p>
<p>This class does not support <code class="docutils literal notranslate"><span class="pre">MVMDS.transform()</span></code> due to the iterative
nature of the algorithm and the fact that the transformation is done
during iterative fitting. Use <code class="docutils literal notranslate"><span class="pre">MVMDS.fit_transform()</span></code> to do both
fitting and transforming at once.</p>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mvlearn.embed</span> <span class="kn">import</span> <span class="n">MVMDS</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mvlearn.datasets</span> <span class="kn">import</span> <span class="n">load_UCImultifeature</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Xs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">load_UCImultifeature</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Xs</span><span class="p">))</span> <span class="c1"># number of samples in each view</span>
<span class="go">6</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Xs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># number of samples in each view</span>
<span class="go">(2000, 76)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mvmds</span> <span class="o">=</span> <span class="n">MVMDS</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Xs_reduced</span> <span class="o">=</span> <span class="n">mvmds</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Xs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Xs_reduced</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2000, 5)</span>
</pre></div>
</div>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="mvmds" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id13">[11]</a></td><td>Trendafilov, Nickolay T. “Stepwise Estimation of Common
Principal Components.” Computational Statistics &amp; Data
Analysis, 54(12):3446–3457, 2010</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id14" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[12]</td><td>Kanaan-Izquierdo, Samir, et al. &quot;Multiview: a software
package for multiview pattern recognition methods.&quot; Bioinformatics,
35(16):2877–2879, 2019</td></tr>
</tbody>
</table>
<dl class="method">
<dt id="mvlearn.embed.MVMDS.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>Xs</em>, <em>y=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/mvmds.html#MVMDS.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.MVMDS.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates dimensionally reduced components by inputting the Euclidean
distances of each view, double centering them, and using the _commonpcs
function to find common components between views. Works similarly to
traditional, single-view Multidimensional Scaling.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>Xs</strong> (<em>list of array-likes</em><em> or </em><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a>) -- <ul>
<li>Xs length: n_views</li>
<li>Xs[i] shape: (n_samples, n_features_i)</li>
</ul>
</li>
<li><strong>y</strong> (<em>ignored</em>) -- Included for API compliance.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.MVMDS.fit_transform">
<code class="descname">fit_transform</code><span class="sig-paren">(</span><em>Xs</em>, <em>y=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/mvmds.html#MVMDS.fit_transform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.MVMDS.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>&quot;
Embeds data matrix(s) using fitted projection matrices</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> (<em>list of array-likes</em><em> or </em><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a>) -- <ul>
<li>Xs length: n_views</li>
<li>Xs[i] shape: (n_samples, n_features_i)</li>
</ul>
<p>The data to embed based on the fit function.</p>
</li>
<li><strong>y</strong> (<em>ignored</em>) -- Included for API compliance.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>X_transformed</strong> -- Joint transformed MVMDS components of the input views.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)">numpy.ndarray</a>, shape(n_samples, n_components)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="split-autoencoder">
<h2>Split Autoencoder<a class="headerlink" href="#split-autoencoder" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="mvlearn.embed.SplitAE">
<em class="property">class </em><code class="descclassname">mvlearn.embed.</code><code class="descname">SplitAE</code><span class="sig-paren">(</span><em>hidden_size=64</em>, <em>num_hidden_layers=2</em>, <em>embed_size=20</em>, <em>training_epochs=10</em>, <em>batch_size=16</em>, <em>learning_rate=0.001</em>, <em>print_info=False</em>, <em>print_graph=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/splitae.html#SplitAE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.SplitAE" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements an autoencoder that creates an embedding of a view View1 and
from that embedding reconstructs View1 and another view View2, as
described in <a class="footnote-reference" href="#split" id="id15">[13]</a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>hidden_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em> (</em><em>default=64</em><em>)</em>) -- number of nodes in the hidden layers</li>
<li><strong>num_hidden_layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em> (</em><em>default=2</em><em>)</em>) -- number of hidden layers in each encoder or decoder net</li>
<li><strong>embed_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em> (</em><em>default=20</em><em>)</em>) -- size of the bottleneck vector in the autoencoder</li>
<li><strong>training_epochs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em> (</em><em>default=10</em><em>)</em>) -- how many times the network trains on the full dataset</li>
<li><strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em> (</em><em>default=16</em><em>)</em><em>:</em>) -- batch size while training the network</li>
<li><strong>learning_rate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a><em> (</em><em>default=0.001</em><em>)</em>) -- learning rate of the Adam optimizer</li>
<li><strong>print_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)"><em>bool</em></a><em> (</em><em>default=True</em><em>)</em>) -- whether or not to print errors as the network trains.</li>
<li><strong>print_graph</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)"><em>bool</em></a><em> (</em><em>default=True</em><em>)</em>) -- whether or not to graph training loss</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="mvlearn.embed.SplitAE.view1_encoder_">
<code class="descname">view1_encoder_</code><a class="headerlink" href="#mvlearn.embed.SplitAE.view1_encoder_" title="Permalink to this definition">¶</a></dt>
<dd><p>the View1 embedding network as a PyTorch module</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">torch.nn.Module</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.SplitAE.view1_decoder_">
<code class="descname">view1_decoder_</code><a class="headerlink" href="#mvlearn.embed.SplitAE.view1_decoder_" title="Permalink to this definition">¶</a></dt>
<dd><p>the View1 decoding network as a PyTorch module</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">torch.nn.Module</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.SplitAE.view2_decoder_">
<code class="descname">view2_decoder_</code><a class="headerlink" href="#mvlearn.embed.SplitAE.view2_decoder_" title="Permalink to this definition">¶</a></dt>
<dd><p>the View2 decoding network as a PyTorch module</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">torch.nn.Module</td>
</tr>
</tbody>
</table>
</dd></dl>

<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Warns:</th><td class="field-body"><ul class="first last simple">
<li><strong>In order to run SplitAE, pytorch and other certain optional dependencies</strong></li>
<li><strong>must be installed. See the installation page for details.</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/splitAE.png"><img alt="SplitAE diagram" src="../_images/splitAE.png" style="width: 250px;" /></a>
</div>
<p>In this figure <span class="math notranslate nohighlight">\(\textbf{x}\)</span> is View1 and <span class="math notranslate nohighlight">\(\textbf{y}\)</span>
is View2</p>
<p>Each encoder / decoder network is a fully connected neural net with
paramater count equal to:</p>
<div class="math notranslate nohighlight">
\[\left(\text{input_size} + \text{embed_size}\right) \cdot
\text{hidden_size} +
\sum_{1}^{\text{num_hidden_layers}-1}\text{hidden_size}^2\]</div>
<p>Where <span class="math notranslate nohighlight">\(\text{input_size}\)</span> is the number of features in View1
or View2.</p>
<p>The loss that is reduced via gradient descent is:</p>
<div class="math notranslate nohighlight">
\[J = \left(p(f(\textbf{x})) - \textbf{x}\right)^2 +
\left(q(f(\textbf{x})) - \textbf{y}\right)^2\]</div>
<p>Where <span class="math notranslate nohighlight">\(f\)</span> is the encoder, <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(q\)</span> are
the decoders, <span class="math notranslate nohighlight">\(\textbf{x}\)</span> is View1,
and <span class="math notranslate nohighlight">\(\textbf{y}\)</span> is View2.</p>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="split" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id15">[13]</a></td><td>Wang, Weiran, et al. &quot;On Deep Multi-View Representation
Learning.&quot; In Proceedings of the 32nd International Conference on
Machine Learning, 37:1083-1092, 2015.</td></tr>
</tbody>
</table>
<p>For more extensive examples, see the <code class="docutils literal notranslate"><span class="pre">tutorials</span></code> for SplitAE in this
documentation.</p>
<dl class="method">
<dt id="mvlearn.embed.SplitAE.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>Xs</em>, <em>validation_Xs=None</em>, <em>y=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/splitae.html#SplitAE.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.SplitAE.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Given two views, create and train the autoencoder.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>Xs</strong> (<em>list of array-likes</em><em> or </em><em>numpy.ndarray.</em>) -- <ul>
<li>Xs[0] is View1 and Xs[1] is View2</li>
<li>Xs length: n_views, only 2 is currently supported for splitAE.</li>
<li>Xs[i] shape: (n_samples, n_features_i)</li>
</ul>
</li>
<li><strong>validation_Xs</strong> (<em>list of array-likes</em><em> or </em><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.19)"><em>numpy.ndarray</em></a>) -- optional validation data in the same shape of Xs. If
<code class="code docutils literal notranslate"><span class="pre">print_info=True</span></code>, then validation error, calculated with
this data, will be printed as the network trains.</li>
<li><strong>y</strong> (<em>ignored</em>) -- Included for API compliance.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.SplitAE.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>Xs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/splitae.html#SplitAE.transform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.SplitAE.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform the given view with the trained autoencoder. Provide
a single view within a list.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>Xs</strong> (<em>a list of exactly one array-like</em><em>, or </em><em>an np.ndarray</em>) -- <p>Represents the View1 of some data. The array must have the same
number of columns  (features) as the View1 presented
in the <code class="code docutils literal notranslate"><span class="pre">fit(...)</span></code> step.</p>
<blockquote>
<div><ul class="simple">
<li>Xs length: 1</li>
<li>Xs[0] shape: (n_samples, n_features_0)</li>
</ul>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><ul class="simple">
<li><strong>embedding</strong> (<em>np.ndarray of shape (n_samples, embedding_size)</em>) -- the embedding of the View1 data</li>
<li><strong>view1_reconstructions</strong> (<em>np.ndarray of shape (n_samples, n_features_0)</em>) -- the reconstructed View1</li>
<li><strong>view2_prediction</strong> (<em>np.ndarray of shape (n_samples, n_features_1)</em>) -- the predicted View2</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.SplitAE.fit_transform">
<code class="descname">fit_transform</code><span class="sig-paren">(</span><em>Xs</em>, <em>y=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/splitae.html#SplitAE.fit_transform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.SplitAE.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="code docutils literal notranslate"><span class="pre">fit(Xs)</span></code> and then <code class="code docutils literal notranslate"><span class="pre">transform(Xs[:1])</span></code>.
Note that this method will be
embedding data that the autoencoder was trained on.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> (see <code class="code docutils literal notranslate"><span class="pre">fit(...)</span></code> Xs parameters) -- </li>
<li><strong>y</strong> (<em>ignored</em>) -- Included for API compliance.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">See <code class="code docutils literal notranslate"><span class="pre">transform(...)</span></code> return values.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="dcca-utilities">
<h2>DCCA Utilities<a class="headerlink" href="#dcca-utilities" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="mvlearn.embed.linear_cca">
<em class="property">class </em><code class="descclassname">mvlearn.embed.</code><code class="descname">linear_cca</code><a class="reference internal" href="../_modules/mvlearn/embed/dcca.html#linear_cca"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.linear_cca" title="Permalink to this definition">¶</a></dt>
<dd><p>Implementation of linear CCA to act on the output of the deep networks
in DCCA.</p>
<p>Consider two views <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span>. Canonical Correlation
Analysis seeks to find vectors <span class="math notranslate nohighlight">\(a_1\)</span> and <span class="math notranslate nohighlight">\(a_2\)</span> to maximize
the correlation between <span class="math notranslate nohighlight">\(X_1 a_1\)</span> and <span class="math notranslate nohighlight">\(X_2 a_2\)</span>.</p>
<dl class="attribute">
<dt id="mvlearn.embed.linear_cca.w_">
<code class="descname">w_</code><a class="headerlink" href="#mvlearn.embed.linear_cca.w_" title="Permalink to this definition">¶</a></dt>
<dd><p>w[i] : nd-array
List of the two weight matrices for projecting each view.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)">list</a> (length=2)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.linear_cca.m_">
<code class="descname">m_</code><a class="headerlink" href="#mvlearn.embed.linear_cca.m_" title="Permalink to this definition">¶</a></dt>
<dd><p>m[i] : nd-array
List of the means of the data in each view.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)">list</a> (length=2)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.linear_cca.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>H1</em>, <em>H2</em>, <em>n_components</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/dcca.html#linear_cca.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.linear_cca.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the linear CCA model to the outputs of the deep network
transformations on the two views of data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>H1</strong> (<em>nd-array</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) -- View 1 data after deep network.</li>
<li><strong>H2</strong> (<em>nd-array</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) -- View 2 data after deep network.</li>
<li><strong>n_components</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em> (</em><em>positive</em><em>)</em>) -- The output dimensionality of the CCA transformation.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.linear_cca.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>H1</em>, <em>H2</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/dcca.html#linear_cca.transform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.linear_cca.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform inputs based on already fit matrices.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>H1</strong> (<em>nd-array</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) -- View 1 data.</li>
<li><strong>H2</strong> (<em>nd-array</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) -- View 2 data.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>results</strong> -- Results of linear transformation on input data.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)">list</a>, length=2</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mvlearn.embed.cca_loss">
<em class="property">class </em><code class="descclassname">mvlearn.embed.</code><code class="descname">cca_loss</code><span class="sig-paren">(</span><em>n_components</em>, <em>use_all_singular_values</em>, <em>device</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/dcca.html#cca_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.cca_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>An implementation of the loss function of linear CCA as introduced
in the original paper for <code class="docutils literal notranslate"><span class="pre">DCCA</span></code> <a class="footnote-reference" href="#dcca" id="id16">[9]</a>. Details of how this loss
is computed can be found in the paper or in the documentation for
<code class="docutils literal notranslate"><span class="pre">DCCA</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_components</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em> (</em><em>positive</em><em>)</em>) -- The output dimensionality of the CCA transformation.</li>
<li><strong>use_all_singular_values</strong> (<em>boolean</em>) -- Whether or not to use all the singular values in the loss calculation.
If False, only use the top n_components singular values.</li>
<li><strong>device</strong> (<em>torch.device object</em>) -- The torch device being used in DCCA.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="mvlearn.embed.cca_loss.n_components_">
<code class="descname">n_components_</code><a class="headerlink" href="#mvlearn.embed.cca_loss.n_components_" title="Permalink to this definition">¶</a></dt>
<dd><p>The output dimensionality of the CCA transformation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a> (positive)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.cca_loss.use_all_singular_values_">
<code class="descname">use_all_singular_values_</code><a class="headerlink" href="#mvlearn.embed.cca_loss.use_all_singular_values_" title="Permalink to this definition">¶</a></dt>
<dd><p>Whether or not to use all the singular values in the loss calculation.
If False, only use the top <code class="docutils literal notranslate"><span class="pre">n_components</span></code> singular values.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">boolean</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.cca_loss.device_">
<code class="descname">device_</code><a class="headerlink" href="#mvlearn.embed.cca_loss.device_" title="Permalink to this definition">¶</a></dt>
<dd><p>The torch device being used in DCCA.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">torch.device object</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.cca_loss.loss">
<code class="descname">loss</code><span class="sig-paren">(</span><em>H1</em>, <em>H2</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/dcca.html#cca_loss.loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.cca_loss.loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the loss (negative correlation) between 2 views. Details can
be found in <a class="footnote-reference" href="#dcca" id="id17">[9]</a> or the documentation for <code class="docutils literal notranslate"><span class="pre">DCCA</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>H1</strong> (<em>torch.tensor</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) -- View 1 data.</li>
<li><strong>H2</strong> (<em>torch.tensor</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) -- View 2 data.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mvlearn.embed.MlpNet">
<em class="property">class </em><code class="descclassname">mvlearn.embed.</code><code class="descname">MlpNet</code><span class="sig-paren">(</span><em>layer_sizes</em>, <em>input_size</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/dcca.html#MlpNet"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.MlpNet" title="Permalink to this definition">¶</a></dt>
<dd><p>Multilayer perceptron implementation for fully connected network. Used
by <code class="docutils literal notranslate"><span class="pre">DCCA</span></code> for the fully transformation of a single view before linear
CCA. Extends <a class="reference external" href="https://pytorch.org/docs/stable/nn.html">torch.nn.Module</a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>layer_sizes</strong> (<em>list of ints</em>) -- The sizes of the layers of the deep network applied to view 1 before
CCA. For example, if the input dimensionality is 256, and there is one
hidden layer with 1024 units and the output dimensionality is 100
before applying CCA, layer_sizes1=[1024, 100].</li>
<li><strong>input_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em> (</em><em>positive</em><em>)</em>) -- The dimensionality of the input vectors to the deep network.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="mvlearn.embed.MlpNet.layers_">
<code class="descname">layers_</code><a class="headerlink" href="#mvlearn.embed.MlpNet.layers_" title="Permalink to this definition">¶</a></dt>
<dd><p>The layers in the network.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">torch.nn.ModuleList object</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.MlpNet.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/dcca.html#MlpNet.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.MlpNet.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Feed input forward through layers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>x</strong> (<em>torch.tensor</em>) -- Input tensor to transform by the network.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>x</strong> -- The output after being fed forward through network.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">torch.tensor</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.MlpNet.bfloat16">
<code class="descname">bfloat16</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; T<a class="headerlink" href="#mvlearn.embed.MlpNet.bfloat16" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> datatype.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">Module</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.MlpNet.parameters">
<code class="descname">parameters</code><span class="sig-paren">(</span><em>recurse: bool = True</em><span class="sig-paren">)</span> &#x2192; Iterator[torch.nn.parameter.Parameter]<a class="headerlink" href="#mvlearn.embed.MlpNet.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)"><em>bool</em></a>) -- if True, then yields parameters of this module
and all submodules. Otherwise, yields only parameters that
are direct members of this module.</td>
</tr>
<tr class="field-even field"><th class="field-name">Yields:</th><td class="field-body"><em>Parameter</em> -- module parameter</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.MlpNet.requires_grad_">
<code class="descname">requires_grad_</code><span class="sig-paren">(</span><em>requires_grad: bool = True</em><span class="sig-paren">)</span> &#x2192; T<a class="headerlink" href="#mvlearn.embed.MlpNet.requires_grad_" title="Permalink to this definition">¶</a></dt>
<dd><p>Change if autograd should record operations on parameters in this
module.</p>
<p>This method sets the parameters' <code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code> attributes
in-place.</p>
<p>This method is helpful for freezing part of the module for finetuning
or training parts of a model individually (e.g., GAN training).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)"><em>bool</em></a>) -- whether autograd should record operations on
parameters in this module. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">Module</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mvlearn.embed.DeepPairedNetworks">
<em class="property">class </em><code class="descclassname">mvlearn.embed.</code><code class="descname">DeepPairedNetworks</code><span class="sig-paren">(</span><em>layer_sizes1</em>, <em>layer_sizes2</em>, <em>input_size1</em>, <em>input_size2</em>, <em>n_components</em>, <em>use_all_singular_values</em>, <em>device=device(type='cpu')</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/dcca.html#DeepPairedNetworks"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.DeepPairedNetworks" title="Permalink to this definition">¶</a></dt>
<dd><p>A pair of deep networks for operating on the two views of data. Consists
of two <code class="docutils literal notranslate"><span class="pre">MlpNet</span></code> objects for transforming 2 views of data in <code class="docutils literal notranslate"><span class="pre">DCCA</span></code>.
Extends <a class="reference external" href="https://pytorch.org/docs/stable/nn.html">torch.nn.Module</a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>layer_sizes1</strong> (<em>list of ints</em>) -- The sizes of the layers of the deep network applied to view 1 before
CCA. For example, if the input dimensionality is 256, and there is one
hidden layer with 1024 units and the output dimensionality is 100
before applying CCA, layer_sizes1=[1024, 100].</li>
<li><strong>layer_sizes2</strong> (<em>list of ints</em>) -- The sizes of the layers of the deep network applied to view 2 before
CCA. Does not need to have the same hidden layer architecture as
layer_sizes1, but the final dimensionality must be the same.</li>
<li><strong>input_size1</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em> (</em><em>positive</em><em>)</em>) -- The dimensionality of the input vectors in view 1.</li>
<li><strong>input_size2</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em> (</em><em>positive</em><em>)</em>) -- The dimensionality of the input vectors in view 2.</li>
<li><strong>n_components</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em> (</em><em>positive</em><em>)</em><em>, </em><em>default=2</em>) -- The output dimensionality of the correlated projections. The deep
network will transform the data to this size. If not specified, will
be set to 2.</li>
<li><strong>use_all_singular_values</strong> (<em>boolean</em><em> (</em><em>default=False</em><em>)</em>) -- Whether or not to use all the singular values in the CCA computation
to calculate the loss. If False, only the top <code class="docutils literal notranslate"><span class="pre">n_components</span></code> singular
values are used.</li>
<li><strong>device</strong> (<em>string</em><em>, </em><em>default='cpu'</em>) -- The torch device for processing.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="mvlearn.embed.DeepPairedNetworks.model1_">
<code class="descname">model1_</code><a class="headerlink" href="#mvlearn.embed.DeepPairedNetworks.model1_" title="Permalink to this definition">¶</a></dt>
<dd><p>Deep network for view 1 transformation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">MlpNet</span></code> object</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.DeepPairedNetworks.model2_">
<code class="descname">model2_</code><a class="headerlink" href="#mvlearn.embed.DeepPairedNetworks.model2_" title="Permalink to this definition">¶</a></dt>
<dd><p>Deep network for view 2 transformation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">MlpNet</span></code> object</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="mvlearn.embed.DeepPairedNetworks.loss_">
<code class="descname">loss_</code><a class="headerlink" href="#mvlearn.embed.DeepPairedNetworks.loss_" title="Permalink to this definition">¶</a></dt>
<dd><p>Loss function for the 2 view DCCA.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">cca_loss</span></code> object</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.DeepPairedNetworks.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x1</em>, <em>x2</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/dcca.html#DeepPairedNetworks.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.DeepPairedNetworks.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Feed two views of data forward through the respective network.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x1</strong> (<em>torch.tensor</em><em>, </em><em>shape=</em><em>(</em><em>batch_size</em><em>, </em><em>n_features</em><em>)</em>) -- View 1 data to transform.</li>
<li><strong>x2</strong> (<em>torch.tensor</em><em>, </em><em>shape=</em><em>(</em><em>batch_size</em><em>, </em><em>n_features</em><em>)</em>) -- View 2 data to transform.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><p><strong>outputs</strong> --</p>
<ul class="simple">
<li>outputs[i] : torch.tensor</li>
</ul>
<p>List of the outputs from each view transformation.</p>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)">list</a>, length=2</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.DeepPairedNetworks.bfloat16">
<code class="descname">bfloat16</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; T<a class="headerlink" href="#mvlearn.embed.DeepPairedNetworks.bfloat16" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> datatype.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">Module</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.DeepPairedNetworks.parameters">
<code class="descname">parameters</code><span class="sig-paren">(</span><em>recurse: bool = True</em><span class="sig-paren">)</span> &#x2192; Iterator[torch.nn.parameter.Parameter]<a class="headerlink" href="#mvlearn.embed.DeepPairedNetworks.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)"><em>bool</em></a>) -- if True, then yields parameters of this module
and all submodules. Otherwise, yields only parameters that
are direct members of this module.</td>
</tr>
<tr class="field-even field"><th class="field-name">Yields:</th><td class="field-body"><em>Parameter</em> -- module parameter</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="mvlearn.embed.DeepPairedNetworks.requires_grad_">
<code class="descname">requires_grad_</code><span class="sig-paren">(</span><em>requires_grad: bool = True</em><span class="sig-paren">)</span> &#x2192; T<a class="headerlink" href="#mvlearn.embed.DeepPairedNetworks.requires_grad_" title="Permalink to this definition">¶</a></dt>
<dd><p>Change if autograd should record operations on parameters in this
module.</p>
<p>This method sets the parameters' <code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code> attributes
in-place.</p>
<p>This method is helpful for freezing part of the module for finetuning
or training parts of a model individually (e.g., GAN training).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)"><em>bool</em></a>) -- whether autograd should record operations on
parameters in this module. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">Module</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="dimension-selection">
<h2>Dimension Selection<a class="headerlink" href="#dimension-selection" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="mvlearn.embed.select_dimension">
<code class="descclassname">mvlearn.embed.</code><code class="descname">select_dimension</code><span class="sig-paren">(</span><em>X</em>, <em>n_components=None</em>, <em>n_elbows=2</em>, <em>threshold=None</em>, <em>return_likelihoods=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/utils.html#select_dimension"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mvlearn.embed.select_dimension" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates profile likelihood from array based on Zhu and Godsie
method <a class="footnote-reference" href="#id20" id="id19">[15]</a>. Elbows correspond to the optimal embedding
dimension.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>1d</em><em> or </em><em>2d array-like</em>) -- Input array generate profile likelihoods for. If 1d-array, it
should be sorted in decreasing order. If 2d-array, shape should be
(n_samples, n_features).</li>
<li><strong>n_components</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em>, </em><em>optional</em><em>, </em><em>default: None.</em>) -- Number of components to embed. If None, <code class="docutils literal notranslate"><span class="pre">n_components</span> <span class="pre">=</span>
<span class="pre">floor(log2(min(n_samples,</span> <span class="pre">n_features)))</span></code>.
Ignored if X is 1d-array.</li>
<li><strong>n_elbows</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em>, </em><em>optional</em><em>, </em><em>default: 2.</em>) -- Number of likelihood elbows to return. Must be &gt; 1.</li>
<li><strong>threshold</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em>, </em><em>optional</em><em>, </em><em>default: None</em>) -- If given, only consider the singular values that
are &gt; threshold. Must be &gt;= 0.</li>
<li><strong>return_likelihoods</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)"><em>bool</em></a><em>, </em><em>optional</em><em>, </em><em>default: False</em>) -- If True, returns the all likelihoods associated with each elbow.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul class="simple">
<li><strong>elbows</strong> (<em>list</em>) -- Elbows indicate subsequent optimal embedding dimensions. Number of
elbows may be less than n_elbows if there are not enough singular
values.</li>
<li><strong>sing_vals</strong> (<em>list</em>) -- The singular values associated with each elbow.</li>
<li><strong>likelihoods</strong> (<em>list of array-like</em>) -- Array of likelihoods of the corresponding to each elbow. Only
returned if <cite>return_likelihoods</cite> is True.</li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="eutils" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[14]</td><td>Code from the <a class="reference external" href="https://github.com/neurodata/graspy">https://github.com/neurodata/graspy</a> package,
reproduced and shared with permission.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id20" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id19">[15]</a></td><td>Zhu, M. and Ghodsi, A.,
&quot;Automatic dimensionality selection from the scree plot via the
use of profile likelihood. Computational Statistics &amp; Data
Analysis.&quot; 51(2):918-930, 2006</td></tr>
</tbody>
</table>
</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="decomposition.html" class="btn btn-neutral float-right" title="Decomposition" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral" title="Reference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019-2020

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
<p style="text-align: center; margin: .5rem;">
    <a href="https://www.netlify.com">
        <img src="https://www.netlify.com/img/global/badges/netlify-color-accent.svg" />
    </a>
</p>
 


</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script type="text/javascript" src="../_static/js/copybutton.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>