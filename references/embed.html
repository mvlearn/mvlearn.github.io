<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Embedding &mdash; mvlearn alpha documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/mvlearn-logo-32x32.ico"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/copybutton.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Decomposition" href="decomposition.html" />
    <link rel="prev" title="Reference" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html">
            <img src="../_static/mvlearn-logo-transparent-white.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                0.4.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Using mvlearn</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Overview of mvlearn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Install</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../install.html#installing-the-released-version-with-pip">Installing the released version with pip</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../install.html#including-optional-dependencies-for-full-functionality">Including optional dependencies for full functionality</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#installing-the-released-version-with-conda-forge">Installing the released version with conda-forge</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#python-package-dependencies">Python package dependencies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#hardware-requirements">Hardware requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#os-requirements">OS Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#testing">Testing</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../auto_examples/index.html">Examples Gallery</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#examples-on-cluster">Examples on cluster</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/cluster/plot_mv_coregularized_spectral_tutorial.html">Multiview Coregularized Spectral Clustering Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/cluster/plot_mv_spherical_kmeans_tutorial.html">Multiview Spherical KMeans Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/cluster/plot_mv_kmeans_tutorial.html">Multiview KMeans Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/cluster/plot_mv_vs_singleview_spectral.html">Multiview vs. Singleview Spectral Clustering of UCI Multiview Digits</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/cluster/plot_mv_kmeans_validation_simulated.html">Multiview vs. Singleview KMeans</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/cluster/plot_mv_spectral_tutorial.html">Multiview Spectral Clustering Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/cluster/plot_mv_spectral_validation_simulated.html">Multiview vs. Singleview Spectral Clustering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/cluster/plot_mv_spectral_validation_complex.html">Conditional Independence of Views on Multiview Spectral Clustering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/cluster/plot_mv_kmeans_validation_complex.html">Conditional Independence of Views on Multiview KMeans Clustering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#examples-on-compose">Examples on compose</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/compose/plot_multiview_construction.html">Constructing multiple views to classify singleview data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/compose/plot_pipeline_sklearn_integration.html">Integrating mvlearn with scikit-learn</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#examples-on-datasets">Examples on datasets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/datasets/plot_load_ucimultifeature.html">Loading and Viewing the UCI Multiple Features Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/datasets/plot_gaussianmixtures.html">Generating Multiview Data from Gaussian Mixtures</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/datasets/plot_nutrimouse.html">An mvlearn case study: the Nutrimouse dataset</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#examples-on-decomposition">Examples on decomposition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/decomposition/plot_group_ica_tutorial.html">ICA: a tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/decomposition/plot_mv_ica_tutorial.html">Multiview Independent Component Analysis (ICA) Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/decomposition/plot_ajive_tutorial.html">Angle-based Joint and Individual Variation Explained (AJIVE)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#examples-on-embed">Examples on embed</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/embed/plot_gcca_tutorial.html">Generalized Canonical Correlation Analysis (GCCA) Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/embed/plot_mcca_tutorial.html">CCA Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/embed/plot_dcca_tutorial.html">Deep CCA (DCCA) Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/embed/plot_omnibus_embedding.html">Omnbius Graph Embedding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/embed/plot_kmcca_pgso_tutorial.html">Partial Gram-Schmidt Orthogonalization (PGSO) for KMCCA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/embed/plot_mvmds_tutorial.html">Multidimensional Scaling (MVMDS) Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/embed/plot_cca_comparison.html">Comparing CCA Variants</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/embed/plot_kmcca_tutorial.html">Kernel MCCA (KMCCA) Tutorial</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#examples-on-plotting">Examples on plotting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plotting/plot_quick_visualize_tutorial.html">Quickly Visualizing Multiview Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plotting/plot_crossviews_plot.html">Plotting Multiview Data with a Cross-view Plot</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#examples-on-semi-supervised">Examples on semi_supervised</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/semi_supervised/plot_cotraining_regression.html">2-View Semi-Supervised Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/semi_supervised/plot_cotraining_classification.html">2-View Semi-Supervised Classification</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Reference</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Embedding</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#canonical-correlation-analysis-cca">Canonical Correlation Analysis (CCA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multiview-canonical-correlation-analysis-mcca">Multiview Canonical Correlation Analysis (MCCA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#kernel-mcca">Kernel MCCA</a></li>
<li class="toctree-l3"><a class="reference internal" href="#generalized-canonical-correlation-analysis-gcca">Generalized Canonical Correlation Analysis (GCCA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deep-canonical-correlation-analysis-dcca">Deep Canonical Correlation Analysis (DCCA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#omnibus-embedding">Omnibus Embedding</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multiview-multidimensional-scaling">Multiview Multidimensional Scaling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#split-autoencoder">Split Autoencoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dcca-utilities">DCCA Utilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dimension-selection">Dimension Selection</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="decomposition.html">Decomposition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="decomposition.html#multiview-ica">Multiview ICA</a></li>
<li class="toctree-l3"><a class="reference internal" href="decomposition.html#group-ica">Group ICA</a></li>
<li class="toctree-l3"><a class="reference internal" href="decomposition.html#group-pca">Group PCA</a></li>
<li class="toctree-l3"><a class="reference internal" href="decomposition.html#angle-based-joint-and-individual-variation-explained-ajive">Angle-Based Joint and Individual Variation Explained (AJIVE)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cluster.html">Clustering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cluster.html#multiview-spectral-clustering">Multiview Spectral Clustering</a></li>
<li class="toctree-l3"><a class="reference internal" href="cluster.html#co-regularized-multiview-spectral-clustering">Co-Regularized Multiview Spectral Clustering</a></li>
<li class="toctree-l3"><a class="reference internal" href="cluster.html#multiview-k-means">Multiview K Means</a></li>
<li class="toctree-l3"><a class="reference internal" href="cluster.html#multiview-spherical-k-means">Multiview Spherical K Means</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="semi_supervised.html">Semi-Supervised</a><ul>
<li class="toctree-l3"><a class="reference internal" href="semi_supervised.html#cotraining-classifier">Cotraining Classifier</a></li>
<li class="toctree-l3"><a class="reference internal" href="semi_supervised.html#cotraining-regressor">Cotraining Regressor</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_selection.html">Model Selection</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_selection.html#cross-validation">Cross Validation</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_selection.html#train-test-split">Train-Test Split</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="compose.html">Compose</a><ul>
<li class="toctree-l3"><a class="reference internal" href="compose.html#averagemerger">AverageMerger</a></li>
<li class="toctree-l3"><a class="reference internal" href="compose.html#concatmerger">ConcatMerger</a></li>
<li class="toctree-l3"><a class="reference internal" href="compose.html#randomgaussianprojection">RandomGaussianProjection</a></li>
<li class="toctree-l3"><a class="reference internal" href="compose.html#randomsubspacemethod">RandomSubspaceMethod</a></li>
<li class="toctree-l3"><a class="reference internal" href="compose.html#simplesplitter">SimpleSplitter</a></li>
<li class="toctree-l3"><a class="reference internal" href="compose.html#viewclassifier">ViewClassifier</a></li>
<li class="toctree-l3"><a class="reference internal" href="compose.html#viewtransformer">ViewTransformer</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="datasets.html">Multiview Datasets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="datasets.html#uci-multiple-feature-dataset">UCI multiple feature dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="datasets.html#nutrimouse-dataset">Nutrimouse dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="datasets.html#data-simulator">Data Simulator</a></li>
<li class="toctree-l3"><a class="reference internal" href="datasets.html#factor-model">Factor Model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="plotting.html">Plotting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="plotting.html#quick-visualize">Quick Visualize</a></li>
<li class="toctree-l3"><a class="reference internal" href="plotting.html#crossviews-plot">Crossviews Plot</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="utils.html">Utility Functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="utils.html#io">IO</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Information</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">Contributing to mvlearn</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#submitting-a-bug-report-or-a-feature-request">Submitting a bug report or a feature request</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#how-to-make-a-good-bug-report">How to make a good bug report</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#contributing-code">Contributing Code</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#pull-request-checklist">Pull Request Checklist</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#guidelines">Guidelines</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#coding-guidelines">Coding Guidelines</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#docstring-guidelines">Docstring Guidelines</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#api-of-mvlearn-objects">API of mvlearn Objects</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#estimators">Estimators</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#additional-functionality">Additional Functionality</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#unreleased">Unreleased</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#version-0-4-1">Version 0.4.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#version-0-4-0">Version 0.4.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../changelog.html#id3">mvlearn.compose</a></li>
<li class="toctree-l3"><a class="reference internal" href="../changelog.html#id11">mvlearn.construct</a></li>
<li class="toctree-l3"><a class="reference internal" href="../changelog.html#id13">mvlearn.decomposition</a></li>
<li class="toctree-l3"><a class="reference internal" href="../changelog.html#id15">mvlearn.embed</a></li>
<li class="toctree-l3"><a class="reference internal" href="../changelog.html#id19">mvlearn.model_selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../changelog.html#id22">mvlearn.utils</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#version-0-3-0">Version 0.3.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#patch-0-2-1">Patch 0.2.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#version-0-2-0">Version 0.2.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="../changelog.html#version-0-1-0">Version 0.1.0</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Useful Links</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/mvlearn/mvlearn">mvlearn &#64; GitHub</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pypi.org/project/mvlearn/">mvlearn &#64; PyPI</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/mvlearn/mvlearn/issues">Issue Tracker</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">mvlearn</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="index.html">Reference</a> &raquo;</li>
      <li>Embedding</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/mvlearn/mvlearn/blob/main/docs/references/embed.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="embedding">
<h1>Embedding<a class="headerlink" href="#embedding" title="Permalink to this headline">¶</a></h1>
<section id="canonical-correlation-analysis-cca">
<h2>Canonical Correlation Analysis (CCA)<a class="headerlink" href="#canonical-correlation-analysis-cca" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="mvlearn.embed.CCA">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mvlearn.embed.</span></span><span class="sig-name descname"><span class="pre">CCA</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_components</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">regs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">signal_ranks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">center</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">i_mcca_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multiview_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/cca.html#CCA"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mvlearn.embed.CCA" title="Permalink to this definition">¶</a></dt>
<dd><p>Canonical Correlation Analysis (CCA)</p>
<p>CCA inherits from MultiCCA (MCCA) but is restricted to 2 views which
allows for certain statistics to be computed about the results.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_components</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em> (</em><em>default 1</em><em>)</em>) -- Number of canonical components to compute and return.</p></li>
<li><p><strong>regs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a><em> | </em><em>'lw'</em><em> | </em><em>'oas'</em><em> | </em><em>None</em><em>, or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)"><em>list</em></a><em>, </em><em>optional</em><em> (</em><em>default None</em><em>)</em>) -- <p>CCA regularization for each data view, which can be important
for high dimensional data. A list will specify for each view
separately. If float, must be between 0 and 1 (inclusive).</p>
<ul>
<li><p>0 or None : corresponds to SUMCORR-AVGVAR MCCA.</p></li>
<li><p>1 : partial least squares SVD (generalizes to more than 2 views)</p></li>
<li><p>'lw' : Default <code class="docutils literal notranslate"><span class="pre">sklearn.covariance.ledoit_wolf</span></code> regularization</p></li>
<li><p>'oas' : Default <code class="docutils literal notranslate"><span class="pre">sklearn.covariance.oas</span></code> regularization</p></li>
</ul>
</p></li>
<li><p><strong>signal_ranks</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em>, </em><em>None</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)"><em>list</em></a><em>, </em><em>optional</em><em> (</em><em>default None</em><em>)</em>) -- The initial signal rank to compute. If None, will compute the full SVD.
A list will specify for each view separately.</p></li>
<li><p><strong>center</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)"><em>list</em></a><em> (</em><em>default True</em><em>)</em>) -- Whether or not to initially mean center the data. A list will specify
for each view separately.</p></li>
<li><p><strong>i_mcca_method</strong> (<em>'auto'</em><em> | </em><em>'svd'</em><em> | </em><em>'gevp'</em><em> (</em><em>default 'auto'</em><em>)</em>) -- Whether or not to use the SVD based method (only works with no
regularization) or the gevp based method for informative MCCA.</p></li>
<li><p><strong>multiview_output</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em><em> (</em><em>default True</em><em>)</em>) -- If True, the <code class="docutils literal notranslate"><span class="pre">.transform</span></code> method returns one dataset per view.
Otherwise, it returns one dataset, of shape (n_samples, n_components)</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.CCA.means_">
<span class="sig-name descname"><span class="pre">means_</span></span><a class="headerlink" href="#mvlearn.embed.CCA.means_" title="Permalink to this definition">¶</a></dt>
<dd><p>The means of each view, each of shape (n_features,)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of numpy.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.CCA.loadings_">
<span class="sig-name descname"><span class="pre">loadings_</span></span><a class="headerlink" href="#mvlearn.embed.CCA.loadings_" title="Permalink to this definition">¶</a></dt>
<dd><p>The loadings for each view used to project new data,
each of shape (n_features_b, n_components).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of numpy.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.CCA.common_score_norms_">
<span class="sig-name descname"><span class="pre">common_score_norms_</span></span><a class="headerlink" href="#mvlearn.embed.CCA.common_score_norms_" title="Permalink to this definition">¶</a></dt>
<dd><p>Column norms of the sum of the fitted view scores.
Used for projecting new data</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.22)">numpy.ndarray</a>, shape (n_components,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.CCA.evals_">
<span class="sig-name descname"><span class="pre">evals_</span></span><a class="headerlink" href="#mvlearn.embed.CCA.evals_" title="Permalink to this definition">¶</a></dt>
<dd><p>The generalized eigenvalue problem eigenvalues.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.22)">numpy.ndarray</a>, shape (n_components,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.CCA.n_views_">
<span class="sig-name descname"><span class="pre">n_views_</span></span><a class="headerlink" href="#mvlearn.embed.CCA.n_views_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of views</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)">int</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.CCA.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#mvlearn.embed.CCA.n_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of features in each fitted view</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">list</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.CCA.n_components_">
<span class="sig-name descname"><span class="pre">n_components_</span></span><a class="headerlink" href="#mvlearn.embed.CCA.n_components_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of components in each transformed view</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)">int</a></p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#mvlearn.embed.MCCA" title="mvlearn.embed.MCCA"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MCCA</span></code></a>, <a class="reference internal" href="#mvlearn.embed.KMCCA" title="mvlearn.embed.KMCCA"><code class="xref py py-obj docutils literal notranslate"><span class="pre">KMCCA</span></code></a></p>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="cca"><span class="brackets">1</span></dt>
<dd><p>Kettenring, J. R., &quot;Canonical Analysis of Several Sets of
Variables.&quot; Biometrika, 58:433-451, (1971)</p>
</dd>
<dt class="label" id="id1"><span class="brackets">2</span></dt>
<dd><p>Tenenhaus, A., et al. &quot;Regularized generalized canonical
correlation analysis.&quot; Psychometrika, 76:257–284, 2011</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mvlearn.embed</span> <span class="kn">import</span> <span class="n">CCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X1</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span><span class="mf">5.</span><span class="p">,</span><span class="mf">4.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X2</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.2</span><span class="p">,</span> <span class="mf">5.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">11.9</span><span class="p">,</span> <span class="mf">12.3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cca</span> <span class="o">=</span> <span class="n">CCA</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cca</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">])</span>
<span class="go">CCA()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Xs_scores</span> <span class="o">=</span> <span class="n">cca</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">])</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="multiview-canonical-correlation-analysis-mcca">
<h2>Multiview Canonical Correlation Analysis (MCCA)<a class="headerlink" href="#multiview-canonical-correlation-analysis-mcca" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="mvlearn.embed.MCCA">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mvlearn.embed.</span></span><span class="sig-name descname"><span class="pre">MCCA</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_components</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">regs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">signal_ranks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">center</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">i_mcca_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multiview_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/mcca.html#MCCA"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mvlearn.embed.MCCA" title="Permalink to this definition">¶</a></dt>
<dd><p>Multiview canonical correlation analysis for any number of views. Includes
options for regularized MCCA and informative MCCA (where a low rank PCA is
first computed).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_components</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em> | </em><em>'min'</em><em> | </em><em>'max'</em><em> | </em><em>None</em><em> (</em><em>default 1</em><em>)</em>) -- Number of final components to compute. If <cite>int</cite>, will compute that
many. If None, will compute as many as possible. 'min' and 'max' will
respectively use the minimum/maximum number of features among views.</p></li>
<li><p><strong>regs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a><em> | </em><em>'lw'</em><em> | </em><em>'oas'</em><em> | </em><em>None</em><em>, or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)"><em>list</em></a><em>, </em><em>optional</em><em> (</em><em>default None</em><em>)</em>) -- <p>MCCA regularization for each data view, which can be important
for high dimensional data. A list will specify for each view
separately. If float, must be between 0 and 1 (inclusive).</p>
<ul>
<li><p>0 or None : corresponds to SUMCORR-AVGVAR MCCA.</p></li>
<li><p>1 : partial least squares SVD (generalizes to more than 2 views)</p></li>
<li><p>'lw' : Default <code class="docutils literal notranslate"><span class="pre">sklearn.covariance.ledoit_wolf</span></code> regularization</p></li>
<li><p>'oas' : Default <code class="docutils literal notranslate"><span class="pre">sklearn.covariance.oas</span></code> regularization</p></li>
</ul>
</p></li>
<li><p><strong>signal_ranks</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em>, </em><em>None</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)"><em>list</em></a><em>, </em><em>optional</em><em> (</em><em>default None</em><em>)</em>) -- The initial signal rank to compute. If None, will compute the full SVD.
A list will specify for each view separately.</p></li>
<li><p><strong>center</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)"><em>list</em></a><em> (</em><em>default True</em><em>)</em>) -- Whether or not to initially mean center the data. A list will specify
for each view separately.</p></li>
<li><p><strong>i_mcca_method</strong> (<em>'auto'</em><em> | </em><em>'svd'</em><em> | </em><em>'gevp'</em><em> (</em><em>default 'auto'</em><em>)</em>) -- Whether or not to use the SVD based method (only works with no
regularization) or the gevp based method for informative MCCA.</p></li>
<li><p><strong>multiview_output</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em><em> (</em><em>default True</em><em>)</em>) -- If True, the <code class="docutils literal notranslate"><span class="pre">.transform</span></code> method returns one dataset per view.
Otherwise, it returns one dataset, of shape (n_samples, n_components)</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.MCCA.means_">
<span class="sig-name descname"><span class="pre">means_</span></span><a class="headerlink" href="#mvlearn.embed.MCCA.means_" title="Permalink to this definition">¶</a></dt>
<dd><p>The means of each view, each of shape (n_features,)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of numpy.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.MCCA.loadings_">
<span class="sig-name descname"><span class="pre">loadings_</span></span><a class="headerlink" href="#mvlearn.embed.MCCA.loadings_" title="Permalink to this definition">¶</a></dt>
<dd><p>The loadings for each view used to project new data,
each of shape (n_features_b, n_components).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of numpy.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.MCCA.common_score_norms_">
<span class="sig-name descname"><span class="pre">common_score_norms_</span></span><a class="headerlink" href="#mvlearn.embed.MCCA.common_score_norms_" title="Permalink to this definition">¶</a></dt>
<dd><p>Column norms of the sum of the fitted view scores.
Used for projecting new data</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.22)">numpy.ndarray</a>, shape (n_components,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.MCCA.evals_">
<span class="sig-name descname"><span class="pre">evals_</span></span><a class="headerlink" href="#mvlearn.embed.MCCA.evals_" title="Permalink to this definition">¶</a></dt>
<dd><p>The generalized eigenvalue problem eigenvalues.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.22)">numpy.ndarray</a>, shape (n_components,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.MCCA.n_views_">
<span class="sig-name descname"><span class="pre">n_views_</span></span><a class="headerlink" href="#mvlearn.embed.MCCA.n_views_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of views</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)">int</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.MCCA.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#mvlearn.embed.MCCA.n_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of features in each fitted view</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">list</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.MCCA.n_components_">
<span class="sig-name descname"><span class="pre">n_components_</span></span><a class="headerlink" href="#mvlearn.embed.MCCA.n_components_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of components in each transformed view</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)">int</a></p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#mvlearn.embed.KMCCA" title="mvlearn.embed.KMCCA"><code class="xref py py-obj docutils literal notranslate"><span class="pre">KMCCA</span></code></a></p>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="mcca"><span class="brackets">3</span></dt>
<dd><p>Kettenring, J. R., &quot;Canonical Analysis of Several Sets of
Variables.&quot; Biometrika, 58:433-451, (1971)</p>
</dd>
<dt class="label" id="id2"><span class="brackets">4</span></dt>
<dd><p>Tenenhaus, A., et al. &quot;Regularized generalized canonical
correlation analysis.&quot; Psychometrika, 76:257–284, 2011</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mvlearn.embed</span> <span class="kn">import</span> <span class="n">MCCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X1</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X2</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.2</span><span class="p">,</span> <span class="mf">5.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">11.9</span><span class="p">,</span> <span class="mf">12.3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X3</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,],</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mcca</span> <span class="o">=</span> <span class="n">MCCA</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mcca</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">X3</span><span class="p">])</span>
<span class="go">MCCA()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Xs_scores</span> <span class="o">=</span> <span class="n">mcca</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">X3</span><span class="p">])</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="kernel-mcca">
<h2>Kernel MCCA<a class="headerlink" href="#kernel-mcca" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="mvlearn.embed.KMCCA">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mvlearn.embed.</span></span><span class="sig-name descname"><span class="pre">KMCCA</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_components</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">regs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">signal_ranks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sval_thresh</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diag_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'A'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">center</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multiview_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pgso</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/kmcca.html#KMCCA"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mvlearn.embed.KMCCA" title="Permalink to this definition">¶</a></dt>
<dd><p>Kernel multi-view canonical correlation analysis.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_components</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em>, </em><em>None</em><em> (</em><em>default 1</em><em>)</em>) -- Number of components to compute. If None, will use the number of
features.</p></li>
<li><p><strong>kernel</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>callable</em><em>, or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)"><em>list</em></a><em> (</em><em>default 'linear'</em><em>)</em>) -- The kernel function to use. This is the metric argument to
<code class="docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise.pairwise_kernels</span></code>. A list will
specify for each view separately.</p></li>
<li><p><strong>kernel_params</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.10)"><em>dict</em></a><em>, or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)"><em>list</em></a><em> (</em><em>default {}</em><em>)</em>) -- Key word arguments to <code class="docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise.pairwise_kernels</span></code>.
A list will specify for each view separately.</p></li>
<li><p><strong>regs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a><em>, </em><em>None</em><em>, or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)"><em>list</em></a><em>, </em><em>optional</em><em> (</em><em>default None</em><em>)</em>) -- None equates to 0. Floats are nonnegative. The value is used to
regularize singular values in each view based on <cite>diag_mode</cite>
A list will specify the method for each view separately.</p></li>
<li><p><strong>signal_ranks</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em>, </em><em>None</em><em>, or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)"><em>list</em></a><em>, </em><em>optional</em><em> (</em><em>default None</em><em>)</em>) -- Largest SVD rank to compute for each view. If None, the full rank
decomposition will be used. A list will specify for each view
separately.</p></li>
<li><p><strong>sval_thresh</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a><em>, or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)"><em>list</em></a><em> (</em><em>default 1e-3</em><em>)</em>) -- For each view we throw out singular values of (1/n)K, the gram matrix
scaled by n_samples, below this threshold. A non-zero value deals with
singular gram matrices.</p></li>
<li><p><strong>diag_mode</strong> (<em>'A'</em><em> | </em><em>'B'</em><em> | </em><em>'C'</em><em> (</em><em>default 'A'</em><em>)</em>) -- <p>Method of regularizing singular values <cite>s</cite> with regularization
parameter <cite>r</cite></p>
<ul>
<li><p>'A' : <span class="math notranslate nohighlight">\((1 - r) * K^2 + r * K\)</span> <a class="footnote-reference brackets" href="#kmcca" id="id3">5</a></p></li>
<li><p>'B' : <span class="math notranslate nohighlight">\((1-r) (K + n/2 \kappa * I)^2\)</span> where
<span class="math notranslate nohighlight">\(\kappa = r / (1 - r)\)</span> <a class="footnote-reference brackets" href="#id7" id="id4">6</a></p></li>
<li><p>'C' : <span class="math notranslate nohighlight">\((1 - r) K^2 + r * I_n\)</span> <a class="footnote-reference brackets" href="#id8" id="id5">7</a></p></li>
</ul>
</p></li>
<li><p><strong>center</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)"><em>list</em></a><em> (</em><em>default True</em><em>)</em>) -- Whether or not to initially mean center the data. A list will
specify for each view separately.</p></li>
<li><p><strong>filter_params</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em> (</em><em>default False</em><em>)</em>) -- See <code class="docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise.pairwise_kernels</span></code> documentation.</p></li>
<li><p><strong>n_jobs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em>, </em><em>None</em><em>, </em><em>optional</em><em> (</em><em>default None</em><em>)</em>) -- Number of jobs to run in parallel when computing kernel matrices.
See <code class="docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise.pairwise_kernels</span></code> documentation.</p></li>
<li><p><strong>multiview_output</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em><em> (</em><em>default True</em><em>)</em>) -- If True, the <code class="docutils literal notranslate"><span class="pre">.transform</span></code> method returns one dataset per view.
Otherwise, it returns one dataset, of shape (n_samples, n_components)</p></li>
<li><p><strong>pgso</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em><em> (</em><em>default False</em><em>)</em>) -- If True, computes a partial Gram-Schmidt orthogonalization
approximation of the kernel matrices to the given tolerance.</p></li>
<li><p><strong>tol</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a><em> (</em><em>default 0.1</em><em>)</em>) -- The minimum matrix trace difference between a kernel matrix and its
computed pgso approximation, relative to the kernel trace.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.KMCCA.kernel_col_means_">
<span class="sig-name descname"><span class="pre">kernel_col_means_</span></span><a class="headerlink" href="#mvlearn.embed.KMCCA.kernel_col_means_" title="Permalink to this definition">¶</a></dt>
<dd><p>The column means of each gram matrix</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of numpy.ndarray, shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.KMCCA.kernel_mat_means_">
<span class="sig-name descname"><span class="pre">kernel_mat_means_</span></span><a class="headerlink" href="#mvlearn.embed.KMCCA.kernel_mat_means_" title="Permalink to this definition">¶</a></dt>
<dd><p>The total means of each gram matrix</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">list</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.KMCCA.dual_vars_">
<span class="sig-name descname"><span class="pre">dual_vars_</span></span><a class="headerlink" href="#mvlearn.embed.KMCCA.dual_vars_" title="Permalink to this definition">¶</a></dt>
<dd><p>The loadings for the gram matrix of each view</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.22)">numpy.ndarray</a>, shape (n_views, n_samples, n_components)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.KMCCA.common_score_norms_">
<span class="sig-name descname"><span class="pre">common_score_norms_</span></span><a class="headerlink" href="#mvlearn.embed.KMCCA.common_score_norms_" title="Permalink to this definition">¶</a></dt>
<dd><p>Column norms of the sum of the view scores.
Useful for projecting new data</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.22)">numpy.ndarray</a>, shape (n_components,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.KMCCA.evals_">
<span class="sig-name descname"><span class="pre">evals_</span></span><a class="headerlink" href="#mvlearn.embed.KMCCA.evals_" title="Permalink to this definition">¶</a></dt>
<dd><p>The generalized eigenvalue problem eigenvalues.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.22)">numpy.ndarray</a>, shape (n_components,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.KMCCA.Xs_">
<span class="sig-name descname"><span class="pre">Xs_</span></span><a class="headerlink" href="#mvlearn.embed.KMCCA.Xs_" title="Permalink to this definition">¶</a></dt>
<dd><ul class="simple">
<li><p>Xs[i] shape (n_samples, n_features_i)</p></li>
</ul>
<p>The original data matrices for use in kernel matrix computation
during calls to <code class="docutils literal notranslate"><span class="pre">.transform</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of numpy.ndarray, length (n_views,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.KMCCA.n_views_">
<span class="sig-name descname"><span class="pre">n_views_</span></span><a class="headerlink" href="#mvlearn.embed.KMCCA.n_views_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of views</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)">int</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.KMCCA.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#mvlearn.embed.KMCCA.n_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of features in each fitted view</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">list</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.KMCCA.n_components_">
<span class="sig-name descname"><span class="pre">n_components_</span></span><a class="headerlink" href="#mvlearn.embed.KMCCA.n_components_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of components in each transformed view</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)">int</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.KMCCA.pgso_Ls_">
<span class="sig-name descname"><span class="pre">pgso_Ls_</span></span><a class="headerlink" href="#mvlearn.embed.KMCCA.pgso_Ls_" title="Permalink to this definition">¶</a></dt>
<dd><ul class="simple">
<li><p>pgso_Ls_[i] shape (n_samples, rank_i)</p></li>
</ul>
<p>The Gram-Schmidt approximations of the kernel matrices</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of numpy.ndarray, length (n_views,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.KMCCA.pgso_norms_">
<span class="sig-name descname"><span class="pre">pgso_norms_</span></span><a class="headerlink" href="#mvlearn.embed.KMCCA.pgso_norms_" title="Permalink to this definition">¶</a></dt>
<dd><ul class="simple">
<li><p>pgso_norms_[i] shape (rank_i,)</p></li>
</ul>
<p>The maximum norms found during the Gram-Schmidt procedure, descending</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of numpy.ndarray, length (n_views,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.KMCCA.pgso_idxs_">
<span class="sig-name descname"><span class="pre">pgso_idxs_</span></span><a class="headerlink" href="#mvlearn.embed.KMCCA.pgso_idxs_" title="Permalink to this definition">¶</a></dt>
<dd><ul class="simple">
<li><p>pgso_idxs_[i] shape (rank_i,)</p></li>
</ul>
<p>The sample indices of the maximum norms</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of numpy.ndarray, length (n_views,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.KMCCA.pgso_Xs_">
<span class="sig-name descname"><span class="pre">pgso_Xs_</span></span><a class="headerlink" href="#mvlearn.embed.KMCCA.pgso_Xs_" title="Permalink to this definition">¶</a></dt>
<dd><ul class="simple">
<li><p>pgso_Xs_[i] shape (rank_i, n_features)</p></li>
</ul>
<p>The samples with indices saved in <a href="#id20"><span class="problematic" id="id21">pgso_idxs_</span></a>, sorted by <a href="#id22"><span class="problematic" id="id23">pgso_norms_</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of numpy.ndarray, length (n_views,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.KMCCA.pgso_ranks_">
<span class="sig-name descname"><span class="pre">pgso_ranks_</span></span><a class="headerlink" href="#mvlearn.embed.KMCCA.pgso_ranks_" title="Permalink to this definition">¶</a></dt>
<dd><p>The ranks of the partial Gram-Schmidt results for each view.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">list</a>, length (n_views,)</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Notes</p>
<p>Traditional CCA aims to find useful projections of features in each view
of data, computing a weighted sum, but may not extract useful descriptors
of the data because of its linearity. KMCCA offers an alternative solution
by first projecting the data onto a higher dimensional feature space.</p>
<div class="math notranslate nohighlight">
\[\phi: \mathbf{x} = (x_1,...,x_m) \mapsto
\phi(\mathbf{x}) = (z_1,...,z_N),
(m &lt;&lt; N)\]</div>
<p>before performing CCA in the new feature space.</p>
<p>Kernels are effectively distance functions that compute inner products in
the higher dimensional feature space, a method known as the kernel trick.
A kernel function K, such that for all <span class="math notranslate nohighlight">\(\mathbf{x},
\mathbf{z} \in X\)</span></p>
<div class="math notranslate nohighlight">
\[K(\mathbf{x}, \mathbf{z}) = \langle\phi(\mathbf{x})
\cdot \phi(\mathbf{z})\rangle.\]</div>
<p>The kernel matrix <span class="math notranslate nohighlight">\(K_i\)</span> has entries computed from the kernel
function. Using the kernel trick, loadings of the kernel matrix
(<a href="#id24"><span class="problematic" id="id25">dual_vars_</span></a>) are solved for rather than of the features from <span class="math notranslate nohighlight">\(\phi\)</span>.</p>
<p>Kernel matrices grow exponentially with the size of data. They not only
have to store <span class="math notranslate nohighlight">\(n^2\)</span> elements, but also face the complexity of matrix
eigenvalue problems. In a Cholesky decomposition a positive definite
matrix K is decomposed to a lower triangular matrix <span class="math notranslate nohighlight">\(L\)</span> :
<span class="math notranslate nohighlight">\(K = LL'\)</span>.</p>
<p>The dual partial Gram-Schmidt orthogonalization (PSGO) is equivalent to the
Incomplete Cholesky Decomposition (ICD) which looks for a low rank
approximation of <span class="math notranslate nohighlight">\(L\)</span>, reducing the cost of operations of the matrix
such that <span class="math notranslate nohighlight">\(\frac{1}{\sum_i K_{ii}} tr(K - LL^T) \leq tol\)</span>.</p>
<p>A PSGO tolerance yielding rank <span class="math notranslate nohighlight">\(m\)</span> leads to storage requirements of
<span class="math notranslate nohighlight">\(O(mn)\)</span> instead of <span class="math notranslate nohighlight">\(O(n^2)\)</span> and becomes <span class="math notranslate nohighlight">\(O(nm^2)\)</span> instead
of <span class="math notranslate nohighlight">\(O(n^3)\)</span> <a class="footnote-reference brackets" href="#id8" id="id6">7</a>.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#mvlearn.embed.MCCA" title="mvlearn.embed.MCCA"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MCCA</span></code></a></p>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="kmcca"><span class="brackets"><a class="fn-backref" href="#id3">5</a></span></dt>
<dd><p>Hardoon D., et al. &quot;Canonical Correlation Analysis: An
Overview with Application to Learning Methods&quot;, Neural
Computation, Volume 16 (12), pp 2639-2664, 2004.</p>
</dd>
<dt class="label" id="id7"><span class="brackets"><a class="fn-backref" href="#id4">6</a></span></dt>
<dd><p>Bach, F. and Jordan, M. &quot;Kernel Independent Component
Analysis.&quot; Journal of Machine Learning Research, 3:1-48, 2002.</p>
</dd>
<dt class="label" id="id8"><span class="brackets">7</span><span class="fn-backref">(<a href="#id5">1</a>,<a href="#id6">2</a>)</span></dt>
<dd><p>Kuss, M. and Graepel, T.. &quot;The Geometry of Kernel Canonical
Correlation Analysis.&quot; MPI Technical Report, 108. (2003).</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mvlearn.embed</span> <span class="kn">import</span> <span class="n">KMCCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X1</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X2</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.2</span><span class="p">,</span> <span class="mf">5.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">11.9</span><span class="p">,</span> <span class="mf">12.3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X3</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,],</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kmcca</span> <span class="o">=</span> <span class="n">KMCCA</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kmcca</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">X3</span><span class="p">])</span>
<span class="go">KMCCA()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Xs_scores</span> <span class="o">=</span> <span class="n">kmcca</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">X3</span><span class="p">])</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="generalized-canonical-correlation-analysis-gcca">
<h2>Generalized Canonical Correlation Analysis (GCCA)<a class="headerlink" href="#generalized-canonical-correlation-analysis-gcca" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="mvlearn.embed.GCCA">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mvlearn.embed.</span></span><span class="sig-name descname"><span class="pre">GCCA</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_components</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fraction_var</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sv_tolerance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_elbows</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tall</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_rank</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/gcca.html#GCCA"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mvlearn.embed.GCCA" title="Permalink to this definition">¶</a></dt>
<dd><p>An implementation of Generalized Canonical Correlation Analysis <a class="footnote-reference brackets" href="#gcca" id="id9">8</a>
suitable for cases where the number of features exceeds the number of
samples by first applying single view dimensionality reduction. Computes
individual projections into a common subspace such that the correlations
between pairwise projections are minimized (ie. maximize pairwise
correlation). An important note: this is applicable to any number of
views, not just two.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_components</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em> (</em><em>positive</em><em>)</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) -- If <code class="docutils literal notranslate"><span class="pre">self.sv_tolerance=None</span></code>, selects the number of SVD
components to keep for each view. If none, another selection
method is used.</p></li>
<li><p><strong>fraction_var</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a><em>, </em><em>default=None</em>) -- If <code class="docutils literal notranslate"><span class="pre">self.sv_tolerance=None</span></code>, and <code class="docutils literal notranslate"><span class="pre">self.n_components=None</span></code>,
selects the number of SVD components to keep for each view by
capturing enough of the variance. If none, another selection
method is used.</p></li>
<li><p><strong>sv_tolerance</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a><em>, </em><em>optional</em><em>, </em><em>default=None</em>) -- Selects the number of SVD components to keep for each view by
thresholding singular values. If none, another selection
method is used.</p></li>
<li><p><strong>n_elbows</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em>, </em><em>optional</em><em>, </em><em>default: 2</em>) -- If <code class="docutils literal notranslate"><span class="pre">self.fraction_var=None</span></code>, <code class="docutils literal notranslate"><span class="pre">self.sv_tolerance=None</span></code>, and
<code class="docutils literal notranslate"><span class="pre">self.n_components=None</span></code>, then compute the optimal embedding
dimension using <code class="xref py py-func docutils literal notranslate"><span class="pre">utils.select_dimension()</span></code>.
Otherwise, ignored.</p></li>
<li><p><strong>tall</strong> (<em>boolean</em><em>, </em><em>default=False</em>) -- Set to true if n_samples &gt; n_features, speeds up SVD</p></li>
<li><p><strong>max_rank</strong> (<em>boolean</em><em>, </em><em>default=False</em>) -- If true, sets the rank of the common latent space as the maximum rank
of the individual spaces. If false, uses the minimum individual rank.</p></li>
<li><p><strong>n_jobs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em> (</em><em>positive</em><em>)</em><em>, </em><em>default=None</em>) -- The number of jobs to run in parallel when computing the SVDs for each
view in <cite>fit</cite> and <cite>partial_fit</cite>. <cite>None</cite> means 1 job, <cite>-1</cite> means using
all processors.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.GCCA.projection_mats_">
<span class="sig-name descname"><span class="pre">projection_mats_</span></span><a class="headerlink" href="#mvlearn.embed.GCCA.projection_mats_" title="Permalink to this definition">¶</a></dt>
<dd><p>A projection matrix for each view, from the given space to the
latent space</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of arrays</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.GCCA.ranks_">
<span class="sig-name descname"><span class="pre">ranks_</span></span><a class="headerlink" href="#mvlearn.embed.GCCA.ranks_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of left singular vectors kept for each view during the first
SVD</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of ints</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Notes</p>
<p>Consider two views <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span>. Canonical Correlation
Analysis seeks to find vectors <span class="math notranslate nohighlight">\(a_1\)</span> and <span class="math notranslate nohighlight">\(a_2\)</span> to maximize
the correlation <span class="math notranslate nohighlight">\(X_1 a_1\)</span> and <span class="math notranslate nohighlight">\(X_2 a_2\)</span>, expanded below.</p>
<div class="math notranslate nohighlight">
\[\left(\frac{a_1^TC_{12}a_2}
    {\sqrt{a_1^TC_{11}a_1a_2^TC_{22}a_2}}
    \right)\]</div>
<p>where <span class="math notranslate nohighlight">\(C_{11}\)</span>, <span class="math notranslate nohighlight">\(C_{22}\)</span>, and <span class="math notranslate nohighlight">\(C_{12}\)</span> are respectively
the view 1, view 2, and between view covariance matrix estimates. GCCA
maximizes the sum of these correlations across all pairwise views and
computes a set of linearly independent components. This specific algorithm
first applies principal component analysis (PCA) independently to each view
and then aligns the most informative projections to find correlated and
informative subspaces. Parameters that control the embedding dimension
apply to the PCA step. The dimension of each aligned subspace is the
maximum or minimum of the individual dimensions, per the <cite>max_ranks</cite>
parameter. Using the maximum will capture the most information from all
views but also noise from some views. Using the minimum will better remove
noise dimensions but at the cost of information from some views.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="gcca"><span class="brackets"><a class="fn-backref" href="#id9">8</a></span></dt>
<dd><p>B. Afshin-Pour, G.A. Hossein-Zadeh, S.C. Strother, H.
Soltanian-Zadeh. &quot;Enhancing reproducibility of fMRI statistical
maps using generalized canonical correlation analysis in NPAIRS
framework.&quot; Neuroimage, volume 60, pp. 1970-1981, 2012</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mvlearn.datasets</span> <span class="kn">import</span> <span class="n">load_UCImultifeature</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mvlearn.embed</span> <span class="kn">import</span> <span class="n">GCCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Load full dataset, labels not needed</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Xs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">load_UCImultifeature</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gcca</span> <span class="o">=</span> <span class="n">GCCA</span><span class="p">(</span><span class="n">fraction_var</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Transform the first 5 views</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Xs_latents</span> <span class="o">=</span> <span class="n">gcca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Xs</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">([</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">X</span> <span class="ow">in</span> <span class="n">Xs_latents</span><span class="p">])</span>
<span class="go">[9, 9, 9, 9, 9]</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="deep-canonical-correlation-analysis-dcca">
<h2>Deep Canonical Correlation Analysis (DCCA)<a class="headerlink" href="#deep-canonical-correlation-analysis-dcca" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="mvlearn.embed.DCCA">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mvlearn.embed.</span></span><span class="sig-name descname"><span class="pre">DCCA</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_size2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_components</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_sizes1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_sizes2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_all_singular_values</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">device(type='cpu')</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch_num</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">800</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reg_par</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tolerance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">print_train_log_info</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/dcca.html#DCCA"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mvlearn.embed.DCCA" title="Permalink to this definition">¶</a></dt>
<dd><p>An implementation of Deep Canonical Correlation Analysis <a class="footnote-reference brackets" href="#dcca" id="id10">9</a> with
PyTorch. It computes projections into a common subspace in order to
maximize the correlation between pairwise projections into the subspace
from two views of data. To obtain these projections, two fully connected
deep networks are trained to initially transform the two views of data.
Then, the transformed data is projected using linear CCA. This can be
thought of as training a kernel for each view that initially acts on the
data before projection. The networks are trained to maximize the ability
of the linear CCA to maximize the correlation between the final
dimensions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size1</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em> (</em><em>positive</em><em>)</em>) -- The dimensionality of the input vectors in view 1.</p></li>
<li><p><strong>input_size2</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em> (</em><em>positive</em><em>)</em>) -- The dimensionality of the input vectors in view 2.</p></li>
<li><p><strong>n_components</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em> (</em><em>positive</em><em>)</em><em>, </em><em>default=2</em>) -- The output dimensionality of the correlated projections. The deep
network wil transform the data to this size. Must satisfy:
<code class="docutils literal notranslate"><span class="pre">n_components</span></code> &lt;= max(layer_sizes1[-1], layer_sizes2[-1]).</p></li>
<li><p><strong>layer_sizes1</strong> (<em>list of ints</em><em>, </em><em>default=None</em>) -- The sizes of the layers of the deep network applied to view 1 before
CCA. For example, if the input dimensionality is 256, and there is one
hidden layer with 1024 units and the output dimensionality is 100
before applying CCA, layer_sizes1=[1024, 100]. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, set to
[1000, <code class="docutils literal notranslate"><span class="pre">self.n_components_</span></code>].</p></li>
<li><p><strong>layer_sizes2</strong> (<em>list of ints</em><em>, </em><em>default=None</em>) -- The sizes of the layers of the deep network applied to view 2 before
CCA. Does not need to have the same hidden layer architecture as
layer_sizes1, but the final dimensionality must be the same. If
<code class="docutils literal notranslate"><span class="pre">None</span></code>, set to [1000, <code class="docutils literal notranslate"><span class="pre">self.n_components_</span></code>].</p></li>
<li><p><strong>use_all_singular_values</strong> (<em>boolean</em><em> (</em><em>default=False</em><em>)</em>) -- Whether or not to use all the singular values in the CCA computation
to calculate the loss. If False, only the top <code class="docutils literal notranslate"><span class="pre">n_components</span></code>
singular values are used.</p></li>
<li><p><strong>device</strong> (<em>string</em><em>, </em><em>default='cpu'</em>) -- The torch device for processing. Can be used with a GPU if available.</p></li>
<li><p><strong>epoch_num</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em> (</em><em>positive</em><em>)</em><em>, </em><em>default=200</em>) -- The max number of epochs to train the deep networks.</p></li>
<li><p><strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em> (</em><em>positive</em><em>)</em><em>, </em><em>default=800</em>) -- Batch size for training the deep networks.</p></li>
<li><p><strong>learning_rate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a><em> (</em><em>positive</em><em>)</em><em>, </em><em>default=1e-3</em>) -- Learning rate for training the deep networks.</p></li>
<li><p><strong>reg_par</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a><em> (</em><em>positive</em><em>)</em><em>, </em><em>default=1e-5</em>) -- Weight decay parameter used in the RMSprop optimizer.</p></li>
<li><p><strong>tolerance</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a><em>, </em><em>(</em><em>positive</em><em>)</em><em>, </em><em>default=1e-2</em>) -- Threshold difference between successive iteration losses to define
convergence and stop training.</p></li>
<li><p><strong>print_train_log_info</strong> (<em>boolean</em><em>, </em><em>default=False</em>) -- If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the training loss at each epoch will be printed to the
console when DCCA.fit() is called.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.DCCA.input_size1_">
<span class="sig-name descname"><span class="pre">input_size1_</span></span><a class="headerlink" href="#mvlearn.embed.DCCA.input_size1_" title="Permalink to this definition">¶</a></dt>
<dd><p>The dimensionality of the input vectors in view 1.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)">int</a> (positive)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.DCCA.input_size2_">
<span class="sig-name descname"><span class="pre">input_size2_</span></span><a class="headerlink" href="#mvlearn.embed.DCCA.input_size2_" title="Permalink to this definition">¶</a></dt>
<dd><p>The dimensionality of the input vectors in view 2.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)">int</a> (positive)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.DCCA.n_components_">
<span class="sig-name descname"><span class="pre">n_components_</span></span><a class="headerlink" href="#mvlearn.embed.DCCA.n_components_" title="Permalink to this definition">¶</a></dt>
<dd><p>The output dimensionality of the correlated projections. The deep
network wil transform the data to this size. If not specified, will
be set to 2.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)">int</a> (positive)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.DCCA.layer_sizes1_">
<span class="sig-name descname"><span class="pre">layer_sizes1_</span></span><a class="headerlink" href="#mvlearn.embed.DCCA.layer_sizes1_" title="Permalink to this definition">¶</a></dt>
<dd><p>The sizes of the layers of the deep network applied to view 1 before
CCA. For example, if the input dimensionality is 256, and there is one
hidden layer with 1024 units and the output dimensionality is 100
before applying CCA, layer_sizes1=[1024, 100].</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of ints</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.DCCA.layer_sizes2_">
<span class="sig-name descname"><span class="pre">layer_sizes2_</span></span><a class="headerlink" href="#mvlearn.embed.DCCA.layer_sizes2_" title="Permalink to this definition">¶</a></dt>
<dd><p>The sizes of the layers of the deep network applied to view 2 before
CCA. Does not need to have the same hidden layer architecture as
layer_sizes1, but the final dimensionality must be the same.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of ints</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.DCCA.device_">
<span class="sig-name descname"><span class="pre">device_</span></span><a class="headerlink" href="#mvlearn.embed.DCCA.device_" title="Permalink to this definition">¶</a></dt>
<dd><p>The torch device for processing.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.DCCA.batch_size_">
<span class="sig-name descname"><span class="pre">batch_size_</span></span><a class="headerlink" href="#mvlearn.embed.DCCA.batch_size_" title="Permalink to this definition">¶</a></dt>
<dd><p>Batch size for training the deep networks.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)">int</a> (positive)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.DCCA.learning_rate_">
<span class="sig-name descname"><span class="pre">learning_rate_</span></span><a class="headerlink" href="#mvlearn.embed.DCCA.learning_rate_" title="Permalink to this definition">¶</a></dt>
<dd><p>Learning rate for training the deep networks.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)">float</a> (positive)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.DCCA.reg_par_">
<span class="sig-name descname"><span class="pre">reg_par_</span></span><a class="headerlink" href="#mvlearn.embed.DCCA.reg_par_" title="Permalink to this definition">¶</a></dt>
<dd><p>Weight decay parameter used in the RMSprop optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)">float</a> (positive)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.DCCA.deep_model_">
<span class="sig-name descname"><span class="pre">deep_model_</span></span><a class="headerlink" href="#mvlearn.embed.DCCA.deep_model_" title="Permalink to this definition">¶</a></dt>
<dd><p>2 view Deep CCA object used to transform 2 views of data together.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">DeepPairedNetworks</span></code> object</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.DCCA.linear_cca_">
<span class="sig-name descname"><span class="pre">linear_cca_</span></span><a class="headerlink" href="#mvlearn.embed.DCCA.linear_cca_" title="Permalink to this definition">¶</a></dt>
<dd><p>Linear CCA object used to project final transformations from output
of <code class="docutils literal notranslate"><span class="pre">deep_model</span></code> to the <code class="docutils literal notranslate"><span class="pre">n_components</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">linear_cca</span></code> object</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.DCCA.model_">
<span class="sig-name descname"><span class="pre">model_</span></span><a class="headerlink" href="#mvlearn.embed.DCCA.model_" title="Permalink to this definition">¶</a></dt>
<dd><p>Wrapper around <code class="docutils literal notranslate"><span class="pre">deep_model</span></code> to allow parallelisation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>torch.nn.DataParallel object</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.DCCA.loss_">
<span class="sig-name descname"><span class="pre">loss_</span></span><a class="headerlink" href="#mvlearn.embed.DCCA.loss_" title="Permalink to this definition">¶</a></dt>
<dd><p>Loss function for <code class="docutils literal notranslate"><span class="pre">deep_model</span></code>. Defined as the negative correlation
between outputs of transformed views.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">cca_loss</span></code> object</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.DCCA.optimizer_">
<span class="sig-name descname"><span class="pre">optimizer_</span></span><a class="headerlink" href="#mvlearn.embed.DCCA.optimizer_" title="Permalink to this definition">¶</a></dt>
<dd><p>Optimizer used to train the networks.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>torch.optim.RMSprop object</p>
</dd>
</dl>
</dd></dl>

<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ModuleNotFoundError" title="(in Python v3.10)"><strong>ModuleNotFoundError</strong></a> -- In order to run DCCA, pytorch and other certain optional
    dependencies must be installed. See the installation page
    for details.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Deep Canonical Correlation Analysis is a method of finding highly
correlated subspaces for 2 views of data using nonlinear transformations
learned by deep networks. It can be thought of as using deep networks
to learn the best potentially nonlinear kernels for a variant of kernel
CCA.</p>
<p>The networks used for each view in DCCA consist of fully connected linear
layers with a sigmoid activation function.</p>
<p>The problem DCCA problem is formulated from <a class="footnote-reference brackets" href="#dcca" id="id11">9</a>. Consider two
views <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span>. DCCA seeks to find the parameters for
each view, <span class="math notranslate nohighlight">\(\Theta_1\)</span> and <span class="math notranslate nohighlight">\(\Theta_2\)</span>, such that they maximize</p>
<div class="math notranslate nohighlight">
\[\text{corr}\left(f_1\left(X_1;\Theta_1\right),
f_2\left(X_2;\Theta_2\right)\right)\]</div>
<p>These parameters are estimated in the deep network by following gradient
descent on the input data. Taking <span class="math notranslate nohighlight">\(H_1, H_2 \in R^{o \times m}\)</span> to
be the outputs of the deep network in each column for the input data of
size <span class="math notranslate nohighlight">\(m\)</span>. Take the centered matrix <span class="math notranslate nohighlight">\(\bar{H}_1 =
H_1-\frac{1}{m}H_1{1}\)</span>, and <span class="math notranslate nohighlight">\(\bar{H}_2 = H_2-\frac{1}{m}H_2{1}\)</span>.
Then, define</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\hat{\Sigma}_{12} &amp;= \frac{1}{m-1}\bar{H}_1\bar{H}_2^T \\
\hat{\Sigma}_{11} &amp;= \frac{1}{m-1}\bar{H}_1\bar{H}_1^T + r_1I \\
\hat{\Sigma}_{22} &amp;= \frac{1}{m-1}\bar{H}_2\bar{H}_2^T + r_2I
\end{align*}\end{split}\]</div>
<p>Where <span class="math notranslate nohighlight">\(r_1\)</span> and <span class="math notranslate nohighlight">\(r_2\)</span> are regularization constants <span class="math notranslate nohighlight">\(&gt;0\)</span>
so the matrices are guaranteed to be positive definite.</p>
<p>The correlation objective function is the sum of the top <span class="math notranslate nohighlight">\(k\)</span>
singular values of the matrix <span class="math notranslate nohighlight">\(T\)</span>, where</p>
<div class="math notranslate nohighlight">
\[T = \hat{\Sigma}_{11}^{-1/2}\hat{\Sigma}_{12}\hat{\Sigma}_{22}^{-1/2}\]</div>
<p>Which is the matrix norm of T. Thus, the loss is</p>
<div class="math notranslate nohighlight">
\[L(X_1, X2) = -\text{corr}\left(H_1, H_2\right) =
-\text{tr}(T^TT)^{1/2}.\]</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mvlearn.embed</span> <span class="kn">import</span> <span class="n">DCCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Exponential data as example of finding good correlation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">view1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">75</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">view2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">view1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">view1_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">75</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">view2_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">view1_test</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_size1</span><span class="p">,</span> <span class="n">input_size2</span> <span class="o">=</span> <span class="mi">75</span><span class="p">,</span> <span class="mi">75</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer_sizes1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer_sizes2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dcca</span> <span class="o">=</span> <span class="n">DCCA</span><span class="p">(</span><span class="n">input_size1</span><span class="p">,</span> <span class="n">input_size2</span><span class="p">,</span> <span class="n">n_components</span><span class="p">,</span> <span class="n">layer_sizes1</span><span class="p">,</span>
<span class="gp">... </span>            <span class="n">layer_sizes2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dcca</span> <span class="o">=</span> <span class="n">dcca</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">view1</span><span class="p">,</span> <span class="n">view2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">dcca</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="n">view1_test</span><span class="p">,</span> <span class="n">view2_test</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(200, 2)</span>
</pre></div>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="dcca"><span class="brackets">9</span><span class="fn-backref">(<a href="#id10">1</a>,<a href="#id11">2</a>,<a href="#id16">3</a>)</span></dt>
<dd><p>Andrew, G., et al., &quot;Deep canonical correlation analysis.&quot; In
Proceedings of the 30th International Conference on International
Conferenceon Machine Learning, volume 28, pages 1247–1255.
JMLR.org, 2013.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="omnibus-embedding">
<h2>Omnibus Embedding<a class="headerlink" href="#omnibus-embedding" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="mvlearn.embed.Omnibus">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mvlearn.embed.</span></span><span class="sig-name descname"><span class="pre">Omnibus</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_components</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distance_metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'euclidean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'l1'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'randomized'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/omnibus.html#Omnibus"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mvlearn.embed.Omnibus" title="Permalink to this definition">¶</a></dt>
<dd><p>Omnibus computes the pairwise distances for each view. Each
of these matrices is a n x n dissimilarity matrix where n is the number
of rows in each view. Omnibus embedding <a class="footnote-reference brackets" href="#omni" id="id12">10</a> is then performed
over the dissimilarity matrices and the computed embeddings are returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_components</strong> (<em>strictly positive int</em><em> (</em><em>default = 2</em><em>)</em>) -- Desired dimensionality of output embeddings. See graspy docs for
additional details.</p></li>
<li><p><strong>distance_metric</strong> (<em>string</em><em> (</em><em>default = 'euclidean'</em><em>)</em>) -- Distance metric used to compute pairwise distances. Metrics must
be found in sklearn.neighbors.DistanceMetric.</p></li>
<li><p><strong>normalize</strong> (<em>string</em><em> or </em><em>None</em><em> (</em><em>default = 'l1'</em><em>)</em>) -- Normalize function to use on views before computing
pairwise distances. Must be 'l2', 'l1', 'max'
or None. If None, the distance matrices will not be normalized.</p></li>
<li><p><strong>algorithm</strong> (<em>string</em><em> (</em><em>default = 'randomized'</em><em>)</em>) -- SVD solver to use. Must be 'full', 'randomized', or 'truncated'.
See graspy docs for details.</p></li>
<li><p><strong>n_iter</strong> (<em>positive int</em><em> (</em><em>default = 5</em><em>)</em>) -- Number of iterations for randomized SVD solver. See graspy docs for
details.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.Omnibus.embeddings_">
<span class="sig-name descname"><span class="pre">embeddings_</span></span><a class="headerlink" href="#mvlearn.embed.Omnibus.embeddings_" title="Permalink to this definition">¶</a></dt>
<dd><p>List of Omnibus embeddings. One embedding matrix is provided
per view. If fit() has not been called, <a href="#id26"><span class="problematic" id="id27">embeddings_</span></a> is set to
None.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of arrays (default = None)</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Notes</p>
<p>From an implementation perspective, omnibus embedding is performed
using the GrasPy package's implementation graspy.embed.OmnibusEmbed
for dissimilarity matrices.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="omni"><span class="brackets"><a class="fn-backref" href="#id12">10</a></span></dt>
<dd><p><a class="reference external" href="https://graspy.neurodata.io/tutorials/embedding/omnibus">https://graspy.neurodata.io/tutorials/embedding/omnibus</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mvlearn.embed</span> <span class="kn">import</span> <span class="n">omnibus</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Create 2 random data views with feature sizes 50 and 100</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">view1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">view2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedder</span> <span class="o">=</span> <span class="n">omnibus</span><span class="o">.</span><span class="n">Omnibus</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([</span><span class="n">view1</span><span class="p">,</span> <span class="n">view2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">view1_hat</span><span class="p">,</span> <span class="n">view2_hat</span> <span class="o">=</span> <span class="n">embeddings</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">view1_hat</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">view2_hat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(1000, 3) (1000, 3)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="multiview-multidimensional-scaling">
<h2>Multiview Multidimensional Scaling<a class="headerlink" href="#multiview-multidimensional-scaling" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="mvlearn.embed.MVMDS">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mvlearn.embed.</span></span><span class="sig-name descname"><span class="pre">MVMDS</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_components</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">15</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dissimilarity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'euclidean'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/mvmds.html#MVMDS"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mvlearn.embed.MVMDS" title="Permalink to this definition">¶</a></dt>
<dd><p>An implementation of Classical Multiview Multidimensional Scaling for
jointly reducing the dimensions of multiple views of data <a class="footnote-reference brackets" href="#mvmds" id="id13">11</a>.
A Euclidean distance matrix is created for each view, double centered,
and the k largest common eigenvectors between the matrices are found
based on the stepwise estimation of common principal components. Using
these common principal components, the views are jointly reduced and
a single view of k-dimensions is returned.</p>
<p>MVMDS is often a better alternative to PCA for multi-view data.
See the <code class="docutils literal notranslate"><span class="pre">tutorials</span></code> in the documentation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_components</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em> (</em><em>positive</em><em>)</em><em>, </em><em>default=2</em>) -- Represents the number of components that the user would like to
be returned from the algorithm. This value must be greater than
0 and less than the number of samples within each view.</p></li>
<li><p><strong>num_iter</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em> (</em><em>positive</em><em>)</em><em>, </em><em>default=15</em>) -- Number of iterations stepwise estimation goes through.</p></li>
<li><p><strong>dissimilarity</strong> (<em>{'euclidean'</em><em>, </em><em>'precomputed'}</em><em>, </em><em>default='euclidean'</em>) -- <p>Dissimilarity measure to use:</p>
<p>'euclidean':
Pairwise Euclidean distances between points in the dataset.</p>
<p>'precomputed':
Xs is treated as pre-computed dissimilarity matrices.</p>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.MVMDS.components_">
<span class="sig-name descname"><span class="pre">components_</span></span><a class="headerlink" href="#mvlearn.embed.MVMDS.components_" title="Permalink to this definition">¶</a></dt>
<dd><p>Joint transformed MVMDS components of the input views.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.22)">numpy.ndarray</a>, shape(n_samples, n_components)</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Notes</p>
<p>Classical Multiview Multidimensional Scaling can be broken down into two
steps. The first step involves calculating the Euclidean Distance matrices,
<span class="math notranslate nohighlight">\(Z_i\)</span>, for each of the <span class="math notranslate nohighlight">\(k\)</span> views and double-centering
these matrices through the following calculations:</p>
<div class="math notranslate nohighlight">
\[\Sigma_{i}=-\frac{1}{2}J_iZ_iJ_i\]</div>
<div class="math notranslate nohighlight">
\[\text{where }J_i=I_i-{\frac {1}{n}}\mathbb{1}\mathbb{1}^T\]</div>
<p>The second step involves finding the common principal components of the
<span class="math notranslate nohighlight">\(\Sigma\)</span> matrices. These can be thought of as multiview
generalizations of the principal components found in principal component
analysis (PCA) given several covariance matrices. The central hypothesis of
the common principal component model states that given k normal populations
(views), their <span class="math notranslate nohighlight">\(p\)</span> x <span class="math notranslate nohighlight">\(p\)</span> covariance matrices
<span class="math notranslate nohighlight">\(\Sigma_{i}\)</span>, for <span class="math notranslate nohighlight">\(i = 1,2,...,k\)</span> are simultaneously
diagonalizable as:</p>
<div class="math notranslate nohighlight">
\[\Sigma_{i} = QD_i^2Q^T\]</div>
<p>where <span class="math notranslate nohighlight">\(Q\)</span> is the common <span class="math notranslate nohighlight">\(p\)</span> x <span class="math notranslate nohighlight">\(p\)</span> orthogonal matrix and
<span class="math notranslate nohighlight">\(D_i^2\)</span> are positive <span class="math notranslate nohighlight">\(p\)</span> x <span class="math notranslate nohighlight">\(p\)</span> diagonal matrices. The
<span class="math notranslate nohighlight">\(Q\)</span> matrix contains all the common principal components. The common
principal component, <span class="math notranslate nohighlight">\(q_j\)</span>, is found by solving the minimization
problem:</p>
<div class="math notranslate nohighlight">
\[\text{Minimize} \sum_{i=1}^{k}n_ilog(q_j^TS_iq_j)\]</div>
<div class="math notranslate nohighlight">
\[\text{Subject to } q_j^Tq_j = 1\]</div>
<p>where <span class="math notranslate nohighlight">\(n_i\)</span> represent the degrees of freedom and <span class="math notranslate nohighlight">\(S_i\)</span>
represent sample covariance matrices.</p>
<p>This class does not support <code class="docutils literal notranslate"><span class="pre">MVMDS.transform()</span></code> due to the iterative
nature of the algorithm and the fact that the transformation is done
during iterative fitting. Use <code class="docutils literal notranslate"><span class="pre">MVMDS.fit_transform()</span></code> to do both
fitting and transforming at once.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mvlearn.embed</span> <span class="kn">import</span> <span class="n">MVMDS</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mvlearn.datasets</span> <span class="kn">import</span> <span class="n">load_UCImultifeature</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Xs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">load_UCImultifeature</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Xs</span><span class="p">))</span> <span class="c1"># number of samples in each view</span>
<span class="go">6</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Xs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># number of samples in each view</span>
<span class="go">(2000, 76)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mvmds</span> <span class="o">=</span> <span class="n">MVMDS</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Xs_reduced</span> <span class="o">=</span> <span class="n">mvmds</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Xs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Xs_reduced</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2000, 5)</span>
</pre></div>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="mvmds"><span class="brackets"><a class="fn-backref" href="#id13">11</a></span></dt>
<dd><p>Trendafilov, Nickolay T. “Stepwise Estimation of Common
Principal Components.” Computational Statistics &amp; Data
Analysis, 54(12):3446–3457, 2010</p>
</dd>
<dt class="label" id="id14"><span class="brackets">12</span></dt>
<dd><p>Kanaan-Izquierdo, Samir, et al. &quot;Multiview: a software
package for multiview pattern recognition methods.&quot; Bioinformatics,
35(16):2877–2879, 2019</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="split-autoencoder">
<h2>Split Autoencoder<a class="headerlink" href="#split-autoencoder" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="mvlearn.embed.SplitAE">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mvlearn.embed.</span></span><span class="sig-name descname"><span class="pre">SplitAE</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_hidden_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embed_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">print_info</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">print_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/splitae.html#SplitAE"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mvlearn.embed.SplitAE" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements an autoencoder that creates an embedding of a view View1 and
from that embedding reconstructs View1 and another view View2, as
described in <a class="footnote-reference brackets" href="#split" id="id15">13</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em> (</em><em>default=64</em><em>)</em>) -- number of nodes in the hidden layers</p></li>
<li><p><strong>num_hidden_layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em> (</em><em>default=2</em><em>)</em>) -- number of hidden layers in each encoder or decoder net</p></li>
<li><p><strong>embed_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em> (</em><em>default=20</em><em>)</em>) -- size of the bottleneck vector in the autoencoder</p></li>
<li><p><strong>training_epochs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em> (</em><em>default=10</em><em>)</em>) -- how many times the network trains on the full dataset</p></li>
<li><p><strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em> (</em><em>default=16</em><em>)</em><em>:</em>) -- batch size while training the network</p></li>
<li><p><strong>learning_rate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a><em> (</em><em>default=0.001</em><em>)</em>) -- learning rate of the Adam optimizer</p></li>
<li><p><strong>print_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em> (</em><em>default=False</em><em>)</em>) -- whether or not to print errors as the network trains.</p></li>
<li><p><strong>print_graph</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em> (</em><em>default=True</em><em>)</em>) -- whether or not to graph training loss</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.SplitAE.view1_encoder_">
<span class="sig-name descname"><span class="pre">view1_encoder_</span></span><a class="headerlink" href="#mvlearn.embed.SplitAE.view1_encoder_" title="Permalink to this definition">¶</a></dt>
<dd><p>the View1 embedding network as a PyTorch module</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>torch.nn.Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.SplitAE.view1_decoder_">
<span class="sig-name descname"><span class="pre">view1_decoder_</span></span><a class="headerlink" href="#mvlearn.embed.SplitAE.view1_decoder_" title="Permalink to this definition">¶</a></dt>
<dd><p>the View1 decoding network as a PyTorch module</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>torch.nn.Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.SplitAE.view2_decoder_">
<span class="sig-name descname"><span class="pre">view2_decoder_</span></span><a class="headerlink" href="#mvlearn.embed.SplitAE.view2_decoder_" title="Permalink to this definition">¶</a></dt>
<dd><p>the View2 decoding network as a PyTorch module</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>torch.nn.Module</p>
</dd>
</dl>
</dd></dl>

<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ModuleNotFoundError" title="(in Python v3.10)"><strong>ModuleNotFoundError</strong></a> -- In order to run SplitAE, pytorch and other certain optional
    dependencies must be installed. See the installation page for
    details.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/splitAE.png"><img alt="SplitAE diagram" src="../_images/splitAE.png" style="width: 250px;" /></a>
</figure>
<p>In this figure <span class="math notranslate nohighlight">\(\textbf{x}\)</span> is View1 and <span class="math notranslate nohighlight">\(\textbf{y}\)</span>
is View2</p>
<p>Each encoder / decoder network is a fully connected neural net with
paramater count equal to:</p>
<div class="math notranslate nohighlight">
\[\left(\text{input_size} + \text{embed_size}\right) \cdot
\text{hidden_size} +
\sum_{1}^{\text{num_hidden_layers}-1}\text{hidden_size}^2\]</div>
<p>Where <span class="math notranslate nohighlight">\(\text{input_size}\)</span> is the number of features in View1
or View2.</p>
<p>The loss that is reduced via gradient descent is:</p>
<div class="math notranslate nohighlight">
\[J = \left(p(f(\textbf{x})) - \textbf{x}\right)^2 +
\left(q(f(\textbf{x})) - \textbf{y}\right)^2\]</div>
<p>Where <span class="math notranslate nohighlight">\(f\)</span> is the encoder, <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(q\)</span> are
the decoders, <span class="math notranslate nohighlight">\(\textbf{x}\)</span> is View1,
and <span class="math notranslate nohighlight">\(\textbf{y}\)</span> is View2.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="split"><span class="brackets"><a class="fn-backref" href="#id15">13</a></span></dt>
<dd><p>Wang, Weiran, et al. &quot;On Deep Multi-View Representation
Learning.&quot; In Proceedings of the 32nd International Conference on
Machine Learning, 37:1083-1092, 2015.</p>
</dd>
</dl>
<p>For more extensive examples, see the <code class="docutils literal notranslate"><span class="pre">tutorials</span></code> for SplitAE in this
documentation.</p>
</dd></dl>

</section>
<section id="dcca-utilities">
<h2>DCCA Utilities<a class="headerlink" href="#dcca-utilities" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="mvlearn.embed.linear_cca">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mvlearn.embed.</span></span><span class="sig-name descname"><span class="pre">linear_cca</span></span><a class="reference internal" href="../_modules/mvlearn/embed/dcca.html#linear_cca"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mvlearn.embed.linear_cca" title="Permalink to this definition">¶</a></dt>
<dd><p>Implementation of linear CCA to act on the output of the deep networks
in DCCA.</p>
<p>Consider two views <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span>. Canonical Correlation
Analysis seeks to find vectors <span class="math notranslate nohighlight">\(a_1\)</span> and <span class="math notranslate nohighlight">\(a_2\)</span> to maximize
the correlation between <span class="math notranslate nohighlight">\(X_1 a_1\)</span> and <span class="math notranslate nohighlight">\(X_2 a_2\)</span>.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.linear_cca.w_">
<span class="sig-name descname"><span class="pre">w_</span></span><a class="headerlink" href="#mvlearn.embed.linear_cca.w_" title="Permalink to this definition">¶</a></dt>
<dd><p>w[i] : nd-array
List of the two weight matrices for projecting each view.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">list</a> (length=2)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.linear_cca.m_">
<span class="sig-name descname"><span class="pre">m_</span></span><a class="headerlink" href="#mvlearn.embed.linear_cca.m_" title="Permalink to this definition">¶</a></dt>
<dd><p>m[i] : nd-array
List of the means of the data in each view.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">list</a> (length=2)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mvlearn.embed.cca_loss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mvlearn.embed.</span></span><span class="sig-name descname"><span class="pre">cca_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_components</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_all_singular_values</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/dcca.html#cca_loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mvlearn.embed.cca_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>An implementation of the loss function of linear CCA as introduced
in the original paper for <code class="docutils literal notranslate"><span class="pre">DCCA</span></code> <a class="footnote-reference brackets" href="#dcca" id="id16">9</a>. Details of how this loss
is computed can be found in the paper or in the documentation for
<code class="docutils literal notranslate"><span class="pre">DCCA</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_components</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em> (</em><em>positive</em><em>)</em>) -- The output dimensionality of the CCA transformation.</p></li>
<li><p><strong>use_all_singular_values</strong> (<em>boolean</em>) -- Whether or not to use all the singular values in the loss calculation.
If False, only use the top n_components singular values.</p></li>
<li><p><strong>device</strong> (<em>torch.device object</em>) -- The torch device being used in DCCA.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.cca_loss.n_components_">
<span class="sig-name descname"><span class="pre">n_components_</span></span><a class="headerlink" href="#mvlearn.embed.cca_loss.n_components_" title="Permalink to this definition">¶</a></dt>
<dd><p>The output dimensionality of the CCA transformation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)">int</a> (positive)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.cca_loss.use_all_singular_values_">
<span class="sig-name descname"><span class="pre">use_all_singular_values_</span></span><a class="headerlink" href="#mvlearn.embed.cca_loss.use_all_singular_values_" title="Permalink to this definition">¶</a></dt>
<dd><p>Whether or not to use all the singular values in the loss calculation.
If False, only use the top <code class="docutils literal notranslate"><span class="pre">n_components</span></code> singular values.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>boolean</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.cca_loss.device_">
<span class="sig-name descname"><span class="pre">device_</span></span><a class="headerlink" href="#mvlearn.embed.cca_loss.device_" title="Permalink to this definition">¶</a></dt>
<dd><p>The torch device being used in DCCA.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>torch.device object</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mvlearn.embed.MlpNet">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mvlearn.embed.</span></span><span class="sig-name descname"><span class="pre">MlpNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer_sizes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_size</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/dcca.html#MlpNet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mvlearn.embed.MlpNet" title="Permalink to this definition">¶</a></dt>
<dd><p>Multilayer perceptron implementation for fully connected network. Used
by <code class="docutils literal notranslate"><span class="pre">DCCA</span></code> for the fully transformation of a single view before linear
CCA. Extends <a class="reference external" href="https://pytorch.org/docs/stable/nn.html">torch.nn.Module</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layer_sizes</strong> (<em>list of ints</em>) -- The sizes of the layers of the deep network applied to view 1 before
CCA. For example, if the input dimensionality is 256, and there is one
hidden layer with 1024 units and the output dimensionality is 100
before applying CCA, layer_sizes1=[1024, 100].</p></li>
<li><p><strong>input_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em> (</em><em>positive</em><em>)</em>) -- The dimensionality of the input vectors to the deep network.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.MlpNet.layers_">
<span class="sig-name descname"><span class="pre">layers_</span></span><a class="headerlink" href="#mvlearn.embed.MlpNet.layers_" title="Permalink to this definition">¶</a></dt>
<dd><p>The layers in the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>torch.nn.ModuleList object</p>
</dd>
</dl>
</dd></dl>

<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mvlearn.embed.DeepPairedNetworks">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mvlearn.embed.</span></span><span class="sig-name descname"><span class="pre">DeepPairedNetworks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer_sizes1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_sizes2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_size1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_size2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_components</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_all_singular_values</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">device(type='cpu')</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/dcca.html#DeepPairedNetworks"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mvlearn.embed.DeepPairedNetworks" title="Permalink to this definition">¶</a></dt>
<dd><p>A pair of deep networks for operating on the two views of data. Consists
of two <code class="docutils literal notranslate"><span class="pre">MlpNet</span></code> objects for transforming 2 views of data in <code class="docutils literal notranslate"><span class="pre">DCCA</span></code>.
Extends <a class="reference external" href="https://pytorch.org/docs/stable/nn.html">torch.nn.Module</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layer_sizes1</strong> (<em>list of ints</em>) -- The sizes of the layers of the deep network applied to view 1 before
CCA. For example, if the input dimensionality is 256, and there is one
hidden layer with 1024 units and the output dimensionality is 100
before applying CCA, layer_sizes1=[1024, 100].</p></li>
<li><p><strong>layer_sizes2</strong> (<em>list of ints</em>) -- The sizes of the layers of the deep network applied to view 2 before
CCA. Does not need to have the same hidden layer architecture as
layer_sizes1, but the final dimensionality must be the same.</p></li>
<li><p><strong>input_size1</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em> (</em><em>positive</em><em>)</em>) -- The dimensionality of the input vectors in view 1.</p></li>
<li><p><strong>input_size2</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em> (</em><em>positive</em><em>)</em>) -- The dimensionality of the input vectors in view 2.</p></li>
<li><p><strong>n_components</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em> (</em><em>positive</em><em>)</em><em>, </em><em>default=2</em>) -- The output dimensionality of the correlated projections. The deep
network will transform the data to this size. If not specified, will
be set to 2.</p></li>
<li><p><strong>use_all_singular_values</strong> (<em>boolean</em><em> (</em><em>default=False</em><em>)</em>) -- Whether or not to use all the singular values in the CCA computation
to calculate the loss. If False, only the top <code class="docutils literal notranslate"><span class="pre">n_components</span></code> singular
values are used.</p></li>
<li><p><strong>device</strong> (<em>string</em><em>, </em><em>default='cpu'</em>) -- The torch device for processing.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.DeepPairedNetworks.model1_">
<span class="sig-name descname"><span class="pre">model1_</span></span><a class="headerlink" href="#mvlearn.embed.DeepPairedNetworks.model1_" title="Permalink to this definition">¶</a></dt>
<dd><p>Deep network for view 1 transformation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">MlpNet</span></code> object</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.DeepPairedNetworks.model2_">
<span class="sig-name descname"><span class="pre">model2_</span></span><a class="headerlink" href="#mvlearn.embed.DeepPairedNetworks.model2_" title="Permalink to this definition">¶</a></dt>
<dd><p>Deep network for view 2 transformation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">MlpNet</span></code> object</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mvlearn.embed.DeepPairedNetworks.loss_">
<span class="sig-name descname"><span class="pre">loss_</span></span><a class="headerlink" href="#mvlearn.embed.DeepPairedNetworks.loss_" title="Permalink to this definition">¶</a></dt>
<dd><p>Loss function for the 2 view DCCA.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">cca_loss</span></code> object</p>
</dd>
</dl>
</dd></dl>

<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
</dd></dl>

</section>
<section id="dimension-selection">
<h2>Dimension Selection<a class="headerlink" href="#dimension-selection" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="mvlearn.embed.select_dimension">
<span class="sig-prename descclassname"><span class="pre">mvlearn.embed.</span></span><span class="sig-name descname"><span class="pre">select_dimension</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_components</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_elbows</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_likelihoods</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mvlearn/embed/utils.html#select_dimension"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mvlearn.embed.select_dimension" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates profile likelihood from array based on Zhu and Godsie
method <a class="footnote-reference brackets" href="#id19" id="id18">15</a>. Elbows correspond to the optimal embedding
dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>1d</em><em> or </em><em>2d array-like</em>) -- Input array generate profile likelihoods for. If 1d-array, it
should be sorted in decreasing order. If 2d-array, shape should be
(n_samples, n_features).</p></li>
<li><p><strong>n_components</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em>, </em><em>optional</em><em>, </em><em>default: None.</em>) -- Number of components to embed. If None, <code class="docutils literal notranslate"><span class="pre">n_components</span> <span class="pre">=</span>
<span class="pre">floor(log2(min(n_samples,</span> <span class="pre">n_features)))</span></code>.
Ignored if X is 1d-array.</p></li>
<li><p><strong>n_elbows</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em>, </em><em>optional</em><em>, </em><em>default: 2.</em>) -- Number of likelihood elbows to return. Must be &gt; 1.</p></li>
<li><p><strong>threshold</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em>, </em><em>optional</em><em>, </em><em>default: None</em>) -- If given, only consider the singular values that
are &gt; threshold. Must be &gt;= 0.</p></li>
<li><p><strong>return_likelihoods</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em><em>, </em><em>default: False</em>) -- If True, returns the all likelihoods associated with each elbow.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>elbows</strong> (<em>list</em>) -- Elbows indicate subsequent optimal embedding dimensions. Number of
elbows may be less than n_elbows if there are not enough singular
values.</p></li>
<li><p><strong>sing_vals</strong> (<em>list</em>) -- The singular values associated with each elbow.</p></li>
<li><p><strong>likelihoods</strong> (<em>list of array-like</em>) -- Array of likelihoods of the corresponding to each elbow. Only
returned if <cite>return_likelihoods</cite> is True.</p></li>
</ul>
</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="eutils"><span class="brackets">14</span></dt>
<dd><p>Code from the <a class="reference external" href="https://github.com/neurodata/graspy">https://github.com/neurodata/graspy</a> package,
reproduced and shared with permission.</p>
</dd>
<dt class="label" id="id19"><span class="brackets"><a class="fn-backref" href="#id18">15</a></span></dt>
<dd><p>Zhu, M. and Ghodsi, A.,
&quot;Automatic dimensionality selection from the scree plot via the
use of profile likelihood. Computational Statistics &amp; Data
Analysis.&quot; 51(2):918-930, 2006</p>
</dd>
</dl>
</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Reference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="decomposition.html" class="btn btn-neutral float-right" title="Decomposition" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019-2020.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
  
<p style="text-align: center; margin: .5rem;">
    <a href="https://www.netlify.com">
        <img src="https://www.netlify.com/img/global/badges/netlify-color-accent.svg" />
    </a>
</p>
 


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>