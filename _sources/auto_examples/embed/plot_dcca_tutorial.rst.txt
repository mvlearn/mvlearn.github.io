
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/embed/plot_dcca_tutorial.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_embed_plot_dcca_tutorial.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_embed_plot_dcca_tutorial.py:


========================
Deep CCA (DCCA) Tutorial
========================

Deep CCA is an extension of Canonical Correlation Analysis which uses 2
Deep Neural Networks to transform each view into a lower dimensional space
which is highly correlated with the other. This can be used to uncover
latent nonlinear relations between views and is often used for feature
learning.

This tutorial uses a synthetic multiview dataset which contains
latent information shared between the views, and DCCA is used to uncover
this information in a low dimensional embedding.

.. GENERATED FROM PYTHON SOURCE LINES 17-27

.. code-block:: default


    # License: MIT

    import numpy as np
    from mvlearn.embed import DCCA
    from mvlearn.datasets import make_gaussian_mixture
    from mvlearn.plotting import crossviews_plot
    from mvlearn.model_selection import train_test_split









.. GENERATED FROM PYTHON SOURCE LINES 28-34

Polynomial-Transformed Latent Correlation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Latent variables are sampled from two multivariate Gaussians with equal
prior probability. Then a polynomial transformation is applied and noise is
added independently to both the transformed and untransformed latents.

.. GENERATED FROM PYTHON SOURCE LINES 34-59

.. code-block:: default



    n_samples = 2000
    means = [[0, 1], [0, -1]]
    covariances = [np.eye(2), np.eye(2)]
    Xs, y, latent = make_gaussian_mixture(
          n_samples, means, covariances, transform='poly', random_state=42,
          shuffle=True, shuffle_random_state=42, return_latents=True)

    # Plot latent data against itself to reveal the underlying distribtution.
    crossviews_plot([latent, latent], labels=y,
                    title='Latent Variable', equal_axes=True)


    # Split data into train and test sets
    Xs_train, Xs_test, y_train, y_test = train_test_split(Xs, y, test_size=0.3,
                                                          random_state=42)


    # Plot the testing data after polynomial transformation
    crossviews_plot(Xs_test, labels=y_test,
                    title='Testing Data View 1 vs. View 2 '
                          '(Polynomial Transform + noise)',
                    equal_axes=True)




.. rst-class:: sphx-glr-horizontal


    *

      .. image:: /auto_examples/embed/images/sphx_glr_plot_dcca_tutorial_001.png
          :alt: Latent Variable
          :class: sphx-glr-multi-img

    *

      .. image:: /auto_examples/embed/images/sphx_glr_plot_dcca_tutorial_002.png
          :alt: Testing Data View 1 vs. View 2 (Polynomial Transform + noise)
          :class: sphx-glr-multi-img





.. GENERATED FROM PYTHON SOURCE LINES 60-64

Fit DCCA Model to Uncover Latent Distribution
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The output dimensionality is still 4.

.. GENERATED FROM PYTHON SOURCE LINES 64-77

.. code-block:: default



    # Define parameters and layers for deep model
    features1 = Xs_train[0].shape[1]  # Feature sizes
    features2 = Xs_train[1].shape[1]
    layers1 = [256, 256, 4]  # nodes in each hidden layer and the output size
    layers2 = [256, 256, 4]

    dcca = DCCA(input_size1=features1, input_size2=features2, n_components=4,
                layer_sizes1=layers1, layer_sizes2=layers2, epoch_num=500)
    dcca.fit(Xs_train)
    Xs_transformed = dcca.transform(Xs_test)








.. GENERATED FROM PYTHON SOURCE LINES 78-82

Visualize the Transformed Data
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We can see that it has uncovered the latent correlation between views.

.. GENERATED FROM PYTHON SOURCE LINES 82-89

.. code-block:: default



    crossviews_plot(Xs_transformed, labels=y_test,
                    title='Transformed Testing Data View 1 vs. View 2 '
                          '(Polynomial Transform + noise)',
                    equal_axes=True)




.. image:: /auto_examples/embed/images/sphx_glr_plot_dcca_tutorial_003.png
    :alt: Transformed Testing Data View 1 vs. View 2 (Polynomial Transform + noise)
    :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 90-97

Sinusoidal-Transformed Latent Correlation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Following the same procedure as above, latent variables are sampled from two
multivariate Gaussians with equal prior probability. This time, a sinusoidal
transformation is applied and noise is added independently to both the
transformed and untransformed latents.

.. GENERATED FROM PYTHON SOURCE LINES 97-116

.. code-block:: default



    n_samples = 2000
    means = [[0, 1], [0, -1]]
    covariances = [np.eye(2), np.eye(2)]
    Xs, y, latent = make_gaussian_mixture(
          n_samples, means, covariances, transform='sin', random_state=42,
          shuffle=True, shuffle_random_state=42, return_latents=True, noise_dims=2)

    # Split data into train and test segments
    Xs_train, Xs_test, y_train, y_test = train_test_split(Xs, y, test_size=0.3,
                                                          random_state=42)

    # Plot the testing data against itself after polynomial transformation
    crossviews_plot(Xs_test, labels=y_test,
                    title='Testing Data View 1 vs. View 2 '
                          '(Polynomial Transform + noise)',
                    equal_axes=True)




.. image:: /auto_examples/embed/images/sphx_glr_plot_dcca_tutorial_004.png
    :alt: Testing Data View 1 vs. View 2 (Polynomial Transform + noise)
    :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 117-121

Fit DCCA Model to Uncover Latent Distribution
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The output dimensionality is still 4.

.. GENERATED FROM PYTHON SOURCE LINES 121-134

.. code-block:: default



    # Define parameters and layers for deep model
    features1 = Xs_train[0].shape[1]  # Feature sizes
    features2 = Xs_train[1].shape[1]
    layers1 = [256, 256, 4]  # nodes in each hidden layer and the output size
    layers2 = [256, 256, 4]

    dcca = DCCA(input_size1=features1, input_size2=features2, n_components=4,
                layer_sizes1=layers1, layer_sizes2=layers2, epoch_num=500)
    dcca.fit(Xs_train)
    Xs_transformed = dcca.transform(Xs_test)








.. GENERATED FROM PYTHON SOURCE LINES 135-139

Visualize the Transformed Data
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We can see that it has uncovered the latent correlation between views.

.. GENERATED FROM PYTHON SOURCE LINES 139-145

.. code-block:: default



    crossviews_plot(Xs_transformed, labels=y_test,
                    title='Transformed Testing Data View 1 vs. View 2 '
                          '(Sinusoidal Transform + noise)',
                    equal_axes=True)



.. image:: /auto_examples/embed/images/sphx_glr_plot_dcca_tutorial_005.png
    :alt: Transformed Testing Data View 1 vs. View 2 (Sinusoidal Transform + noise)
    :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  27.659 seconds)


.. _sphx_glr_download_auto_examples_embed_plot_dcca_tutorial.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_dcca_tutorial.py <plot_dcca_tutorial.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_dcca_tutorial.ipynb <plot_dcca_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
